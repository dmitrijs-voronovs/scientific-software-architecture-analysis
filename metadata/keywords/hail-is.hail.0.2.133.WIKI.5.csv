id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117823,Energy Efficiency,efficient,efficient,117823," locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic varia",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:134975,Energy Efficiency,efficient,efficiently,134975,"of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / block_size)^2`` if all blocks are computed. Warning; -------; See all warnings on :meth:`.BlockMatrix.from_entry_expr`. In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; :meth:`.BlockMatrix.write_from_entry_expr`. The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; :meth:`ld_matrix`. To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; block_size : :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.BlockMatrix`; Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index.; """"""; bm = BlockMatrix.from_entry_expr(entry_expr, mean_impute=True, center=True, normalize=True, block_size=block_size); return bm @ bm.T. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; block_size=nullable(int),; ); def ld_matrix(entry_expr, locus_expr, radius, coord_expr=None, block_size=None) -> BlockMatrix:; """"""Com",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29304,Integrability,depend,depends,29304,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:31677,Integrability,depend,dependent,31677," =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44720,Integrability,depend,depending,44720,"type = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:49794,Integrability,depend,depends,49794,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52133,Integrability,depend,dependent,52133,"fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:58794,Integrability,depend,dependent,58794,"ariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77492,Integrability,integrat,integration,77492,"-----+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pape",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77668,Integrability,integrat,integration,77668," allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79251,Integrability,depend,dependent,79251,"| NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93214,Integrability,integrat,integration,93214,"---+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93390,Integrability,integrat,integration,93390,"llele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94956,Integrability,depend,dependent,94956,"--+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Retu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106025,Integrability,integrat,integration,106025,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162549,Integrability,depend,depend,162549," based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; hl.range(0, mt.alleles.length()).map(lambda newi: mt.AD[mt.new_to_old[newi]])",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8336,Modifiability,variab,variable,8336,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8376,Modifiability,variab,variables,8376,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8881,Modifiability,variab,variable,8881,"heck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8947,Modifiability,variab,variables,8947,"heck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:10512,Modifiability,variab,variable,10512,"of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math::. \mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). Boolean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; -",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:11957,Modifiability,variab,variable,11957,"ean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_ind",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:14507,Modifiability,config,config,14507,"_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('linear_regression_rows', covariates). x_field_name = Env.get_uid(); if is_chained:; y_field_names = [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_names), itertools.chain.from_iterable(y))); func = 'LinearRegressionRowsChained'. else:; y_field_names = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_names, y)); func = 'LinearRegressionRowsSingle'. cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). row_fields = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': func,; 'yFields': y_field_names,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'rowBlockSize': block_size,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; }; ht_result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}). return ht_result.persist(). @typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; pass_through=sequenceof(oneof(str, Expression)),; ); def _linear_regression_rows_nd(y, x, covariates, block_size=16, weights=None, pass_through=()) -> Table:; mt = matrix_table_source('linear_regression_rows_nd/x', x); raise_unless_entry_indexed('linear_regression_rows_nd/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows_nd': found ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:14762,Modifiability,config,config,14762,"= [[f'__y_{i}_{j}' for j in range(len(y[i]))] for i in range(len(y))]; y_dict = dict(zip(itertools.chain.from_iterable(y_field_names), itertools.chain.from_iterable(y))); func = 'LinearRegressionRowsChained'. else:; y_field_names = list(f'__y_{i}' for i in range(len(y))); y_dict = dict(zip(y_field_names, y)); func = 'LinearRegressionRowsSingle'. cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). row_fields = _get_regression_row_fields(mt, pass_through, 'linear_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': func,; 'yFields': y_field_names,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'rowBlockSize': block_size,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; }; ht_result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}). return ht_result.persist(). @typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; pass_through=sequenceof(oneof(str, Expression)),; ); def _linear_regression_rows_nd(y, x, covariates, block_size=16, weights=None, pass_through=()) -> Table:; mt = matrix_table_source('linear_regression_rows_nd/x', x); raise_unless_entry_indexed('linear_regression_rows_nd/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows_nd': found no values for 'y'""); is_chained = y_is_list and isinstance(y[0], list). if is_chained and any(len(lst) == 0 for lst in y):; raise ValueError(""'linear_regression_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22005,Modifiability,variab,variables,22005,".annotate_globals(; weight_nds=hl.enumerate(ht.kept_samples).starmap(; lambda group_idx, group_sample_indices: hl.nd.array(; group_sample_indices.map(lambda group_sample_idx: ht.weight_arrays[group_sample_idx][group_idx]); ); ); ); ht = ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22029,Modifiability,variab,variables,22029,".annotate_globals(; weight_nds=hl.enumerate(ht.kept_samples).starmap(; lambda group_idx, group_sample_indices: hl.nd.array(; group_sample_indices.map(lambda group_sample_idx: ht.weight_arrays[group_sample_idx][group_idx]); ); ); ); ht = ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22241,Modifiability,variab,variables,22241," ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_rows_with_themselves(ht.Qtys[i].T); ),; ). ht = setup_globals(ht). def process_block(block):; rows_in_block = hl.len(block). # Processes one block group based on given idx. Returns a single struct.; def process_y_group(idx):; if weights ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:22265,Modifiability,variab,variables,22265," ht.annotate_globals(; sqrt_weights=ht.weight_nds.map(lambda weight_nd: weight_nd.map(lambda e: hl.sqrt(e))); ); ht = ht.annotate_globals(; scaled_y_nds=hl.zip(ht.y_nds, ht.sqrt_weights).starmap(; lambda y, sqrt_weight: y * sqrt_weight.reshape(-1, 1); ); ); ht = ht.annotate_globals(; scaled_cov_nds=hl.zip(ht.cov_nds, ht.sqrt_weights).starmap(; lambda cov, sqrt_weight: cov * sqrt_weight.reshape(-1, 1); ); ). k = builtins.len(covariates); ht = ht.annotate_globals(ns=ht.kept_samples.map(lambda one_sample_set: hl.len(one_sample_set))). def log_message(i):; if is_chained:; return (; ""linear regression_rows[""; + hl.str(i); + ""] running on ""; + hl.str(ht.ns[i]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ); else:; return (; ""linear_regression_rows running on ""; + hl.str(ht.ns[0]); + "" samples for ""; + hl.str(ht.scaled_y_nds[i].shape[1]); + f"" response variables y, with input variables x, and {len(covariates)} additional covariates...""; ). ht = ht.annotate_globals(ns=hl.range(num_y_lists).map(lambda i: hl._console_log(log_message(i), ht.ns[i]))); ht = ht.annotate_globals(; cov_Qts=hl.if_else(; k > 0,; ht.scaled_cov_nds.map(lambda one_cov_nd: hl.nd.qr(one_cov_nd)[0].T),; ht.ns.map(lambda n: hl.nd.zeros((0, n))),; ); ); ht = ht.annotate_globals(Qtys=hl.zip(ht.cov_Qts, ht.scaled_y_nds).starmap(lambda cov_qt, y: cov_qt @ y)). return ht.select_globals(; kept_samples=ht.kept_samples,; __scaled_y_nds=ht.scaled_y_nds,; __sqrt_weight_nds=ht.sqrt_weights,; ns=ht.ns,; ds=ht.ns.map(lambda n: n - k - 1),; __cov_Qts=ht.cov_Qts,; __Qtys=ht.Qtys,; __yyps=hl.range(num_y_lists).map(; lambda i: dot_rows_with_themselves(ht.scaled_y_nds[i].T) - dot_rows_with_themselves(ht.Qtys[i].T); ),; ). ht = setup_globals(ht). def process_block(block):; rows_in_block = hl.len(block). # Processes one block group based on given idx. Returns a single struct.; def process_y_group(idx):; if weights ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26317,Modifiability,variab,variable,26317,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26366,Modifiability,variab,variable,26366,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27753,Modifiability,variab,variables,27753," dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\be",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28043,Modifiability,variab,variable,28043,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28099,Modifiability,variab,variable,28099,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28162,Modifiability,variab,variable,28162,"stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28484,Modifiability,variab,variable,28484,"tes=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36375,Modifiability,variab,variable,36375,"ests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38358,Modifiability,config,config,38358,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38665,Modifiability,config,config,38665," y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hsta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47210,Modifiability,variab,variable,47210,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47259,Modifiability,variab,variable,47259,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48243,Modifiability,variab,variables,48243,"or association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\be",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48533,Modifiability,variab,variable,48533,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48589,Modifiability,variab,variable,48589,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48652,Modifiability,variab,variable,48652,"nary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48974,Modifiability,variab,variable,48974,"... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:56820,Modifiability,variab,variable,56820,"ests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_in",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:58804,Modifiability,variab,variable,58804,"ariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); h",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61253,Modifiability,variab,variable,61253,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61301,Modifiability,variab,variable,61301,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61950,Modifiability,variab,variable,61950,"t64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regres",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63611,Modifiability,config,config,63611,"se ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowere",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63919,Modifiability,config,config,63919,"_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=d",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:71061,Modifiability,variab,variable,71061,"vmat.T) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). fisher_div_score = hl.nd.solve(fisher, score, no_crash=True); chi_sq = hl.or_missing(~fisher_div_score.failed, score @ fisher_div_score.solution); p = hl.pchisqtail(chi_sq, dof); return chi_sq, p. [docs]def linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True):; r""""""Initialize a linear mixed model from a matrix table. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). [docs]@typecheck(; entry_expr=expr_float64,; model=LinearMixedModel,; pa_t_path=nullable(str),; a_t_path=nullable(str),; mean_impute=bool,; partition_size=nullable(int),; pass_through=sequenceof(oneof(str, Expression)),; ); def linear_mixed_regression_rows(; entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=(); ):; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). @typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; accuracy=numeric,; iterations=int,; ); def _linear_skat(; group, weight, y, x, covariates, max_size: int = 46340, accuracy: float = 1e-6, iterations: int = 10000; ):; r""""""The linear sequence kernel association test (SKAT). Linear SKAT tests if the phenotype, `y`, is significantly associated with the genotype, `x`. For; :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the model is; given by:. .. math::. \",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73297,Modifiability,variab,variables,73297,"nder the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; r &= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73602,Modifiability,rewrite,rewrite,73602,"&= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73687,Modifiability,variab,variables,73687,"&= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74340,Modifiability,variab,variables,74340,"he variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; se",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74404,Modifiability,variab,variables,74404," math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74490,Modifiability,variab,variables,74490," r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74690,Modifiability,variab,variables,74690,"widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.ba",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:75170,Modifiability,variab,variables,75170,"rm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79261,Modifiability,variab,variable,79261,"| NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79365,Modifiability,variab,variable,79365,"----+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the nul",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:83827,Modifiability,variab,variables,83827,"al @ ht.G).map(lambda x: x**2) * ht.weight).sum(0)). # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenval",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84346,Modifiability,variab,variables,84346," # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84631,Modifiability,variab,variables,84631,"re symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84689,Modifiability,variab,variables,84689,"erted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84759,Modifiability,variab,variable,84759,"t will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weight",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94966,Modifiability,variab,variable,94966,"--+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Retu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95070,Modifiability,variab,variable,95070,"----+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. -",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106727,Modifiability,variab,variable,106727,"-------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logist",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109291,Modifiability,config,config,109291,"notation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109689,Modifiability,config,config,109689,"= '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(p_value, approximate=True):; chisq = hl.qchisqtail(p_value, 1); if approximate:; med_chisq = hl.agg.filter(~hl.is_nan(p_value), hl.agg.approx_quantiles(chisq, 0.5)); else:; med_chisq = hl.agg.filter(~hl.is_nan(p_value), hl.m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122546,Modifiability,variab,variable-length,122546,"New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (; 'intergenic_consequences',; 'motif_feature_consequences',; 'regulatory_feature_consequences',; 'transcript_consequences',; ); }). if isinstance(ds, Table):; return split.annotate(**update_rows_expression).drop('",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12126,Performance,perform,perform,12126,"ssion model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chain",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12210,Performance,perform,performance,12210,"ing, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chained = y_is_list and isinstance(y[0], list); if is_chained and any(len(lst) ==",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27985,Performance,perform,performs,27985,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48475,Performance,perform,performs,48475,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:78681,Performance,perform,perform,78681,"d a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will ha",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94405,Performance,perform,perform,94405,"a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-val",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:103176,Performance,scalab,scalable,103176,"ero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` rows, several copies of an; :math:`m \times m` matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the `max_size` parameter to skip; groups larger than `max_size`. Warning; -------; :func:`.skat` considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which **all** covariates are defined.; For each row, missing values of `x` are mean-imputed over these columns.; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:108486,Performance,optimiz,optimized,108486,"; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117351,Performance,throughput,throughput,117351,"); .or_error(""Found non-left-aligned variant in split_multi""); ). return hl.bind(error_on_moved, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). return split_rows(hl.sorted(kept_alleles.map(make_struct)), permit_shuffle); else:. def make_struct(i, cond):; def struct_or_empty(v):; return hl.case().when(cond(v.locus), hl.array([new_struct(v, i)])).or_missing(). return hl.bind(struct_or_empty, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). def make_array(cond):; return hl.sorted(kept_alleles.flatmap(lambda i: make_struct(i, cond))). left = split_rows(make_array(lambda locus: locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split tho",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:165589,Performance,queue,queue,165589,"tarray(tint32)),; ),; # DP unchanged; GQ=hl.gq_from_pl(newPL),; PL=newPL,; ).drop('__old_to_new_no_na'). @typecheck(mt=MatrixTable, call_field=str, r2=numeric, bp_window_size=int, memory_per_core=int); def _local_ld_prune(mt, call_field, r2=0.2, bp_window_size=1000000, memory_per_core=256):; bytes_per_core = memory_per_core * 1024 * 1024; fraction_memory_to_use = 0.25; variant_byte_overhead = 50; genotypes_per_pack = 32; n_samples = mt.count_cols(); min_bytes_per_core = math.ceil((1 / fraction_memory_to_use) * 8 * n_samples + variant_byte_overhead); if bytes_per_core < min_bytes_per_core:; raise ValueError(""memory_per_core must be greater than {} MB"".format(min_bytes_per_core // (1024 * 1024))); bytes_per_variant = math.ceil(8 * n_samples / genotypes_per_pack) + variant_byte_overhead; bytes_available_per_core = bytes_per_core * fraction_memory_to_use; max_queue_size = int(max(1.0, math.ceil(bytes_available_per_core / bytes_per_variant))). info(f'ld_prune: running local pruning stage with max queue size of {max_queue_size} variants'). return Table(; ir.MatrixToTableApply(; mt._mir,; {; 'name': 'LocalLDPrune',; 'callField': call_field,; 'r2Threshold': float(r2),; 'windowSize': bp_window_size,; 'maxQueueSize': max_queue_size,; },; ); ).persist(). [docs]@typecheck(; call_expr=expr_call,; r2=numeric,; bp_window_size=int,; memory_per_core=int,; keep_higher_maf=bool,; block_size=nullable(int),; ); def ld_prune(call_expr, r2=0.2, bp_window_size=1000000, memory_per_core=256, keep_higher_maf=True, block_size=None):; """"""Returns a maximal subset of variants that are nearly uncorrelated within each window. .. include:: ../_templates/req_diploid_gt.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Prune variants in linkage disequilibrium by filtering a dataset to those variants returned; by :func:`.ld_prune`. If the dataset contains multiallelic variants, the multiallelic variants; must be filtered out or split befo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167518,Performance,queue,queue,167518,"contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to :func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167581,Performance,queue,queue,167581,"func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:169100,Performance,queue,queue,169100,"y_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.from_entry_expr` with; regard to memory and Hadoop replication errors. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 : :obj:`float`; Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size: :obj:`int`; Window size in base pairs (inclusive upper bound).; memory_per_core : :obj:`int`; Memory in MB per core for local pruning queue.; keep_higher_maf: :obj:`int`; If ``True``, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size: :obj:`int`, optional; Block size for block matrices in the second stage.; Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.Table`; Table of a maximal independent set of variants.; """"""; if block_size is None:; block_size = BlockMatrix.default_block_size(). if not 0.0 <= r2 <= 1:; raise ValueError(f'r2 must be in the range [0.0, 1.0], found {r2}'). if bp_window_size < 0:; raise ValueError(f'bp_window_size must be non-negative, found {bp_window_size}'). raise_unless_entry_indexed('ld_prune/call_expr', call_expr); mt = matrix_table_source('ld_prune/call_expr', call_expr). require_row_key_variant(mt, 'ld_prune'). # FIXME: remove once select_entries on a field is free; if call_expr in mt._fields_inverse:; field = mt._fields_inverse[call_expr]; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28055,Safety,predict,predicting,28055,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48545,Safety,predict,predicting,48545,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:81642,Safety,predict,predicted,81642,"ot explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.count()), _localize=False; ); mt = mt.annotate_globals(yvec=hl.nd.array(yvec), covmat=hl.nd.array(covmat), n_complete_samples=n); # Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #; covmat_Q, _ = hl.nd.qr(mt.covmat); mt = mt.annotate_globals(covmat_Q=covmat_Q); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= ma",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:114112,Safety,avoid,avoids,114112,"iants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is_table else ds._rvrow; kept_alleles = hl.range(1, hl.len(old_row.alleles)); if not keep_star:; kept_alleles = kept_alleles.filter(lambda i: old_row.alleles[i] != ""*""). def new_struct(variant, i):; return hl.struct(alleles=variant.alleles, locus=variant.locus, a_index=i, was_split=hl.len(old_row.alleles) > 2). def split_rows(expr, rekey):; if isinstance(ds, MatrixTable):; mt = ds.annotate_rows(**{new_id: expr}).explode_rows(new_id); if rekey:; mt = mt.key_rows_by(); else:; mt = mt.key_rows_by('locus'); new_row_expr = mt.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122411,Safety,avoid,avoids,122411,"ows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:133308,Safety,avoid,avoid,133308,"G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v: str, s: str, GT: call}')); >>> mt = ht.to_matrix_table(row_key=['v'], col_key=['s']). Compute genotype correlation between all pairs of variants:. >>> ld = hl.row_correlation(mt.GT.n_alt_alleles()); >>> ld.to_numpy(); array([[ 1. , -0.85280287, 0.42640143],; [-0.85280287, 1. , -0.5 ],; [ 0.42640143, -0.5 , 1. ]]). Compute genotype correlation between consecutively-indexed variants:. >>> ld.sparsify_band(lower=0, upper=1).to_numpy(); array([[ 1. , -0.85280287, 0. ],; [ 0. , 1. , -0.5 ],; [ 0. , 0. , 1. ]]). Warning; -------; Rows with a constant value (i.e., zero variance) will result `nan`; correlation values. To avoid this, first check that all rows vary or filter; out constant rows (for example, with the help of :func:`.aggregators.stats`). Notes; -----; In this method, each row of entries is regarded as a vector with elements; defined by `entry_expr` and missing values mean-imputed per row.; The ``(i, j)`` element of the resulting block matrix is the correlation; between rows ``i`` and ``j`` (as 0-indexed by order in the matrix table;; see :meth:`~hail.MatrixTable.add_row_index`). The correlation of two vectors is defined as the; `Pearson correlation coeffecient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / bl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:139327,Safety,avoid,avoid,139327,"ly compute linkage disequilibrium between nearby; variants. Use :meth:`row_correlation` directly to calculate correlation; without windowing. More precisely, variants are 0-indexed by their order in the matrix table; (see :meth:`~hail.MatrixTable.add_row_index`). Each variant is regarded as a vector of; elements defined by `entry_expr`, typically the number of alternate alleles; or genotype dosage. Missing values are mean-imputed within variant. The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants :math:`i` and :math:`j` are on the; same contig and within `radius` base pairs (inclusive) then the; :math:`(i, j)` element is their; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__.; Otherwise, the :math:`(i, j)` element is ``0.0``. Rows with a constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment ma",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:7271,Testability,assert,assert,7271,"parse_locus_interval(x_contig, rg), rg.x_contigs), keep=True; ); if not include_par:; interval_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); mt = hl.filter_intervals(mt, hl.literal(rg.par, interval_type), keep=False). mt = mt.filter_rows((mt[aaf] > aaf_threshold) & (mt[aaf] < (1 - aaf_threshold))); mt = mt.annotate_cols(ib=agg.inbreeding(mt.call, mt[aaf])); kt = mt.select_cols(; is_female=hl.if_else(; mt.ib.f_stat < female_threshold, True, hl.if_else(mt.ib.f_stat > male_threshold, False, hl.missing(tbool)); ),; **mt.ib,; ).cols(). return kt. def _get_regression_row_fields(mt, pass_through, method) -> Dict[str, str]:; row_fields = dict(zip(mt.row_key.keys(), mt.row_key.keys())); for f in pass_through:; if isinstance(f, str):; if f not in mt.row:; raise ValueError(f""'{method}/pass_through': MatrixTable has no row field {f!r}""); if f in row_fields:; # allow silent pass through of key fields; if f in mt.row_key:; pass; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8322,Testability,test,test,8322,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:10629,Testability,test,tested,10629,"1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math::. \mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). Boolean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rs",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:25855,Testability,test,test,25855,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26155,Testability,test,test,26155,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26303,Testability,test,test,26303,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26381,Testability,log,logistic,26381,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26430,Testability,log,logistic,26430,". res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26455,Testability,test,test,26455,". res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26614,Testability,test,test,26614,"; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regressi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26766,Testability,log,logistic,26766," [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mea",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26791,Testability,test,test,26791," [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mea",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26964,Testability,test,test,26964,",; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27321,Testability,test,test,27321,"r association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28024,Testability,test,test,28024,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28122,Testability,log,logistic,28122,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28334,Testability,test,test,28334,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28366,Testability,test,test,28366,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28391,Testability,test,test,28391,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28417,Testability,test,test,28417,"t.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29319,Testability,test,test,29319,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29849,Testability,test,testing,29849," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30052,Testability,test,testing,30052," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30165,Testability,test,testing,30165," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30312,Testability,test,tests,30312,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iter",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30333,Testability,log,logistic,30333,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iter",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:30497,Testability,test,test,30497,"========================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:31417,Testability,test,testing,31417,"ields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence p",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:31959,Testability,test,testing,31959,"for Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression model",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32079,Testability,log,logistic,32079,"teration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32228,Testability,test,testing,32228,"ta` changes by less than :math:`10^{-6}` by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-val",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32527,Testability,log,logistic,32527,"to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32583,Testability,test,tests,32583,"to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32918,Testability,log,logistic,32918,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32934,Testability,log,logistic,32934,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33040,Testability,log,logistf,33040,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33062,Testability,log,logistf,33062,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33188,Testability,log,logfit,33188,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33241,Testability,log,logistf,33241,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33663,Testability,test,test,33663,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statisti",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:33872,Testability,test,test,33872,"nd linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Cour",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34157,Testability,test,testing,34157,"10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34260,Testability,log,logreg,34260,"- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34426,Testability,test,testing,34426," to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missin",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34557,Testability,log,logistic,34557," from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34595,Testability,test,tests,34595," from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34672,Testability,test,tests,34672,"rom small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Si",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35107,Testability,log,logistic,35107," for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expres",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35267,Testability,log,logistic,35267,"the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35287,Testability,test,tests,35287,"the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35351,Testability,test,tests,35351,"ended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed exp",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35944,Testability,test,test,35944,"ers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterati",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35998,Testability,test,test,35998,"ers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterati",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36973,Testability,test,test,36973,"', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37081,Testability,test,test,37081,"lumn-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37215,Testability,assert,assert,37215,"BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37283,Testability,log,logistic,37283,"is property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_ex",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38402,Testability,test,test,38402,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38409,Testability,test,test,38409,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38793,Testability,log,logreg,38793,"logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39311,Testability,log,logit,39311,"lectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39409,Testability,assert,assert,39409,"': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; b",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39437,Testability,assert,assert,39437,"': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; b",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39457,Testability,assert,assert,39457,"ame,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39631,Testability,log,log,39631,"TableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def search(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = hl.log((y * mu) + (1 - y) * (1 - mu)).sum(). n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:40603,Testability,log,log,40603,"t_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def search(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = hl.log((y * mu) + (1 - y) * (1 - mu)).sum(). next_b = b + delta_b; next_mu = sigmoid(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = X.T @ (X * (next_mu * (1 - next_mu)).reshape(-1, 1)). return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution; max_delta_b = nd_max(hl.abs(delta_b)); return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_s",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43552,Testability,log,logit,43552,"it.select('n_iterations', 'converged', 'exploded'),; ). def logistic_score_test(X, y, null_fit):; m = X.shape[1]; m0 = null_fit.b.shape[0]; b = hl.nd.hstack([null_fit.b, hl.nd.zeros((hl.int32(m - m0)))]). X0 = X[:, 0:m0]; X1 = X[:, m0:]. mu = hl.expit(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterati",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43650,Testability,assert,assert,43650,"(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b));",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43678,Testability,assert,assert,43678,"(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b));",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43698,Testability,assert,assert,43698,"e; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariate",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43718,Testability,assert,assert,43718,"(y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44006,Testability,log,log,44006,"0, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44080,Testability,log,log,44080,"True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:46744,Testability,test,test,46744,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47048,Testability,test,test,47048,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47196,Testability,test,test,47196,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47274,Testability,log,logistic,47274,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47323,Testability,log,logistic,47323,"i_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47348,Testability,test,test,47348,"i_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47507,Testability,test,test,47507,"irth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47659,Testability,log,logistic,47659,"). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Bo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47684,Testability,test,test,47684,"). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Bo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47857,Testability,test,test,47857,"ovariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('l",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48514,Testability,test,test,48514,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48612,Testability,log,logistic,48612,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48824,Testability,test,test,48824,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48856,Testability,test,test,48856,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48881,Testability,test,test,48881,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48907,Testability,test,test,48907,"='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ======",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:49809,Testability,test,test,49809,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50339,Testability,test,testing,50339," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50542,Testability,test,testing,50542," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50655,Testability,test,testing,50655," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50802,Testability,test,tests,50802,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50823,Testability,log,logistic,50823,"============= ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:50987,Testability,test,test,50987,"========================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level nu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:51873,Testability,test,testing,51873,"on and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============================; Wald, LRT, Firth `fit.n_iterations` int32 number of iterations until; convergence, explosion, or; reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produce",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52415,Testability,test,testing,52415,"reaching the max (25 for; Wald, LRT; 100 for Firth); Wald, LRT, Firth `fit.converged` bool ``True`` if iteration converged; Wald, LRT, Firth `fit.exploded` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression model",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52535,Testability,log,logistic,52535,"ed` bool ``True`` if iteration exploded; ================ =================== ======= ===============================. We consider iteration to have converged when every coordinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52684,Testability,test,testing,52684,"rdinate of; :math:`\beta` changes by less than :math:`10^{-6}`. For Wald and LRT,; up to 25 iterations are attempted; in testing we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-val",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52983,Testability,log,logistic,52983,"to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53039,Testability,test,tests,53039,"to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53374,Testability,log,logistic,53374,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53390,Testability,log,logistic,53390,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53496,Testability,log,logistf,53496,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53518,Testability,log,logistf,53518,"f variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53644,Testability,log,logfit,53644,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly alwa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:53697,Testability,log,logistf,53697,"e to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly alwa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54119,Testability,test,test,54119,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54329,Testability,test,test,54329,"nd linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54602,Testability,test,testing,54602,"00), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54705,Testability,log,logreg,54705," firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54871,Testability,test,testing,54871,".991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missin",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55002,Testability,log,logistic,55002,"he p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55040,Testability,test,tests,55040,"he p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55117,Testability,test,tests,55117,"uces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Si",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55552,Testability,log,logistic,55552," for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expres",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55712,Testability,log,logistic,55712,"the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55732,Testability,test,tests,55732,"the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` w",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55796,Testability,test,tests,55796,"ended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed exp",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:56389,Testability,test,test,56389,"ers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:56443,Testability,test,test,56443,"ers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57161,Testability,test,test,57161,"ests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows').",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57228,Testability,assert,assert,57228," fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.arr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57296,Testability,log,logistic,57296,"; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field shoul",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:58949,Testability,log,logreg,58949,"ict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:59194,Testability,log,logistic,59194,"hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:59353,Testability,log,logistic,59353,"_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.z",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:59887,Testability,test,test,59887,"y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:59967,Testability,test,test,59967,"variates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60197,Testability,test,test,60197,"ror(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60257,Testability,assert,assert,60257,"ton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for ass",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60264,Testability,test,test,60264,"ton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for ass",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60279,Testability,test,test,60279,"ton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for ass",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60465,Testability,assert,assert,60465,"; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of g",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60503,Testability,assert,assert,60503,"; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of g",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60570,Testability,assert,assert,60570,"lerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60628,Testability,assert,assert,60628,"t = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to includ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60856,Testability,test,test,60856,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61103,Testability,test,test,61103,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61239,Testability,test,test,61239,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61472,Testability,test,tests,61472,"; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62519,Testability,test,test,62519,"ough` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62653,Testability,assert,assert,62653,"e an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63654,Testability,test,test,63654,"se ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowere",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63661,Testability,test,test,63661,"se ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowere",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63952,Testability,test,test,63952,"variates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covari",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64208,Testability,test,test,64208,"variates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covari",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64317,Testability,assert,assert,64317,"variates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covari",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64385,Testability,assert,assert,64385,"ng an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt =",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:65709,Testability,log,logmean,65709,"_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt = mt.annotate_globals(; yvec=(; hl.case(); .when(mt.n - k - 1 >= 1, hl.nd.array(mt.yvec)); .or_error(; hl.format(""_lowered_poisson_regression_rows: insufficient degrees of freedom: n=%s, k=%s"", mt.n, k); ); ),; covmat=hl.nd.array(mt.covmat),; n_complete_samples=mt.n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:65722,Testability,log,log,65722,"_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt = mt.annotate_globals(; yvec=(; hl.case(); .when(mt.n - k - 1 >= 1, hl.nd.array(mt.yvec)); .or_error(; hl.format(""_lowered_poisson_regression_rows: insufficient degrees of freedom: n=%s, k=%s"", mt.n, k); ); ),; covmat=hl.nd.array(mt.covmat),; n_complete_samples=mt.n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:65760,Testability,log,logmean,65760,"t, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt = mt.annotate_globals(; yvec=(; hl.case(); .when(mt.n - k - 1 >= 1, hl.nd.array(mt.yvec)); .or_error(; hl.format(""_lowered_poisson_regression_rows: insufficient degrees of freedom: n=%s, k=%s"", mt.n, k); ); ),; covmat=hl.nd.array(mt.covmat),; n_complete_samples=mt.n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:66504,Testability,test,test,66504,"nsufficient degrees of freedom: n=%s, k=%s"", mt.n, k); ); ),; covmat=hl.nd.array(mt.covmat),; n_complete_samples=mt.n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:66596,Testability,test,test,66596,"n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpres",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67328,Testability,test,test,67328,"annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67466,Testability,assert,assert,67466,"fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67473,Testability,test,test,67473,"fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68083,Testability,assert,assert,68083,"her11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(rec",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68111,Testability,assert,assert,68111,"her11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(rec",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68131,Testability,assert,assert,68131,".T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_m",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68151,Testability,assert,assert,68151,"r = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68171,Testability,assert,assert,68171,".nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68192,Testability,assert,assert,68192,"isher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(f",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68216,Testability,assert,assert,68216,"[fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=Tr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68462,Testability,log,log,68462,"sert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=True). exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution; max_delta_b = nd_max(delta_b.map(lambda e: hl.abs(e))); return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.select(n_iterations=0,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:71047,Testability,test,test,71047,"vmat.T) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). fisher_div_score = hl.nd.solve(fisher, score, no_crash=True); chi_sq = hl.or_missing(~fisher_div_score.failed, score @ fisher_div_score.solution); p = hl.pchisqtail(chi_sq, dof); return chi_sq, p. [docs]def linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True):; r""""""Initialize a linear mixed model from a matrix table. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). [docs]@typecheck(; entry_expr=expr_float64,; model=LinearMixedModel,; pa_t_path=nullable(str),; a_t_path=nullable(str),; mean_impute=bool,; partition_size=nullable(int),; pass_through=sequenceof(oneof(str, Expression)),; ); def linear_mixed_regression_rows(; entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=(); ):; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). @typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; accuracy=numeric,; iterations=int,; ); def _linear_skat(; group, weight, y, x, covariates, max_size: int = 46340, accuracy: float = 1e-6, iterations: int = 10000; ):; r""""""The linear sequence kernel association test (SKAT). Linear SKAT tests if the phenotype, `y`, is significantly associated with the genotype, `x`. For; :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the model is; given by:. .. math::. \",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:71659,Testability,test,test,71659,"ionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). [docs]@typecheck(; entry_expr=expr_float64,; model=LinearMixedModel,; pa_t_path=nullable(str),; a_t_path=nullable(str),; mean_impute=bool,; partition_size=nullable(int),; pass_through=sequenceof(oneof(str, Expression)),; ); def linear_mixed_regression_rows(; entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=(); ):; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). @typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; accuracy=numeric,; iterations=int,; ); def _linear_skat(; group, weight, y, x, covariates, max_size: int = 46340, accuracy: float = 1e-6, iterations: int = 10000; ):; r""""""The linear sequence kernel association test (SKAT). Linear SKAT tests if the phenotype, `y`, is significantly associated with the genotype, `x`. For; :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the model is; given by:. .. math::. \begin{align*}; X &: R^{N \times K} \quad\quad \textrm{covariates} \\; G &: \{0, 1, 2\}^{N \times M} \textrm{genotypes} \\; \\; \varepsilon &\sim N(0, \sigma^2) \\; y &= \beta_0 X + \beta_1 G + \varepsilon; \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual pheno",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:71684,Testability,test,tests,71684,"rMixedModel,; pa_t_path=nullable(str),; a_t_path=nullable(str),; mean_impute=bool,; partition_size=nullable(int),; pass_through=sequenceof(oneof(str, Expression)),; ); def linear_mixed_regression_rows(; entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=(); ):; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). @typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; accuracy=numeric,; iterations=int,; ); def _linear_skat(; group, weight, y, x, covariates, max_size: int = 46340, accuracy: float = 1e-6, iterations: int = 10000; ):; r""""""The linear sequence kernel association test (SKAT). Linear SKAT tests if the phenotype, `y`, is significantly associated with the genotype, `x`. For; :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the model is; given by:. .. math::. \begin{align*}; X &: R^{N \times K} \quad\quad \textrm{covariates} \\; G &: \{0, 1, 2\}^{N \times M} \textrm{genotypes} \\; \\; \varepsilon &\sim N(0, \sigma^2) \\; y &= \beta_0 X + \beta_1 G + \varepsilon; \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; r &= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:72162,Testability,test,tests,72162,"e NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). @typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; accuracy=numeric,; iterations=int,; ); def _linear_skat(; group, weight, y, x, covariates, max_size: int = 46340, accuracy: float = 1e-6, iterations: int = 10000; ):; r""""""The linear sequence kernel association test (SKAT). Linear SKAT tests if the phenotype, `y`, is significantly associated with the genotype, `x`. For; :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the model is; given by:. .. math::. \begin{align*}; X &: R^{N \times K} \quad\quad \textrm{covariates} \\; G &: \{0, 1, 2\}^{N \times M} \textrm{genotypes} \\; \\; \varepsilon &\sim N(0, \sigma^2) \\; y &= \beta_0 X + \beta_1 G + \varepsilon; \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; r &= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix wh",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74976,Testability,test,test,74976,"etric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is si",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:75339,Testability,test,testing,75339,"vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:75405,Testability,test,test,75405,"vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:76577,Testability,test,test,76577,"e a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.76e+02 | 1.23e-05 | 0 |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the nul",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79646,Testability,test,test,79646,"cept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing cov",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:80312,Testability,test,test,80312,"ndependent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_co",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:83687,Testability,assert,asserted,83687,"al @ ht.G).map(lambda x: x**2) * ht.weight).sum(0)). # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenval",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84310,Testability,assert,asserted,84310,"note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87086,Testability,log,logistic,87086," ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and resi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87123,Testability,test,test,87123," ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and resi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87150,Testability,test,tests,87150," write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87465,Testability,log,logit,87465,"extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. Therefore :math:`r_i`, the residual phenotype, is the portion of t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87571,Testability,test,tests,87571,"'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. Therefore :math:`r_i`, the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. 1. Each sample's phenotype is Bernoulli distributed with mean :math:`p_i` and variance; :math:`\sigma^2_i = p_i(1 - p_i)`, ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:88118,Testability,log,logit,88118,"tion test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. Therefore :math:`r_i`, the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. 1. Each sample's phenotype is Bernoulli distributed with mean :math:`p_i` and variance; :math:`\sigma^2_i = p_i(1 - p_i)`, the binomial variance. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\q",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:88241,Testability,log,logit,88241,", with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. Therefore :math:`r_i`, the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. 1. Each sample's phenotype is Bernoulli distributed with mean :math:`p_i` and variance; :math:`\sigma^2_i = p_i(1 - p_i)`, the binomial variance. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:90855,Testability,test,test,90855,"; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. The null hypothesis test tests for the probability of observing even larger values of :math:`Q`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:90860,Testability,test,tests,90860,"; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. The null hypothesis test tests for the probability of observing even larger values of :math:`Q`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:91049,Testability,test,testing,91049,"nd{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. The null hypothesis test tests for the probability of observing even larger values of :math:`Q`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:91115,Testability,test,test,91115,"nd{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. The null hypothesis test tests for the probability of observing even larger values of :math:`Q`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:92297,Testability,test,test,92297,"t with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the n",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95351,Testability,test,test,95351," specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the gen",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95545,Testability,log,logistic,95545,"aper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95629,Testability,log,logisitic,95629,"ters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype fr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:96271,Testability,test,test,96271,"mum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. - null_fit:. - b : :obj:`.tndarray` vector of coefficients. - score : :obj:`.tndarray` vector of score statistics. - fisher : :obj:`.tndarray` matrix of fisher statistics. - mu : :obj:`.tndarray` the expected value under the null model. - n_iterations : :obj:`.tint32` the number of iterations before termination. - log_lkhd : :obj:`.tfloat64` the log-likelihood of the final iteration. - converged : :obj:`.tbool` True if the null model converged. - exploded : :obj:`.tbo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:97190,Testability,log,log-likelihood,97190,"r why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. - null_fit:. - b : :obj:`.tndarray` vector of coefficients. - score : :obj:`.tndarray` vector of score statistics. - fisher : :obj:`.tndarray` matrix of fisher statistics. - mu : :obj:`.tndarray` the expected value under the null model. - n_iterations : :obj:`.tint32` the number of iterations before termination. - log_lkhd : :obj:`.tfloat64` the log-likelihood of the final iteration. - converged : :obj:`.tbool` True if the null model converged. - exploded : :obj:`.tbool` True if the null model failed to converge due to numerical; explosion. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_logistic_skat: at least one covariate is required.'); _warn_if_no_intercept('_logistic_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); if mt.y.dtype != hl.tbool:; mt = mt.annotate_cols(; y=(; hl.case(); .when(hl.any(mt.y == 0, mt.y == 1), hl.bool(mt.y)); .or_error(; hl.format(; f'hl._logistic_skat: phenotypes must either be True, False, 0, or 1, found: %s of type {mt.y.dtype}',; mt.y,; ); ); ); ); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101395,Testability,log,logistic,101395,".len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101576,Testability,log,logistic,101576,".len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101778,Testability,log,logistic,101778,".len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101793,Testability,test,test,101793,".len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` r",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101896,Testability,test,test,101896,"he complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` rows, several copies of an; :math:`m \times m` matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the `max_size` parameter to skip; groups larger than `max_size`. Warning; -------; :func:`.skat` considers the same set of col",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:103239,Testability,test,test,103239,"ero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` rows, several copies of an; :math:`m \times m` matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the `max_size` parameter to skip; groups larger than `max_size`. Warning; -------; :func:`.skat` considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which **all** covariates are defined.; For each row, missing values of `x` are mean-imputed over these columns.; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+----",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:103848,Testability,log,logistic,103848," for which **all** covariates are defined.; For each row, missing values of `x` are mean-imputed over these columns.; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105000,Testability,log,logistic,105000," Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105201,Testability,log,logistic,105201," a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+------------------------------------",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106463,Testability,log,logistic,106463,"------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106842,Testability,log,logistic,106842,"---------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); els",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106938,Testability,test,test,106938,"---------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); els",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106961,Testability,log,logistic,106961,"-+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106970,Testability,test,test,106970,"-+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106997,Testability,log,logistic,106997,"-+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107084,Testability,log,logistic,107084,"ory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107093,Testability,test,test,107093,"ory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107266,Testability,test,test,107266,".Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107564,Testability,log,logistic,107564,"t64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weigh",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107637,Testability,log,logistic,107637,"t64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weigh",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107698,Testability,log,logistic,107698,"t64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weigh",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:108454,Testability,log,logic,108454,"; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109003,Testability,log,logistic,109003,"ntry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109087,Testability,log,logistic,109087,"ntry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109173,Testability,assert,assert,109173,"e('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109191,Testability,log,logistic,109191,"e('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109216,Testability,log,logistic,109216,"e('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109281,Testability,log,logistic,109281,"e('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109466,Testability,log,logistic,109466,"notation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:115532,Testability,assert,assert,115532,"v.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is_table else ds._rvrow; kept_alleles = hl.range(1, hl.len(old_row.alleles)); if not keep_star:; kept_alleles = kept_alleles.filter(lambda i: old_row.alleles[i] != ""*""). def new_struct(variant, i):; return hl.struct(alleles=variant.alleles, locus=variant.locus, a_index=i, was_split=hl.len(old_row.alleles) > 2). def split_rows(expr, rekey):; if isinstance(ds, MatrixTable):; mt = ds.annotate_rows(**{new_id: expr}).explode_rows(new_id); if rekey:; mt = mt.key_rows_by(); else:; mt = mt.key_rows_by('locus'); new_row_expr = mt._rvrow.annotate(; locus=mt[new_id]['locus'],; alleles=mt[new_id]['alleles'],; a_index=mt[new_id]['a_index'],; was_split=mt[new_id]['was_split'],; old_locus=mt.locus,; old_alleles=mt.alleles,; ).drop(new_id). mt = mt._select_rows('split_multi', new_row_expr); if rekey:; return mt.key_rows_by('locus', 'alleles'); else:; return MatrixTable(ir.MatrixKeyRowsBy(mt._mir, ['locus', 'alleles'], is_sorted=True)); else:; assert isinstance(ds, Table); ht = ds.annotate(**{new_id: expr}).explode(new_id); if rekey:; ht = ht.key_by(); else:; ht = ht.key_by('locus'); new_row_expr = ht.row.annotate(; locus=ht[new_id]['locus'],; alleles=ht[new_id]['alleles'],; a_index=ht[new_id]['a_index'],; was_split=ht[new_id]['was_split'],; old_locus=ht.locus,; old_alleles=ht.alleles,; ).drop(new_id). ht = ht._select('split_multi', new_row_expr); if rekey:; return ht.key_by('locus', 'alleles'); else:; return Table(ir.TableKeyBy(ht._tir, ['locus', 'alleles'], is_sorted=True)). if left_aligned:. def make_struct(i):; def error_on_moved(v):; return (; hl.case(); .when(v.locus == old_row.locus, new_struct(v, i)); .or_error(""Found non-left-aligned variant in split_multi""); ). return hl.bind(error_on_moved, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). return split_rows(hl.sorted(kept_alleles.map(make_struct)), permit_shuffle); else:. def make_struct(i, cond):; def struct_or_empty(v):; return hl.case",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117566,Testability,log,logic,117566,"array([new_struct(v, i)])).or_missing(). return hl.bind(struct_or_empty, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). def make_array(cond):; return hl.sorted(kept_alleles.flatmap(lambda i: make_struct(i, cond))). left = split_rows(make_array(lambda locus: locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) >",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:159734,Testability,log,log,159734,"mputing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:89675,Usability,simpl,simplify,89675,"etric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^T` and :math:`Z^T Z` are the squared singular values of :math:`Z`;; therefore, we instead focus on :math:`Z^T Z`. In the expressions below, we elide transpositions; of symmetric matrices:. .. math::. \begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}. Before substituting the definition of :math:`P_0`, simplify it using the reduced QR; decomposition:. .. math::. \begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:90099,Usability,simpl,simplified,90099,"_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^T` and :math:`Z^T Z` are the squared singular values of :math:`Z`;; therefore, we instead focus on :math:`Z^T Z`. In the expressions below, we elide transpositions; of symmetric matrices:. .. math::. \begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}. Before substituting the definition of :math:`P_0`, simplify it using the reduced QR; decomposition:. .. math::. \begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \textrm{the singular value decomposition} \\; \lambda_s &= S_{ss}^2 \\; \\; Q &\sim \textrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. The null hypothesis test tests for the probability of observing even larger values of :math:`Q`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel associa",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:129927,Usability,simpl,simply,129927,"---. >>> rrm = hl.realized_relationship_matrix(dataset.GT). Notes; -----; The realized relationship matrix (RRM) is defined as follows. Consider the; :math:`n \times m` matrix :math:`C` of raw genotypes, with rows indexed by; :math:`n` samples and columns indexed by the :math:`m` bialellic autosomal; variants; :math:`C_{ij}` is the number of alternate alleles of variant; :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For; each variant :math:`j`, the sample alternate allele frequency :math:`p_j` is; computed as half the mean of the non-missing entries of column :math:`j`.; Entries of :math:`M` are then mean-centered and variance-normalized as. .. math::. M_{ij} =; \frac{C_{ij}-2p_j}; {\sqrt{\frac{m}{n} \sum_{k=1}^n (C_{ij}-2p_j)^2}},. with :math:`M_{ij} = 0` for :math:`C_{ij}` missing (i.e. mean genotype; imputation). This scaling normalizes each variant column to have empirical; variance :math:`1/m`, which gives each sample row approximately unit total; variance (assuming linkage equilibrium) and yields the :math:`n \times n`; sample correlation or realized relationship matrix (RRM) :math:`K` as simply. .. math::. K = MM^T. Note that the only difference between the realized relationship matrix and; the genetic relatedness matrix (GRM) used in; :func:`.realized_relationship_matrix` is the variant (column) normalization:; where RRM uses empirical variance, GRM uses expected variance under; Hardy-Weinberg Equilibrium. This method drops variants with zero variance before computing kinship. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on matrix table with columns corresponding; to samples. Returns; -------; :class:`.BlockMatrix`; Realized relationship matrix for all samples. Row and column indices; correspond to matrix table column index.; """"""; mt = matrix_table_source('realized_relationship_matrix/call_expr', call_expr); raise_unless_entry_indexed('realized_relationship_matrix/call_expr', call_expr",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:22694,Deployability,update,updated,22694,"2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a > b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a > b, a, b)). [docs]@typecheck(nd1=expr_ndarray(), nd2=oneof(expr_ndarray(), list)); def minimum(nd1, nd2):; """"""Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index. If an array element being compared has the value NaN,; the minimum for that index will be NaN. Examples; --------; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters; ----------; nd1 : :class:`.NDArrayExpression`; nd2 : class:`.NDArrayExpression`, `.ArrayExpression`, numpy ndarray, or nested python lists/tuples.; nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns; -------; min_array : :class:`.NDArrayExpression`; Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a < b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a < b, a, b)).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:481,Energy Efficiency,reduce,reduce,481,". Hail | ; hail.nd.nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.nd.nd. Source code for hail.nd.nd; from functools import reduce. import hail as hl; from hail.expr.expressions import (; Int64Expression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_tuple,; unify_all,; ); from hail.expr.expressions.typed_expressions import NDArrayNumericExpression; from hail.expr.functions import _ndarray; from hail.expr.functions import array as aarray; from hail.expr.types import HailType, tfloat32, tfloat64, tndarray, ttuple; from hail.ir import Apply, NDArrayConcat, NDArrayEigh, NDArrayInv, NDArrayQR, NDArraySVD; from hail.typecheck import nullable, oneof, sequenceof, tupleof, typecheck. tsequenceof_nd = oneof(sequenceof(expr_ndarray()), expr_array(expr_ndarray())); shape_type = oneof(expr_int64, tupleof(expr_int64), expr_tuple()). [docs]def array(input_array, dtype=None):; """"""Construct an :class:`.NDArrayExpression`. Examples; --------. >>> hl.eval(hl.nd.array([1, 2, 3, 4])); array([1, 2, 3, 4], dtype=int32). >>> hl.eval(hl.nd.array([[1, 2, 3], [4, 5, 6]])); array([[1, 2, 3],; [4, 5, 6]], dtype=int32). >>> hl.eval(hl.nd.array(np.identity(3))); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]). >>> hl.eval(hl.nd.array(hl.range(10, 20))); array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=int32). Parameters; ----------; input_array : :class:`.ArrayExpression`, numpy ndarray, or nested python lists/tuples; The array to convert to a Hail ndarray.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. Returns; -------; :class:`.NDArray",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:4234,Energy Efficiency,reduce,reduce,4234,"eturns; -------; :class:`.NDArrayNumericExpression`; A 1-dimensional ndarray from `start` to `stop` by `step`.; """"""; return _ndarray(hl.range(start, stop, step)). [docs]@typecheck(shape=shape_type, value=expr_any, dtype=nullable(HailType)); def full(shape, value, dtype=None):; """"""Creates a hail :class:`.NDArrayNumericExpression` full of the specified value. Examples; --------. Create a 5 by 7 NDArray of type :py:data:`.tfloat64` 9s. >>> hl.nd.full((5, 7), 9). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.full((5, 7), 9, dtype=hl.tint32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; value : :class:`.Expression` or python value; Value to fill ndarray with.; dtype : :class:`.HailType`; Desired hail type. Returns; -------; :class:`.NDArrayNumericExpression`; An ndarray of the specified shape filled with the specified value.; """"""; if isinstance(shape, Int64Expression):; shape_product = shape; else:; shape_product = reduce(lambda a, b: a * b, shape); return arange(hl.int32(shape_product)).map(lambda x: cast_expr(value, dtype)).reshape(shape). [docs]@typecheck(shape=shape_type, dtype=HailType); def zeros(shape, dtype=tfloat64):; """"""Creates a hail :class:`.NDArrayNumericExpression` full of zeros. Examples; --------. Create a 5 by 7 NDArray of type :py:data:`.tfloat64` zeros. >>> hl.nd.zeros((5, 7)). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.zeros((5, 7), dtype=hl.tfloat32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. See Also; --------; :func:`.full`. Returns; -------; :class:`.NDArrayNumericExpression`; ndarray of the specified size full of zeros.; """"""; return full(shape, 0, dtype). [docs]@typecheck(shape=shape_type, dtype=HailType); def ones(shape, dtype=tfloat64):; """"""Creates a hail :class:`.NDArrayNumericExpre",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9271,Energy Efficiency,reduce,reduced,9271,"ndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9344,Energy Efficiency,reduce,reduced,9344,"); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9582,Energy Efficiency,reduce,reduced,9582,"near_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10144,Energy Efficiency,reduce,reduced,10144,"p(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tnd",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10194,Energy Efficiency,reduce,reduced,10194,"d_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10624,Energy Efficiency,reduce,reduced,10624,"th::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). [docs]@typecheck(nd=expr_ndarray(), full_matrices=bool, compute_uv=bool); def svd(nd, full_matrices=True, compute_uv=True):; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv : :class:`.b",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:11084,Energy Efficiency,reduce,reduced,11084," Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). [docs]@typecheck(nd=expr_ndarray(), full_matrices=bool, compute_uv=bool); def svd(nd, full_matrices=True, compute_uv=True):; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv : :class:`.bool`; If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns; -------; - u: :class:`.NDArrayNumericExpression`; The left singular vectors.; - s: :class:`.NDArrayNumericExpression`; The singular values.; - vt: :class:`.NDArrayNumericExpression`; The right singular vectors.; """"""; float_nd = nd.ma",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:6713,Modifiability,variab,variables,6713,"rrayNumericExpression`; ndarray of the specified size full of ones.; """"""; return full(shape, 1, dtype). [docs]@typecheck(nd=expr_ndarray()); def diagonal(nd):; """"""Gets the diagonal of a 2 dimensional NDArray. Examples; --------. >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional NDArray, shape(M, N). Returns; -------; :class:`.NDArrayExpression`; A 1 dimension NDArray of length min(M, N), containing the diagonal of `nd`.; """"""; assert nd.ndim == 2, ""diagonal requires 2 dimensional ndarray""; shape_min = hl.min(nd.shape[0], nd.shape[1]); return hl.nd.array(hl.range(hl.int32(shape_min)).map(lambda i: nd[i, i])). [docs]@typecheck(a=expr_ndarray(), b=expr_ndarray(), no_crash=bool); def solve(a, b, no_crash=False):; """"""Solve a linear system. Parameters; ----------; a : :class:`.NDArrayNumericExpression`, (N, N); Coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_type, indices, aggregations). if b_ndim_orig == 1:; if no_crash:; result = hl.struct(solution=result.solution.reshape((-1)), failed=result.failed); else:; result = result.reshape((-1)); return result. [docs]@typecheck(A=expr_ndarray(), b=expr_ndarray(), lower=expr_bool, no_crash=bool); def solve_triangular(A, b, lower=False, no_crash=False):; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpr",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:7807,Modifiability,variab,variables,7807," (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_type, indices, aggregations). if b_ndim_orig == 1:; if no_crash:; result = hl.struct(solution=result.solution.reshape((-1)), failed=result.failed); else:; result = result.reshape((-1)); return result. [docs]@typecheck(A=expr_ndarray(), b=expr_ndarray(), lower=expr_bool, no_crash=bool); def solve_triangular(A, b, lower=False, no_crash=False):; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpression`, (N, N); Triangular coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables.; lower : `bool`:; If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the triangular system Ax = B. Shape is same as shape of B. """"""; nd_dep_ndim_orig = b.ndim; A, b = solve_helper(A, b, nd_dep_ndim_orig). indices, aggregations = unify_all(A, b). if no_crash:; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = res",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:2180,Testability,assert,assert,2180,"pr_ndarray()), expr_array(expr_ndarray())); shape_type = oneof(expr_int64, tupleof(expr_int64), expr_tuple()). [docs]def array(input_array, dtype=None):; """"""Construct an :class:`.NDArrayExpression`. Examples; --------. >>> hl.eval(hl.nd.array([1, 2, 3, 4])); array([1, 2, 3, 4], dtype=int32). >>> hl.eval(hl.nd.array([[1, 2, 3], [4, 5, 6]])); array([[1, 2, 3],; [4, 5, 6]], dtype=int32). >>> hl.eval(hl.nd.array(np.identity(3))); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]). >>> hl.eval(hl.nd.array(hl.range(10, 20))); array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=int32). Parameters; ----------; input_array : :class:`.ArrayExpression`, numpy ndarray, or nested python lists/tuples; The array to convert to a Hail ndarray.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. Returns; -------; :class:`.NDArrayExpression`; An ndarray based on the input array.; """"""; return _ndarray(input_array, dtype=dtype). @typecheck(a=expr_array(), shape=shape_type); def from_column_major(a, shape):; assert len(shape) == 2; return array(a).reshape(tuple(reversed(shape))).T. [docs]@typecheck(start=expr_int32, stop=nullable(expr_int32), step=expr_int32); def arange(start, stop=None, step=1) -> NDArrayNumericExpression:; """"""Returns a 1-dimensions ndarray of integers from `start` to `stop` by `step`. Examples; --------. >>> hl.eval(hl.nd.arange(10)); array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32). >>> hl.eval(hl.nd.arange(3, 10)); array([3, 4, 5, 6, 7, 8, 9], dtype=int32). >>> hl.eval(hl.nd.arange(0, 10, step=3)); array([0, 3, 6, 9], dtype=int32). Notes; -----; The range includes `start`, but excludes `stop`. If provided exactly one argument, the argument is interpreted as `stop` and; `start` is set to zero. This matches the behavior of Python's ``range``. Parameters; ----------; start : int or :class:`.Expression` of type :py:data:`.tint32`; Start of range.; stop : int or :class:`.Expression` of type :py:data:`.tint32`; End of range.; step : int or :class:`.Exp",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:6240,Testability,assert,assert,6240,"es; --------. Create a 5 by 7 NDArray of type :py:data:`.tfloat64` ones. >>> hl.nd.ones((5, 7)). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.ones((5, 7), dtype=hl.tfloat32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. See Also; --------; :func:`.full`. Returns; -------; :class:`.NDArrayNumericExpression`; ndarray of the specified size full of ones.; """"""; return full(shape, 1, dtype). [docs]@typecheck(nd=expr_ndarray()); def diagonal(nd):; """"""Gets the diagonal of a 2 dimensional NDArray. Examples; --------. >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional NDArray, shape(M, N). Returns; -------; :class:`.NDArrayExpression`; A 1 dimension NDArray of length min(M, N), containing the diagonal of `nd`.; """"""; assert nd.ndim == 2, ""diagonal requires 2 dimensional ndarray""; shape_min = hl.min(nd.shape[0], nd.shape[1]); return hl.nd.array(hl.range(hl.int32(shape_min)).map(lambda i: nd[i, i])). [docs]@typecheck(a=expr_ndarray(), b=expr_ndarray(), no_crash=bool); def solve(a, b, no_crash=False):; """"""Solve a linear system. Parameters; ----------; a : :class:`.NDArrayNumericExpression`, (N, N); Coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:8874,Testability,assert,assert,8874,"rpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the triangular system Ax = B. Shape is same as shape of B. """"""; nd_dep_ndim_orig = b.ndim; A, b = solve_helper(A, b, nd_dep_ndim_orig). indices, aggregations = unify_all(A, b). if no_crash:; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:8900,Testability,assert,assert,8900,"rpreted as a upper triangular matrix. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the triangular system Ax = B. Shape is same as shape of B. """"""; nd_dep_ndim_orig = b.ndim; A, b = solve_helper(A, b, nd_dep_ndim_orig). indices, aggregations = unify_all(A, b). if no_crash:; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:10516,Testability,assert,assert,10516,"eturns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with orthonormal columns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). [docs]@typecheck(nd=expr_ndarray(), full_matrices=bool, compute_uv=bool); def svd(nd, full_matrices=True, compute_uv=True):; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:13426,Testability,assert,assert,13426,"position of a symmetric matrix. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(N, N).; eigvals_only: :class:`.bool`; If False (default), compute the eigenvectors and eigenvalues. Otherwise, only compute eigenvalues. Returns; -------; - w: :class:`.NDArrayNumericExpression`; The eigenvalues, shape(N).; - v: :class:`.NDArrayNumericExpression`; The eigenvectors, shape(N, N). Only returned if eigvals_only is false.; """"""; float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayEigh(float_nd._ir, eigvals_only). return_type = tndarray(tfloat64, 1) if eigvals_only else ttuple(tndarray(tfloat64, 1), tndarray(tfloat64, 2)); return construct_expr(ir, return_type, nd._indices, nd._aggregations). [docs]@typecheck(nd=expr_ndarray()); def inv(nd):; """"""Performs a matrix inversion. Parameters; ----------. nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray. Returns; -------; :class:`.NDArrayNumericExpression`; The inverted matrix.; """""". assert nd.ndim == 2, ""Matrix inversion requires 2 dimensional ndarray"". float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayInv(float_nd._ir); return construct_expr(ir, tndarray(tfloat64, 2), nd._indices, nd._aggregations). [docs]@typecheck(nds=tsequenceof_nd, axis=int); def concatenate(nds, axis=0):; """"""Join a sequence of arrays along an existing axis. Examples; --------. >>> x = hl.nd.array([[1., 2.], [3., 4.]]); >>> y = hl.nd.array([[5.], [6.]]); >>> hl.eval(hl.nd.concatenate([x, y], axis=1)); array([[1., 2., 5.],; [3., 4., 6.]]); >>> x = hl.nd.array([1., 2.]); >>> y = hl.nd.array([3., 4.]); >>> hl.eval(hl.nd.concatenate((x, y), axis=0)); array([1., 2., 3., 4.]). Parameters; ----------; nds : a sequence of :class:`.NDArrayNumericExpression`; The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).; Note: unlike Numpy, the numerical element type of each array_like must match.; axis : int, optional; The axis along which the arrays will",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/nd/nd.html:15058,Testability,assert,assert,15058,", axis=0)); array([1., 2., 3., 4.]). Parameters; ----------; nds : a sequence of :class:`.NDArrayNumericExpression`; The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).; Note: unlike Numpy, the numerical element type of each array_like must match.; axis : int, optional; The axis along which the arrays will be joined. Default is 0.; Note: unlike Numpy, if provided, axis cannot be None. Returns; -------; :class:`.NDArrayExpression`; The concatenated array; """"""; head_nd = nds[0]. if isinstance(nds, list):; indices, aggregations = unify_all(*nds); typs = {x.dtype for x in nds}. if len(typs) != 1:; element_types = {t.element_type for t in typs}; if len(element_types) != 1:; argument_element_types_str = "", "".join(str(nd.dtype.element_type) for nd in nds); raise ValueError(; f'hl.nd.concatenate: ndarrays must have same element types, found these element types: ({argument_element_types_str})'; ). ndims = {t.ndim for t in typs}; assert len(ndims) != 1; ndims_str = "", "".join(str(nd.dtype.ndim) for nd in nds); raise ValueError(f'hl.nd.concatenate: ndarrays must have same number of dimensions, found: {ndims_str}.'); else:; indices = nds._indices; aggregations = nds._aggregations. makearr = aarray(nds); concat_ir = NDArrayConcat(makearr._ir, axis). return construct_expr(concat_ir, tndarray(head_nd._type.element_type, head_nd.ndim), indices, aggregations). [docs]@typecheck(N=expr_numeric, M=nullable(expr_numeric), dtype=HailType); def eye(N, M=None, dtype=tfloat64):; """"""; Construct a 2-D :class:`.NDArrayExpression` with ones on the *main* diagonal; and zeros elsewhere. Examples; --------; >>> hl.eval(hl.nd.eye(3)); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]); >>> hl.eval(hl.nd.eye(2, 5, dtype=hl.tint32)); array([[1, 0, 0, 0, 0],; [0, 1, 0, 0, 0]], dtype=int32). Parameters; ----------; N : :class:`.NumericExpression` or Python number; Number of rows in the output.; M : :class:`.NumericExpression` or Python number, opt",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:6698,Availability,down,down,6698,"es), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; fy = y[ui] + e; new_y[ui] = fy; keep[ui] = True; j = ui + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; if j >= len(x):; break; if compare(udx, udy, dx, judy) < 0:; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; if compare(ldx, ldy, dx, jldy) > 0:; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; j += 1; return new_y, keep. [docs]def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; tit",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:18648,Availability,avail,available,18648,"x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; data = _generate_hist2d_data(x, y, bins, range).to_pandas(). # Use python prettier float -> str function; data['x'] = data['x'].apply(lambda e: str(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_c",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23267,Availability,down,downsample,23267,"evels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, co",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23974,Availability,down,downsample,23974,"hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes import viridis. _palette = viridis(n). return CategoricalColorMapper(factors=factors, palette=_palette). def _get_scatter_plot_elements(; sp: Plot,; source_pd: pd.DataFrame,; x_col: str,; y_col: str,; label_cols: List[str],; colors: Optional[Dict[str, ColorMapper]] = None,; size: int = 4,; hover_cols: Optional[Set[str]] = None,; ) -> Union[; Tuple[Plot, Dict[str, List[LegendItem]], Legend, ColorBar,",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:31076,Availability,down,down,31076,"element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space unit",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:32829,Availability,down,downsample,32829,"ntinuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.models.Plot` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""; hover_fields = {} if hover_fields is None else hover_fields. label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors. label_cols = list(label_by_col.keys()); if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divi",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:37951,Availability,down,down,37951,"ement is used as the hover label. This function returns a :class:`bokeh.models.layouts.Column` containing two :class:`figure.Row`:; - The first row contains the X-axis marginal density and a selection widget if multiple entries are specified in the ``label``; - The second row contains the scatter plot and the y-axis marginal density. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:39732,Availability,down,downsample,39732,"ntinuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`.GridPlot`; """"""; # Collect data; hover_fields = {} if hover_fields is None else hover_fields. label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors; if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. label_cols = list(label_by_col.keys()); source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divisions=_downsampling_factor('join_plot', n_divisions, collect_all),; missing_labe",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:47482,Availability,down,down,47482,"https://en.wikipedia.org/wiki/Q-Q_plot). If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive qq plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:49077,Availability,down,downsample,49077,"ical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.plotting.figure` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""; hover_fields = {} if hover_fields is None else hover_fields; label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. source = pvals._indices.source; if isinstance(source, Table):; ht = source.select(p_value=pvals, **hover_fields, **label_by_col); else:; assert isinstance(source, MatrixTable); ht = source.select_rows(p_value=pvals, **hover_fields, **label_by_col).rows(); ht = ht.key_by().select('p_value', *hover_fields, *label_by_col).key_by('p_value'); n = ht.aggregate(aggregators.count",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:52482,Availability,down,downsample,52482,"visions=nullable(int),; significance_line=nullable(numeric),; ); def manhattan(; pvals: 'Float64Expression',; locus: 'Optional[LocusExpression]' = None,; title: 'Optional[str]' = None,; size: int = 4,; hover_fields: 'Optional[Dict[str, Expression]]' = None,; collect_all: 'Optional[bool]' = None,; n_divisions: 'Optional[int]' = 500,; significance_line: 'Optional[Union[int, float]]' = 5e-8,; ) -> Plot:; """"""Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters; ----------; pvals : :class:`.Float64Expression`; P-values to be plotted.; locus : :class:`.LocusExpression`, optional; Locus values to be plotted.; title : str, optional; Title of the plot.; size : int; Size of markers in screen space units.; hover_fields : Dict[str, :class:`.Expression`], optional; Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all : bool, optional; Deprecated - use `n_divisions` instead.; n_divisions : int, optional.; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; significance_line : float, optional; p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If ``None``, no line is added. Returns; -------; :class:`bokeh.models.Plot`; """"""; if locus is None:; locus = pvals._indices.source.locus. ref = locus.dtype.reference_genome. if hover_fields is None:; hover_fields = {}. hover_fields['locus'] = hail.str(locus). pvals = -hail.log10(pvals). source_pd = _collect_scatter_plot_data(; ('_global_locus', locus.global_position()),; ('_pval', pvals),; fields=hover_fields,; n_divisions=_downsampling_factor('manhattan', n_divisions, collect_all),; ); source_pd['p_value'] = [10 ** (-p) for p in source_pd['_pval']]; source_pd['_contig'] = [locus.split("":"")[0] for locus in source_pd['locus']]. observed_contigs = [contig for contig in ref.contigs.copy() if contig in set(source_pd['_contig'])]. contig_ticks = [ref.",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5419,Deployability,update,update,5419," is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5996,Deployability,update,update,5996,"e), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; f",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9467,Deployability,update,update,9467,"if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Labe",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9661,Deployability,update,update,9661,"l_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(d",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:13100,Deployability,update,update,13100,"cy'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='black',; ); if data.n_larger > 0:; p.quad(; bottom=0,; top=data.n_larger,; left=data.bin_edges[-1],; right=(data.bin_edges[-1] + (data.bin_edges[1] - data.bin_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; --",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:13711,Deployability,update,update,13711,"in_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_metho",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:22840,Deployability,update,update,22840," x_range[0]) / x_bins; y_spacing = (y_range[1] - y_range[0]) / y_bins. def frange(start, stop, step):; from itertools import count, takewhile. return takewhile(lambda x: x <= stop, count(start, step)). x_levels = hail.literal(list(frange(x_range[0], x_range[1], x_spacing))[::-1]); y_levels = hail.literal(list(frange(y_range[0], y_range[1], y_spacing))[::-1]); grouped_ht = source.group_by(; x=hail.str(x_levels.find(lambda w: x >= w)), y=hail.str(y_levels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinsta",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23287,Deployability,continuous,continuous,23287,"evels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, co",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23472,Deployability,update,update,23472,"& (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23573,Deployability,update,update,23573,"mericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23678,Deployability,update,update,23678,"divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes import viridis. _palette = viridis(n). return CategoricalColorMapper(factors=factors, palette=_palette). ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:30792,Deployability,continuous,continuous,30792,"Union[Plot, Column]:; """"""Create an interactive scatter plot. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be d",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:31818,Deployability,continuous,continuous,31818,"For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:34929,Deployability,update,update,34929,"eight=height, width=width); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; callback_args: Dict[str, Any]; callback_args = dict(color_mappers=sp_color_mappers, scatter_renderers=sp_scatter_renderers); callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); select = Select(title=""Color by"", value=label_cols[0], options=label_cols); select.js_on_change('value', callback); return Column(children=[select, sp]). return sp. @typecheck(; x=oneof(expr_numeric, sized_tupleof(str, expr_numeric)),; y=oneof(expr_numeric, sized_tupleof(str, expr_numeric)),; label=nullable(oneof(dictof(str, expr_any), expr_any)),; title=nullable(str),; xlabel=nullable(str),; ylabel=nullable(str),; size=int,; legend=bool,; hover_fields=nullable(dictof(str, expr_any)),; colors=nullable(oneof(bokeh.models.mappers.ColorMapper, dictof(str, bokeh.models.mappers.ColorMapper))),; width=int,; height=int,; collect_all=nullable(bool),; n_divisions=nullable(int),; missing_label=str,; ); def ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:37647,Deployability,continuous,continuous,37647,"t:; """"""Create an interactive scatter plot with marginal densities on the side. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. This function returns a :class:`bokeh.models.layouts.Column` containing two :class:`figure.Row`:; - The first row contains the X-axis marginal density and a selection widget if multiple entries are specified in the ``label``; - The second row contains the scatter plot and the y-axis marginal density. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:38721,Deployability,continuous,continuous,38721,"tems in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:44839,Deployability,update,update,44839,"range.end = y_max_densities[label_cols[0]]. callback_args: Dict[str, Any]; callback_args = dict(; scatter_renderers=sp_scatter_renderers,; color_mappers=sp_color_mappers,; density_renderers=x_renderers + y_renderers,; x_range=xp.y_range,; x_max_densities=x_max_densities,; y_range=yp.x_range,; y_max_densities=y_max_densities,; ). callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. for (var i = 0; i < density_renderers.length; i++){; density_renderers[i][2].visible = density_renderers[i][0] == cb_obj.value; }. x_range.start = 0; y_range.start = 0; x_range.end = x_max_densities[cb_obj.value]; y_range.end = y_max_densities[cb_obj.value]. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); select = Select(title=""Color by"", value=label_cols[0], options=label_cols); select.js_on_change('value', callback); first_row.append(select). return gridplot([first_row, [sp, yp]]). [docs]@typecheck(; pvals=expr_numeric,; label=nullable(oneof(dictof(str, expr_any), expr_any)),; title=nullable(str),; xlabel=nullable(str),; ylabel=nullable(str),; size=int,; legend=bool,; hover_fields=nullable(dictof(str, expr_any)),; colors=nullable(oneof(bokeh.models.mappers.ColorMapper, dictof(str, bokeh.models.mappers.ColorMapper))),; width=int,; height=int,; collect_all=nullable(bool),; n_divisions=nullable(int),; missing_label=str,; ); def qq(; pvals: NumericExpression,; label: Optional[Union[Expression, Dict[s",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:47198,Deployability,continuous,continuous,47198,"s: Optional[Union[ColorMapper, Dict[str, ColorMapper]]] = None,; width: int = 800,; height: int = 800,; collect_all: Optional[bool] = None,; n_divisions: Optional[int] = 500,; missing_label: str = 'NA',; ) -> Union[figure, Column]:; """"""Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot). If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive qq plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xla",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:48076,Deployability,continuous,continuous,48076,"docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by whi",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:59474,Deployability,update,updated,59474,"gate(is_defined=hail.agg.fraction(hail.is_defined(entry_field))); ); else:; mt = mt._select_all(; row_exprs={'_new_row_key': row_field}, entry_exprs={'is_defined': hail.is_defined(entry_field)}; ); ht = mt.localize_entries('entry_fields', 'phenos'); ht = ht.select(entry_fields=ht.entry_fields.map(lambda entry: entry.is_defined)); data = ht.entry_fields.collect(); if len(data) > 200:; warning(; f'Missingness dataset has {len(data)} rows. '; f'This may take {""a very long time"" if len(data) > 1000 else ""a few minutes""} to plot.'; ); rows = hail.str(ht._new_row_key).collect(). df = pd.DataFrame(data); df = df.rename(columns=dict(enumerate(columns))).rename(index=dict(enumerate(rows))); df.index.name = 'row'; df.columns.name = 'column'. df = pd.DataFrame(df.stack(), columns=['defined']).reset_index(). p = figure(; x_range=columns,; y_range=list(reversed(rows)),; x_axis_location=""above"",; width=plot_width,; height=plot_height,; toolbar_location='below',; tooltips=[('defined', '@defined'), ('row', '@row'), ('column', '@column')],; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_text_font_size = ""5pt""; p.axis.major_label_standoff = 0; colors = [""#75968f"", ""#a5bab7"", ""#c9d9d3"", ""#e2e2e2"", ""#dfccce"", ""#ddb7b1"", ""#cc7878"", ""#933b41"", ""#550b1d""]; from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter. mapper = LinearColorMapper(palette=colors, low=df.defined.min(), high=df.defined.max()). p.rect(; x='column',; y='row',; width=1,; height=1,; source=df,; fill_color={'field': 'defined', 'transform': mapper},; line_color=None,; ). color_bar = ColorBar(; color_mapper=mapper,; major_label_text_font_size=""5pt"",; ticker=BasicTicker(desired_num_ticks=len(colors)),; formatter=PrintfTickFormatter(format=""%d""),; label_standoff=6,; border_line_color=None,; location=(0, 0),; ); p.add_layout(color_bar, 'right'); return p.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9159,Energy Efficiency,power,power,9159,"g.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the res",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9179,Energy Efficiency,power,power,9179,"nce(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggreg",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:12776,Energy Efficiency,green,green,12776," with height 0, those cannot be log transformed and were left as 0s.""; ). changes = {; ""bin_freq"": bin_freq,; ""n_larger"": math.log10(data.n_larger) if data.n_larger > 0.0 else data.n_larger,; ""n_smaller"": math.log10(data.n_smaller) if data.n_smaller > 0.0 else data.n_smaller,; }; data = data.annotate(**changes); y_axis_label = 'log10 Frequency'; else:; y_axis_label = 'Frequency'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='black',; ); if data.n_larger > 0:; p.quad(; bottom=0,; top=data.n_larger,; left=data.bin_edges[-1],; right=(data.bin_edges[-1] + (data.bin_edges[1] - data.bin_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_inte",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:17232,Modifiability,variab,variable,17232,"s.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size = font_size; return p. [docs]@typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; title=nullable(str),; width=int,; height=int,; colors=sequenceof(str),; log=bool,; ); def histogram2d(; x: NumericExpression,; y: NumericExpression,; bins: int = 40,; range: Optional[Tuple[int, int]] = None,; title: Optional[str] = None,; width: int = 600,; height: int = 600,; colors: Sequence[str] = bokeh.palettes.all_palettes['Blues'][7][::-1],; log: bool = False,; ) -> figure:; """"""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be consid",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:41795,Modifiability,extend,extend,41795,"idth); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). continuous_cols = [; col; for col in label_cols; if (str(source_pd.dtypes[col]).startswith('float') or str(source_pd.dtypes[col]).startswith('int')); ]; factor_cols = [col for col in label_cols if col not in continuous_cols]. # Density plots; def get_density_plot_items(; source_pd,; data_col,; p,; x_axis,; colors: Optional[Dict[str, ColorMapper]],; continuous_cols: List[str],; factor_cols: List[str],; ):; density_renderers = []; max_densities = {}; if not factor_cols or continuous_cols:; dens, edges = np.histogram(source_pd[data_col], density=True); edges = edges[:-1]; xy = (edges, dens) if x_axis else (dens, edges); cds = ColumnDataSource({'x': xy[0], 'y': xy[1]}); line = p.line('x', 'y', source=cds); density_renderers.extend([(col, """", line) for col in continuous_cols]); max_densities = {col: np.max(dens) for col in continuous_cols}. for factor_col in factor_cols:; assert colors is not None, (colors, factor_cols); factor_colors = colors.get(factor_col, _get_categorical_palette(list(set(source_pd[factor_col])))); factor_colors = dict(zip(factor_colors.factors, factor_colors.palette)); density_data = (; source_pd[[factor_col, data_col]]; .groupby(factor_col); .apply(lambda df: np.histogram(df['x' if x_axis else 'y'], density=True)); ); for factor, (dens, edges) in density_data.iteritems():; _edges = edges[:-1]; xy = (_edges, dens) if x_axis else (dens, _edges); cds = ColumnDataSource({'x': xy[0], 'y': xy[1]}); density_renderers.append((; factor_col,; factor,; p.line('x', 'y', color=factor_colors.get(factor, 'gray'), source=cds),; )); max_densities[factor_col] = np.max([*list(dens), max_densities.get(factor_col, 0)]). p.grid.visible = False; p.outline_line_color = None; return p, density_renderers, max_densities. xp = figure(title=title, hei",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:2599,Testability,log,log,2599,"able import MatrixTable; from hail.table import Table; from hail.typecheck import dictof, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils.java import warning; from hail.utils.struct import Struct. palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']. [docs]def output_notebook():; """"""Configure the Bokeh output state to generate output in notebook; cells when :func:`bokeh.io.show` is called. Calls; :func:`bokeh.io.output_notebook`. """"""; bokeh.io.output_notebook(). def show(obj, interact=None):; """"""Immediately display a Bokeh object or application. Calls; :func:`bokeh.io.show`. Parameters; ----------; obj; A Bokeh object to display.; interact; A handle returned by a plotting method with `interactive=True`.; """"""; if interact is None:; bokeh.io.show(obj); else:; handle = bokeh.io.show(obj, notebook_handle=True); interact(handle). [docs]def cdf(data, k=350, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". if normalize:; y_axis_label = 'Quantile'; else:; y_axis_label = 'Rank'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; background_fill_colo",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:2986,Testability,log,log,2986,"generate output in notebook; cells when :func:`bokeh.io.show` is called. Calls; :func:`bokeh.io.output_notebook`. """"""; bokeh.io.output_notebook(). def show(obj, interact=None):; """"""Immediately display a Bokeh object or application. Calls; :func:`bokeh.io.show`. Parameters; ----------; obj; A Bokeh object to display.; interact; A handle returned by a plotting method with `interactive=True`.; """"""; if interact is None:; bokeh.io.show(obj); else:; handle = bokeh.io.show(obj, notebook_handle=True); interact(handle). [docs]def cdf(data, k=350, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". if normalize:; y_axis_label = 'Quantile'; else:; y_axis_label = 'Rank'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; background_fill_color='#EEEEEE',; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; ); p.add_tools(HoverTool(tooltips=[(""value"", ""$x""), (""rank"", ""@top"")], mode='vline')). ranks = np.array(data.ranks); values = np.array(data['values']); if normalize:; ranks = ranks / ranks[-1]. # invisible, there to support tooltips; p.quad(top=ranks[1:-1], bottom=ranks[1:-1], left=values[:-1], right=values[1:], fill_alpha=0, line_alpha=",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:3041,Testability,log,log,3041,"generate output in notebook; cells when :func:`bokeh.io.show` is called. Calls; :func:`bokeh.io.output_notebook`. """"""; bokeh.io.output_notebook(). def show(obj, interact=None):; """"""Immediately display a Bokeh object or application. Calls; :func:`bokeh.io.show`. Parameters; ----------; obj; A Bokeh object to display.; interact; A handle returned by a plotting method with `interactive=True`.; """"""; if interact is None:; bokeh.io.show(obj); else:; handle = bokeh.io.show(obj, notebook_handle=True); interact(handle). [docs]def cdf(data, k=350, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". if normalize:; y_axis_label = 'Quantile'; else:; y_axis_label = 'Rank'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; background_fill_color='#EEEEEE',; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; ); p.add_tools(HoverTool(tooltips=[(""value"", ""$x""), (""rank"", ""@top"")], mode='vline')). ranks = np.array(data.ranks); values = np.array(data['values']); if normalize:; ranks = ranks / ranks[-1]. # invisible, there to support tooltips; p.quad(top=ranks[1:-1], bottom=ranks[1:-1], left=values[:-1], right=values[1:], fill_alpha=0, line_alpha=",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:3388,Testability,log,log,3388,"ok_handle=True); interact(handle). [docs]def cdf(data, k=350, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". if normalize:; y_axis_label = 'Quantile'; else:; y_axis_label = 'Rank'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; background_fill_color='#EEEEEE',; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; ); p.add_tools(HoverTool(tooltips=[(""value"", ""$x""), (""rank"", ""@top"")], mode='vline')). ranks = np.array(data.ranks); values = np.array(data['values']); if normalize:; ranks = ranks / ranks[-1]. # invisible, there to support tooltips; p.quad(top=ranks[1:-1], bottom=ranks[1:-1], left=values[:-1], right=values[1:], fill_alpha=0, line_alpha=0); p.step(x=[*values, values[-1]], y=ranks, line_width=2, line_color='black', legend_label=legend); return p. [docs]def pdf(; data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label =",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:3409,Testability,log,log,3409,"ok_handle=True); interact(handle). [docs]def cdf(data, k=350, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". if normalize:; y_axis_label = 'Quantile'; else:; y_axis_label = 'Rank'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; background_fill_color='#EEEEEE',; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; ); p.add_tools(HoverTool(tooltips=[(""value"", ""$x""), (""rank"", ""@top"")], mode='vline')). ranks = np.array(data.ranks); values = np.array(data['values']); if normalize:; ranks = ranks / ranks[-1]. # invisible, there to support tooltips; p.quad(top=ranks[1:-1], bottom=ranks[1:-1], left=values[:-1], right=values[1:], fill_alpha=0, line_alpha=0); p.step(x=[*values, values[-1]], y=ranks, line_width=2, line_color='black', legend_label=legend); return p. [docs]def pdf(; data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label =",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:4195,Testability,log,log,4195,"ion_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". if normalize:; y_axis_label = 'Quantile'; else:; y_axis_label = 'Rank'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; background_fill_color='#EEEEEE',; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; ); p.add_tools(HoverTool(tooltips=[(""value"", ""$x""), (""rank"", ""@top"")], mode='vline')). ranks = np.array(data.ranks); values = np.array(data['values']); if normalize:; ranks = ranks / ranks[-1]. # invisible, there to support tooltips; p.quad(top=ranks[1:-1], bottom=ranks[1:-1], left=values[:-1], right=values[1:], fill_alpha=0, line_alpha=0); p.step(x=[*values, values[-1]], y=ranks, line_width=2, line_color='black', legend_label=legend); return p. [docs]def pdf(; data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:4514,Testability,log,log,4514,"; active_scroll='xwheel_zoom',; ); p.add_tools(HoverTool(tooltips=[(""value"", ""$x""), (""rank"", ""@top"")], mode='vline')). ranks = np.array(data.ranks); values = np.array(data['values']); if normalize:; ranks = ranks / ranks[-1]. # invisible, there to support tooltips; p.quad(top=ranks[1:-1], bottom=ranks[1:-1], left=values[:-1], right=values[1:], fill_alpha=0, line_alpha=0); p.step(x=[*values, values[-1]], y=ranks, line_width=2, line_color='black', legend_label=legend); return p. [docs]def pdf(; data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:4535,Testability,log,log,4535,"; active_scroll='xwheel_zoom',; ); p.add_tools(HoverTool(tooltips=[(""value"", ""$x""), (""rank"", ""@top"")], mode='vline')). ranks = np.array(data.ranks); values = np.array(data['values']); if normalize:; ranks = ranks / ranks[-1]. # invisible, there to support tooltips; p.quad(top=ranks[1:-1], bottom=ranks[1:-1], left=values[:-1], right=values[1:], fill_alpha=0, line_alpha=0); p.step(x=[*values, values[-1]], y=ranks, line_width=2, line_color='black', legend_label=legend); return p. [docs]def pdf(; data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5166,Testability,log,log,5166,"onfidence=5, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5670,Testability,log,log,5670,",; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] -",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:7512,Testability,log,log,7512," = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; fy = y[ui] + e; new_y[ui] = fy; keep[ui] = True; j = ui + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; if j >= len(x):; break; if compare(udx, udy, dx, judy) < 0:; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; if compare(ldx, ldy, dx, jldy) > 0:; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; j += 1; return new_y, keep. [docs]def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:7887,Testability,log,log,7887,"; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; fy = y[ui] + e; new_y[ui] = fy; keep[ui] = True; j = ui + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; if j >= len(x):; break; if compare(udx, udy, dx, judy) < 0:; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; if compare(ldx, ldy, dx, jldy) > 0:; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; j += 1; return new_y, keep. [docs]def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:8428,Testability,log,log,8428,"def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:8449,Testability,log,log,8449,"def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9900,Testability,log,log,9900,"[1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; if interactive:; raise ValueError(""'interactive' flag can only be used on data from 'approx_cdf'.""); agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; e",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:10001,Testability,log,log,10001,"[1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; if interactive:; raise ValueError(""'interactive' flag can only be used on data from 'approx_cdf'.""); agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; e",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:10551,Testability,log,log,10551,"final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; if interactive:; raise ValueError(""'interactive' flag can only be used on data from 'approx_cdf'.""); agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; finite_data = hail.bind(lambda x: hail.case().when(hail.is_finite(x), x).or_missing(), data); start, end = agg_f((aggregators.min(finite_data), aggregators.max(finite_data))); if start is None and end is None:; raise ValueError(""'data' contains no values that are defined and finite""); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'); elif 'values' in data:; cdf = data; hist, edges = np.histogram(cdf['values'], bins=bins, weights=np.diff(cdf.ranks), density=True); data = Struct(bin_freq=hist, bin_edges=edges, n_larger=0, n_smaller=0). if legend is None:; legend = """". if log:; bin_freq = []; cou",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:11549,Testability,log,log,11549,"g10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; if interactive:; raise ValueError(""'interactive' flag can only be used on data from 'approx_cdf'.""); agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; finite_data = hail.bind(lambda x: hail.case().when(hail.is_finite(x), x).or_missing(), data); start, end = agg_f((aggregators.min(finite_data), aggregators.max(finite_data))); if start is None and end is None:; raise ValueError(""'data' contains no values that are defined and finite""); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'); elif 'values' in data:; cdf = data; hist, edges = np.histogram(cdf['values'], bins=bins, weights=np.diff(cdf.ranks), density=True); data = Struct(bin_freq=hist, bin_edges=edges, n_larger=0, n_smaller=0). if legend is None:; legend = """". if log:; bin_freq = []; count_problems = 0; for x in data.bin_freq:; if x == 0.0:; count_problems += 1; bin_freq.append(x); else:; bin_freq.append(math.log10(x)). if count_problems > 0:; warning(; f""There were {count_problems} bins with height 0, those cannot be log transformed and were left as 0s.""; ). changes = {; ""bin_freq"": bin_freq,; ""n_larger"": math.log10(data.n_larger) if data.n_larger > 0.0 else data.n_larger,; ""n_smaller"": math.log10(data.n_smaller) if data.n_smaller > 0.0 else data.n_smaller,; }; data = data.annotate(**changes); y_axis_label = 'log10 Frequency'; else:; y_axis_label = 'Frequency'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='blac",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:11809,Testability,log,log,11809,"g can only be used on data from 'approx_cdf'.""); agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; finite_data = hail.bind(lambda x: hail.case().when(hail.is_finite(x), x).or_missing(), data); start, end = agg_f((aggregators.min(finite_data), aggregators.max(finite_data))); if start is None and end is None:; raise ValueError(""'data' contains no values that are defined and finite""); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'); elif 'values' in data:; cdf = data; hist, edges = np.histogram(cdf['values'], bins=bins, weights=np.diff(cdf.ranks), density=True); data = Struct(bin_freq=hist, bin_edges=edges, n_larger=0, n_smaller=0). if legend is None:; legend = """". if log:; bin_freq = []; count_problems = 0; for x in data.bin_freq:; if x == 0.0:; count_problems += 1; bin_freq.append(x); else:; bin_freq.append(math.log10(x)). if count_problems > 0:; warning(; f""There were {count_problems} bins with height 0, those cannot be log transformed and were left as 0s.""; ). changes = {; ""bin_freq"": bin_freq,; ""n_larger"": math.log10(data.n_larger) if data.n_larger > 0.0 else data.n_larger,; ""n_smaller"": math.log10(data.n_smaller) if data.n_smaller > 0.0 else data.n_smaller,; }; data = data.annotate(**changes); y_axis_label = 'log10 Frequency'; else:; y_axis_label = 'Frequency'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='black',; ); if data.n_larger > 0:; p.quad(; bottom=0,; top=data.n_larger,; left=data.bin_edges[-1],; right=(data.bin_edges[-1] + (data.bin_edges[1] - data.bin_edges[0])),; line_color='black',; fill_color='g",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:13977,Testability,log,log,13977,"ges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; start, end = agg_f((aggregators.min(data), aggregators.max(data))); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'). if lege",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:14084,Testability,log,log,14084,"ges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; start, end = agg_f((aggregators.min(data), aggregators.max(data))); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'). if lege",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:14510,Testability,log,log,14510,"[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; start, end = agg_f((aggregators.min(data), aggregators.max(data))); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'). if legend is None:; legend = """". cumulative_data = np.cumsum(data.bin_freq) + data.n_smaller; np.append(cumulative_data, [cumulative_data[-1] + data.n_larger]); num_data_points = max(cumulative_data). if normalize:; cumulative_data = cumulative_data / num_data_points; if title is not None:; title = f'{title} ({num_data_points:,} data points)'; if log:; p = figure(; title=title,; x_axis_label=legend,; y_axis_label='Frequency',; background_fill_color='#EEEEEE',; y_axis_type='log',; ); else:; p = figure(title=title, x_axis_label=legend, y_axis_label='Frequency', background",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:14565,Testability,log,log,14565,"[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; start, end = agg_f((aggregators.min(data), aggregators.max(data))); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'). if legend is None:; legend = """". cumulative_data = np.cumsum(data.bin_freq) + data.n_smaller; np.append(cumulative_data, [cumulative_data[-1] + data.n_larger]); num_data_points = max(cumulative_data). if normalize:; cumulative_data = cumulative_data / num_data_points; if title is not None:; title = f'{title} ({num_data_points:,} data points)'; if log:; p = figure(; title=title,; x_axis_label=legend,; y_axis_label='Frequency',; background_fill_color='#EEEEEE',; y_axis_type='log',; ); else:; p = figure(title=title, x_axis_label=legend, y_axis_label='Frequency', background",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:15312,Testability,log,log,15312,"r; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; start, end = agg_f((aggregators.min(data), aggregators.max(data))); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'). if legend is None:; legend = """". cumulative_data = np.cumsum(data.bin_freq) + data.n_smaller; np.append(cumulative_data, [cumulative_data[-1] + data.n_larger]); num_data_points = max(cumulative_data). if normalize:; cumulative_data = cumulative_data / num_data_points; if title is not None:; title = f'{title} ({num_data_points:,} data points)'; if log:; p = figure(; title=title,; x_axis_label=legend,; y_axis_label='Frequency',; background_fill_color='#EEEEEE',; y_axis_type='log',; ); else:; p = figure(title=title, x_axis_label=legend, y_axis_label='Frequency', background_fill_color='#EEEEEE'); p.line(data.bin_edges[:-1], cumulative_data, line_color='#036564', line_width=3); return p. @typecheck(p=figure, font_size=str); def set_font_size(p, font_size: str = '12pt'):; """"""Set most of the font sizes in a bokeh figure. Parameters; ----------; p : :class:`bokeh.plotting.figure`; Input figure.; font_size : str; String of font size in points (e.g. '12pt'). Returns; -------; :class:`bokeh.plotting.figure`; """"""; p.legend.label_text_font_size = font_size; p.xaxis.axis_label_text_font_size = font_size; p.yaxis.axis_label_text_font_size = font_size; p.xaxis.major_label_text_font_size = font_size; p.yaxis.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:15441,Testability,log,log,15441,"r; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_method(); if range is not None:; start = range[0]; end = range[1]; else:; start, end = agg_f((aggregators.min(data), aggregators.max(data))); data = agg_f(aggregators.hist(data, start, end, bins)); else:; raise ValueError('Invalid input'). if legend is None:; legend = """". cumulative_data = np.cumsum(data.bin_freq) + data.n_smaller; np.append(cumulative_data, [cumulative_data[-1] + data.n_larger]); num_data_points = max(cumulative_data). if normalize:; cumulative_data = cumulative_data / num_data_points; if title is not None:; title = f'{title} ({num_data_points:,} data points)'; if log:; p = figure(; title=title,; x_axis_label=legend,; y_axis_label='Frequency',; background_fill_color='#EEEEEE',; y_axis_type='log',; ); else:; p = figure(title=title, x_axis_label=legend, y_axis_label='Frequency', background_fill_color='#EEEEEE'); p.line(data.bin_edges[:-1], cumulative_data, line_color='#036564', line_width=3); return p. @typecheck(p=figure, font_size=str); def set_font_size(p, font_size: str = '12pt'):; """"""Set most of the font sizes in a bokeh figure. Parameters; ----------; p : :class:`bokeh.plotting.figure`; Input figure.; font_size : str; String of font size in points (e.g. '12pt'). Returns; -------; :class:`bokeh.plotting.figure`; """"""; p.legend.label_text_font_size = font_size; p.xaxis.axis_label_text_font_size = font_size; p.yaxis.axis_label_text_font_size = font_size; p.xaxis.major_label_text_font_size = font_size; p.yaxis.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:16666,Testability,log,log,16666," return p. @typecheck(p=figure, font_size=str); def set_font_size(p, font_size: str = '12pt'):; """"""Set most of the font sizes in a bokeh figure. Parameters; ----------; p : :class:`bokeh.plotting.figure`; Input figure.; font_size : str; String of font size in points (e.g. '12pt'). Returns; -------; :class:`bokeh.plotting.figure`; """"""; p.legend.label_text_font_size = font_size; p.xaxis.axis_label_text_font_size = font_size; p.yaxis.axis_label_text_font_size = font_size; p.xaxis.major_label_text_font_size = font_size; p.yaxis.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size = font_size; return p. [docs]@typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; title=nullable(str),; width=int,; height=int,; colors=sequenceof(str),; log=bool,; ); def histogram2d(; x: NumericExpression,; y: NumericExpression,; bins: int = 40,; range: Optional[Tuple[int, int]] = None,; title: Optional[str] = None,; width: int = 600,; height: int = 600,; colors: Sequence[str] = bokeh.palettes.all_palettes['Blues'][7][::-1],; log: bool = False,; ) -> figure:; """"""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:16944,Testability,log,log,16944,"ting.figure`; """"""; p.legend.label_text_font_size = font_size; p.xaxis.axis_label_text_font_size = font_size; p.yaxis.axis_label_text_font_size = font_size; p.xaxis.major_label_text_font_size = font_size; p.yaxis.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size = font_size; return p. [docs]@typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; title=nullable(str),; width=int,; height=int,; colors=sequenceof(str),; log=bool,; ); def histogram2d(; x: NumericExpression,; y: NumericExpression,; bins: int = 40,; range: Optional[Tuple[int, int]] = None,; title: Optional[str] = None,; width: int = 600,; height: int = 600,; colors: Sequence[str] = bokeh.palettes.all_palettes['Blues'][7][::-1],; log: bool = False,; ) -> figure:; """"""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:18735,Testability,log,log,18735,".; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; data = _generate_hist2d_data(x, y, bins, range).to_pandas(). # Use python prettier float -> str function; data['x'] = data['x'].apply(lambda e: str(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_standoff = 0; import math. p.xaxis.major_label_orientation = math.pi / 3. p.rect(; x='x",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:19074,Testability,log,log,19074,"sion:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; data = _generate_hist2d_data(x, y, bins, range).to_pandas(). # Use python prettier float -> str function; data['x'] = data['x'].apply(lambda e: str(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_standoff = 0; import math. p.xaxis.major_label_orientation = math.pi / 3. p.rect(; x='x', y='y', width=1, height=1, source=data, fill_color={'field': 'c', 'transform': mapper}, line_color=None; ). color_bar = ColorBar(; color_mapper=mapper,; ticker=LogTicker(desired_num_ticks=len(colors)) if log else BasicTicker(desired_num_ticks=len(colors)),; label_standoff=12 if log else 6,; border_line_color=None,; location=(0, ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:19963,Testability,log,log,19963,"r(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_standoff = 0; import math. p.xaxis.major_label_orientation = math.pi / 3. p.rect(; x='x', y='y', width=1, height=1, source=data, fill_color={'field': 'c', 'transform': mapper}, line_color=None; ). color_bar = ColorBar(; color_mapper=mapper,; ticker=LogTicker(desired_num_ticks=len(colors)) if log else BasicTicker(desired_num_ticks=len(colors)),; label_standoff=12 if log else 6,; border_line_color=None,; location=(0, 0),; ); p.add_layout(color_bar, 'right'). hovertool = p.select_one(HoverTool); assert hovertool is not None; hovertool.tooltips = [; ('x', '@x'),; (; 'y',; '@y',; ),; ('count', '@c'),; ]. return p. @typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; ); def _generate_hist2d_data(x, y, bins, range):; source = x._indices.source; y_source = y._indices.source; if source is None or y_source is None:; raise ValueError(""histogram_2d expects two expressions of 'Table', found scalar expression""); if isinstance(source, hail.MatrixTable):; raise ValueError(""histogram_2d requires source to be Table, not MatrixTable""); if source != y_source:; raise ValueError(f""histogram_2d expects two expressions from the same 'Table', found {source} and {y_source}""",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:20038,Testability,log,log,20038,"r(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_standoff = 0; import math. p.xaxis.major_label_orientation = math.pi / 3. p.rect(; x='x', y='y', width=1, height=1, source=data, fill_color={'field': 'c', 'transform': mapper}, line_color=None; ). color_bar = ColorBar(; color_mapper=mapper,; ticker=LogTicker(desired_num_ticks=len(colors)) if log else BasicTicker(desired_num_ticks=len(colors)),; label_standoff=12 if log else 6,; border_line_color=None,; location=(0, 0),; ); p.add_layout(color_bar, 'right'). hovertool = p.select_one(HoverTool); assert hovertool is not None; hovertool.tooltips = [; ('x', '@x'),; (; 'y',; '@y',; ),; ('count', '@c'),; ]. return p. @typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; ); def _generate_hist2d_data(x, y, bins, range):; source = x._indices.source; y_source = y._indices.source; if source is None or y_source is None:; raise ValueError(""histogram_2d expects two expressions of 'Table', found scalar expression""); if isinstance(source, hail.MatrixTable):; raise ValueError(""histogram_2d requires source to be Table, not MatrixTable""); if source != y_source:; raise ValueError(f""histogram_2d expects two expressions from the same 'Table', found {source} and {y_source}""",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:20168,Testability,assert,assert,20168,"nearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_standoff = 0; import math. p.xaxis.major_label_orientation = math.pi / 3. p.rect(; x='x', y='y', width=1, height=1, source=data, fill_color={'field': 'c', 'transform': mapper}, line_color=None; ). color_bar = ColorBar(; color_mapper=mapper,; ticker=LogTicker(desired_num_ticks=len(colors)) if log else BasicTicker(desired_num_ticks=len(colors)),; label_standoff=12 if log else 6,; border_line_color=None,; location=(0, 0),; ); p.add_layout(color_bar, 'right'). hovertool = p.select_one(HoverTool); assert hovertool is not None; hovertool.tooltips = [; ('x', '@x'),; (; 'y',; '@y',; ),; ('count', '@c'),; ]. return p. @typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; ); def _generate_hist2d_data(x, y, bins, range):; source = x._indices.source; y_source = y._indices.source; if source is None or y_source is None:; raise ValueError(""histogram_2d expects two expressions of 'Table', found scalar expression""); if isinstance(source, hail.MatrixTable):; raise ValueError(""histogram_2d requires source to be Table, not MatrixTable""); if source != y_source:; raise ValueError(f""histogram_2d expects two expressions from the same 'Table', found {source} and {y_source}""); raise_unless_row_indexed('histogram_2d', x); raise_unless_row_indexed('histogram_2d', y); if isinstance(bins, int):; x_bins = y_bins = bins; else:; x_bins, y_bins = bins; if range is None:; x",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23253,Testability,log,logic,23253,"evels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, co",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:33389,Testability,assert,assert,33389,"al; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.models.Plot` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""; hover_fields = {} if hover_fields is None else hover_fields. label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors. label_cols = list(label_by_col.keys()); if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divisions=_downsampling_factor('scatter', n_divisions, collect_all),; missing_label=missing_label,; ); sp = figure(title=title, x_axis_label=xlabel, y_axis_label=ylabel, height=height, width=width); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:34256,Testability,assert,assert,34256,"s None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors. label_cols = list(label_by_col.keys()); if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divisions=_downsampling_factor('scatter', n_divisions, collect_all),; missing_label=missing_label,; ); sp = figure(title=title, x_axis_label=xlabel, y_axis_label=ylabel, height=height, width=width); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; callback_args: Dict[str, Any]; callback_args = dict(color_mappers=sp_color_mappers, scatter_renderers=sp_scatter_renderers); callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); s",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:34286,Testability,assert,assert,34286,"s None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors. label_cols = list(label_by_col.keys()); if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divisions=_downsampling_factor('scatter', n_divisions, collect_all),; missing_label=missing_label,; ); sp = figure(title=title, x_axis_label=xlabel, y_axis_label=ylabel, height=height, width=width); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; callback_args: Dict[str, Any]; callback_args = dict(color_mappers=sp_color_mappers, scatter_renderers=sp_scatter_renderers); callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); s",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:40212,Testability,assert,assert,40212,"ers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`.GridPlot`; """"""; # Collect data; hover_fields = {} if hover_fields is None else hover_fields. label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors; if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. label_cols = list(label_by_col.keys()); source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divisions=_downsampling_factor('join_plot', n_divisions, collect_all),; missing_label=missing_label,; ); sp = figure(title=title, x_axis_label=xlabel, y_axis_label=ylabel, height=height, width=width); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). continuous_cols = [; col; for col in label_cols; if (str(source_pd.dtypes[col]).startswith('float",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:41945,Testability,assert,assert,41945,"ce_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). continuous_cols = [; col; for col in label_cols; if (str(source_pd.dtypes[col]).startswith('float') or str(source_pd.dtypes[col]).startswith('int')); ]; factor_cols = [col for col in label_cols if col not in continuous_cols]. # Density plots; def get_density_plot_items(; source_pd,; data_col,; p,; x_axis,; colors: Optional[Dict[str, ColorMapper]],; continuous_cols: List[str],; factor_cols: List[str],; ):; density_renderers = []; max_densities = {}; if not factor_cols or continuous_cols:; dens, edges = np.histogram(source_pd[data_col], density=True); edges = edges[:-1]; xy = (edges, dens) if x_axis else (dens, edges); cds = ColumnDataSource({'x': xy[0], 'y': xy[1]}); line = p.line('x', 'y', source=cds); density_renderers.extend([(col, """", line) for col in continuous_cols]); max_densities = {col: np.max(dens) for col in continuous_cols}. for factor_col in factor_cols:; assert colors is not None, (colors, factor_cols); factor_colors = colors.get(factor_col, _get_categorical_palette(list(set(source_pd[factor_col])))); factor_colors = dict(zip(factor_colors.factors, factor_colors.palette)); density_data = (; source_pd[[factor_col, data_col]]; .groupby(factor_col); .apply(lambda df: np.histogram(df['x' if x_axis else 'y'], density=True)); ); for factor, (dens, edges) in density_data.iteritems():; _edges = edges[:-1]; xy = (_edges, dens) if x_axis else (dens, _edges); cds = ColumnDataSource({'x': xy[0], 'y': xy[1]}); density_renderers.append((; factor_col,; factor,; p.line('x', 'y', color=factor_colors.get(factor, 'gray'), source=cds),; )); max_densities[factor_col] = np.max([*list(dens), max_densities.get(factor_col, 0)]). p.grid.visible = False; p.outline_line_color = None; return p, density_renderers, max_densities. xp = figure(title=title, height=int(height / 3), width=width, x_range=sp.x_range); xp, x_renderers, x_max_densities = get_density_plot_items(; source_pd,; _x[",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:43470,Testability,assert,assert,43470," density_renderers.append((; factor_col,; factor,; p.line('x', 'y', color=factor_colors.get(factor, 'gray'), source=cds),; )); max_densities[factor_col] = np.max([*list(dens), max_densities.get(factor_col, 0)]). p.grid.visible = False; p.outline_line_color = None; return p, density_renderers, max_densities. xp = figure(title=title, height=int(height / 3), width=width, x_range=sp.x_range); xp, x_renderers, x_max_densities = get_density_plot_items(; source_pd,; _x[0],; xp,; x_axis=True,; colors=sp_color_mappers,; continuous_cols=continuous_cols,; factor_cols=factor_cols,; ); xp.xaxis.visible = False; yp = figure(height=height, width=int(width / 3), y_range=sp.y_range); yp, y_renderers, y_max_densities = get_density_plot_items(; source_pd,; _y[0],; yp,; x_axis=False,; colors=sp_color_mappers,; continuous_cols=continuous_cols,; factor_cols=factor_cols,; ); yp.yaxis.visible = False; density_renderers = x_renderers + y_renderers; first_row = [xp]. if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; for factor_col, _, renderer in density_renderers:; renderer.visible = factor_col == label_cols[0]. if label_cols[0] in factor_cols:; xp.y_range.start = 0; xp.y_range.end = x_max_densities[label_cols[0]]; yp.x_range.start = 0; yp.x_range.end = y_max_densities[label_cols[0]]. callback_args: Dict[str, Any]; callback_args = dict(; scatter_renderers=sp_scatter_renderers,; color_mappers=sp_color_mappers,; density_renderers=x_renderers + y_renderers,; x_range=xp.y_range,; x_max_densities=x_max_densities,; y_range=yp.x_range,; y_max_densities=y_max_densities,; ). callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_ob",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:43500,Testability,assert,assert,43500," density_renderers.append((; factor_col,; factor,; p.line('x', 'y', color=factor_colors.get(factor, 'gray'), source=cds),; )); max_densities[factor_col] = np.max([*list(dens), max_densities.get(factor_col, 0)]). p.grid.visible = False; p.outline_line_color = None; return p, density_renderers, max_densities. xp = figure(title=title, height=int(height / 3), width=width, x_range=sp.x_range); xp, x_renderers, x_max_densities = get_density_plot_items(; source_pd,; _x[0],; xp,; x_axis=True,; colors=sp_color_mappers,; continuous_cols=continuous_cols,; factor_cols=factor_cols,; ); xp.xaxis.visible = False; yp = figure(height=height, width=int(width / 3), y_range=sp.y_range); yp, y_renderers, y_max_densities = get_density_plot_items(; source_pd,; _y[0],; yp,; x_axis=False,; colors=sp_color_mappers,; continuous_cols=continuous_cols,; factor_cols=factor_cols,; ); yp.yaxis.visible = False; density_renderers = x_renderers + y_renderers; first_row = [xp]. if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; for factor_col, _, renderer in density_renderers:; renderer.visible = factor_col == label_cols[0]. if label_cols[0] in factor_cols:; xp.y_range.start = 0; xp.y_range.end = x_max_densities[label_cols[0]]; yp.x_range.start = 0; yp.x_range.end = y_max_densities[label_cols[0]]. callback_args: Dict[str, Any]; callback_args = dict(; scatter_renderers=sp_scatter_renderers,; color_mappers=sp_color_mappers,; density_renderers=x_renderers + y_renderers,; x_range=xp.y_range,; x_max_densities=x_max_densities,; y_range=yp.x_range,; y_max_densities=y_max_densities,; ). callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_ob",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:49641,Testability,assert,assert,49641,"`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.plotting.figure` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""; hover_fields = {} if hover_fields is None else hover_fields; label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. source = pvals._indices.source; if isinstance(source, Table):; ht = source.select(p_value=pvals, **hover_fields, **label_by_col); else:; assert isinstance(source, MatrixTable); ht = source.select_rows(p_value=pvals, **hover_fields, **label_by_col).rows(); ht = ht.key_by().select('p_value', *hover_fields, *label_by_col).key_by('p_value'); n = ht.aggregate(aggregators.count(), _localize=False); ht = ht.annotate(observed_p=-hail.log10(ht['p_value']), expected_p=-hail.log10((hail.scan.count() + 1) / n)); if 'p' not in hover_fields:; hover_fields['p_value'] = ht['p_value']; p = scatter(; ht.expected_p,; ht.observed_p,; label={x: ht[x] for x in label_by_col},; title=title,; xlabel=xlabel,; ylabel=ylabel,; size=size,; legend=legend,; hover_fields={x: ht[x] for x in hover_fields},; colors=colors,; width=width,; height=height,; n_divisions=_downsampling_fa",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:49832,Testability,assert,assert,49832,"l, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.plotting.figure` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""; hover_fields = {} if hover_fields is None else hover_fields; label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. source = pvals._indices.source; if isinstance(source, Table):; ht = source.select(p_value=pvals, **hover_fields, **label_by_col); else:; assert isinstance(source, MatrixTable); ht = source.select_rows(p_value=pvals, **hover_fields, **label_by_col).rows(); ht = ht.key_by().select('p_value', *hover_fields, *label_by_col).key_by('p_value'); n = ht.aggregate(aggregators.count(), _localize=False); ht = ht.annotate(observed_p=-hail.log10(ht['p_value']), expected_p=-hail.log10((hail.scan.count() + 1) / n)); if 'p' not in hover_fields:; hover_fields['p_value'] = ht['p_value']; p = scatter(; ht.expected_p,; ht.observed_p,; label={x: ht[x] for x in label_by_col},; title=title,; xlabel=xlabel,; ylabel=ylabel,; size=size,; legend=legend,; hover_fields={x: ht[x] for x in hover_fields},; colors=colors,; width=width,; height=height,; n_divisions=_downsampling_factor('qq', n_divisions, collect_all),; missing_label=missing_label,; ); from hail.methods.statgen import _lambda_gc_agg. lambda_gc, max_p = ht.aggregate((; _lambda_gc_agg(ht['p_value']),; hail.agg.max(hail.max(ht.observed_p, ht.expected_p)),; )); if isinstance(p, Column):;",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/plot/plots.html:54042,Testability,assert,assert,54042,"reference_genome. if hover_fields is None:; hover_fields = {}. hover_fields['locus'] = hail.str(locus). pvals = -hail.log10(pvals). source_pd = _collect_scatter_plot_data(; ('_global_locus', locus.global_position()),; ('_pval', pvals),; fields=hover_fields,; n_divisions=_downsampling_factor('manhattan', n_divisions, collect_all),; ); source_pd['p_value'] = [10 ** (-p) for p in source_pd['_pval']]; source_pd['_contig'] = [locus.split("":"")[0] for locus in source_pd['locus']]. observed_contigs = [contig for contig in ref.contigs.copy() if contig in set(source_pd['_contig'])]. contig_ticks = [ref._contig_global_position(contig) + ref.contig_length(contig) // 2 for contig in observed_contigs]; color_mapper = CategoricalColorMapper(factors=ref.contigs, palette=palette[:2] * int((len(ref.contigs) + 1) / 2)). p = figure(title=title, x_axis_label='Chromosome', y_axis_label='P-value (-log10 scale)', width=1000); p, _, legend, _, _, _ = _get_scatter_plot_elements(; p,; source_pd,; x_col='_global_locus',; y_col='_pval',; label_cols=['_contig'],; colors={'_contig': color_mapper},; size=size,; hover_cols={'locus', 'p_value'} | set(hover_fields),; ); assert legend is not None; legend.visible = False; p.xaxis.ticker = contig_ticks; p.xaxis.major_label_overrides = dict(zip(contig_ticks, [contig.replace(""chr"", """") for contig in observed_contigs])). if significance_line is not None:; p.renderers.append(; Span(; location=-math.log10(significance_line),; dimension='width',; line_color='red',; line_dash='dashed',; line_width=1.5,; ); ). return p. [docs]@typecheck(; entry_field=expr_any,; row_field=nullable(oneof(expr_numeric, expr_locus())),; column_field=nullable(expr_str),; window=nullable(int),; plot_width=int,; plot_height=int,; ); def visualize_missingness(; entry_field, row_field=None, column_field=None, window=6000000, plot_width=1800, plot_height=900; ) -> figure:; """"""Visualize missingness in a MatrixTable. Inspired by `naniar <https://cran.r-project.org/web/packages/naniar/index",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html:893,Deployability,update,updated,893,". Hail | ; hail.stats.linear_mixed_model. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.stats.linear_mixed_model. Source code for hail.stats.linear_mixed_model; [docs]class LinearMixedModel(object):; r""""""Class representing a linear mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94. """""". def __init__(self, py, px, s, y=None, x=None, p_path=None):; raise NotImplementedError(""LinearMixedModel is no longer implemented/supported as of Hail 0.2.94"").  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/stats/linear_mixed_model.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:2235,Availability,error,error,2235,"://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mod",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:2498,Availability,error,error,2498,")). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mode, buffer_size); _, ext = os.path.splitext(path); if ext in ('.gz', '.bgz'):; binary_mode = 'wb' if mode[0] == 'w' else 'rb'; file = fs.open(path, binary_mode, buffer_size); file = gzip.GzipFile(fileobj=file, mode=mode); if 'b' not in mode:; file = io.TextIOWrapp",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:5192,Availability,error,error,5192,"source and destination file paths must be URIs; (uniform resource identifiers). Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; return Env.fs().copy(src, dest). [docs]def hadoop_exists(path: str) -> bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().exists(path). [docs]def hadoop_is_file(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_file(path). [docs]def hadoop_is_dir(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_dir(path). [docs]def hadoop_stat(path: str) -> Dict[str, Any]:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return Env.fs().stat(path).to_legacy_dict(). [docs]def hadoop_ls(path: str) -> List[Dict[str, Any]]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:5814,Availability,error,error,5814,"ile(path). [docs]def hadoop_is_dir(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_dir(path). [docs]def hadoop_stat(path: str) -> Dict[str, Any]:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return Env.fs().stat(path).to_legacy_dict(). [docs]def hadoop_ls(path: str) -> List[Dict[str, Any]]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azu",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8240,Availability,error,error,8240,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8296,Deployability,update,updated,8296,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:2988,Security,access,access,2988,"---; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mode, buffer_size); _, ext = os.path.splitext(path); if ext in ('.gz', '.bgz'):; binary_mode = 'wb' if mode[0] == 'w' else 'rb'; file = fs.open(path, binary_mode, buffer_size); file = gzip.GzipFile(fileobj=file, mode=mode); if 'b' not in mode:; file = io.TextIOWrapper(file, encoding='utf-8'); else:; file = fs.open(path, mode, buffer_size); return file. [docs]@typecheck(src=str, dest=str); def hadoop_copy(src, dest):; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----.",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7118,Testability,log,log,7118,"-- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7253,Testability,log,log,7253,"n.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; La",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7291,Testability,log,log,7291,"n.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; La",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7332,Testability,log,log,7332,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7429,Testability,log,log,7429,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7490,Testability,log,log,7490,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7540,Testability,log,log,7540,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7637,Testability,log,logs,7637,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7699,Testability,log,log,7699,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7780,Testability,log,log,7780,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7828,Testability,log,log,7828,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7955,Testability,log,log,7955,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8049,Testability,log,log,8049,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8103,Testability,log,log,8103,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8152,Testability,log,log,8152,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8223,Testability,log,log,8223,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:4024,Usability,simpl,simpler,4024,"e, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mode, buffer_size); _, ext = os.path.splitext(path); if ext in ('.gz', '.bgz'):; binary_mode = 'wb' if mode[0] == 'w' else 'rb'; file = fs.open(path, binary_mode, buffer_size); file = gzip.GzipFile(fileobj=file, mode=mode); if 'b' not in mode:; file = io.TextIOWrapper(file, encoding='utf-8'); else:; file = fs.open(path, mode, buffer_size); return file. [docs]@typecheck(src=str, dest=str); def hadoop_copy(src, dest):; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. Try using :func:`.hadoop_open` first, it's simpler, but not great; for large data! For example:. >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; return Env.fs().copy(src, dest). [docs]def hadoop_exists(path: str) -> bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().exists(path). [docs]def hadoop_is_file(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_file(path). [docs]def hadoop_is_dir(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_di",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
https://hail.is/docs/0.2/_modules/hail/utils/interval.html:4710,Deployability,update,updated,4710,"es_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return self._point_type. def contains(self, value):; """"""True if `value` is contained within the interval. Examples; --------. >>> interval2.contains(5); True. >>> interval2.contains(6); False. Parameters; ----------; value :; Object with type :meth:`.point_type`. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).contains(value)). @typecheck_method(interval=interval_type); def overlaps(self, interval):; """"""True if the the supplied interval contains any value in common with this one. Parameters; ----------; interval : :class:`.Interval`; Interval object with the same point type. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).overlaps(interval)). interval_type.set(Interval).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html
https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2856,Security,hash,hash,2856,"e incompatible types: '{}', '{}'."".format(start_type, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html
https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2876,Security,hash,hash,2876,"}'."".format(start_type, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Ty",MatchSource.WIKI,docs/0.2/_modules/hail/utils/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html
https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2894,Security,hash,hash,2894,"ype, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return se",MatchSource.WIKI,docs/0.2/_modules/hail/utils/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html
https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2923,Security,hash,hash,2923,"point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return self._point_type. def cont",MatchSource.WIKI,docs/0.2/_modules/hail/utils/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:929,Availability,error,error,929,". Hail | ; hail.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.misc. Source code for hail.utils.misc; import atexit; import datetime; import difflib; import json; import os; import re; import secrets; import shutil; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', '",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:10856,Availability,error,error,10856,"ield_names:; dd[f.lower()].append(f). item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, dd, n=5). s = [""{} instance has no field '{}'"".format(class_name, item)]; if field_matches:; s.append('\n Did you mean:'); for f in field_matches:; for orig_f in dd[f]:; s.append(""\n {}"".format(handler(orig_f))); if has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def check_collisions(caller, names, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException. fields = indices.source._fields. if override_protected_indices is not None:. def invalid(e):; return e._indices in override_protected_indices. else:. def invalid(e):; return e._indices != indices. # check collisions with fields on other axes; for name in names:; if name in fields and invalid(fields[name]):; msg = f""{caller!r}: name collision with field indexed by {list(fields[name]._indices.axes)}: {name!r}""; error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). # check duplicate fields; for k, v in Counter(names).items():; if v > 1:; from hail.expr.expressions import ExpressionException. raise ExpressionException(f""{caller!r}: selection would produce duplicate field {k!r}""). def get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}. bindings = []. def is_top_level_field(e):; return e in indices.source._fields_inverse. existing_key_fields = []; final_key = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = h",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:12563,Availability,error,error,12563," = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = ht.key_by(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.key_by(ht.x.replace(' ', '_'))""; ). name = e._ir.name; final_key.append(name). if not is_top_level_field(e):; bindings.append((name, e)); else:; existing_key_fields.append(name). final_key.extend(named_exprs); bindings.extend(named_exprs.items()); check_collisions(caller, final_key, indices, override_protected_indices=override_protected_indices); return final_key, dict(bindings). def check_keys(caller, name, protected_key):; from hail.expr.expressions import ExpressionException. if name in protected_key:; msg = (; f""{caller!r}: cannot overwrite key field {name!r} with annotate, select or drop; ""; f""use key_by to modify keys.""; ); error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). def get_select_exprs(caller, exprs, named_exprs, indices, base_struct):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.ap",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:13915,Availability,redundant,redundant,13915," v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.append(name); if is_top_level_field(e):; select_fields.append(name); else:; insertions[name] = e; for k, e in named_exprs.items():; check_keys(caller, k, protected_key); final_fields.append(k); insertions[k] = e. check_collisions(caller, final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:18317,Deployability,update,updated,18317,"ape_str(s, backticked=True)). def parsable_strings(strs):; strs = ' '.join(f'""{escape_str(s)}""' for s in strs); return f""({strs})"". def _dumps_partitions(partitions, row_key_type):; parts_type = partitions.dtype; if not (isinstance(parts_type, hl.tarray) and isinstance(parts_type.element_type, hl.tinterval)):; raise ValueError(f'partitions type invalid: {parts_type} must be array of intervals'). point_type = parts_type.element_type.point_type. f1, t1 = next(iter(row_key_type.items())); if point_type == t1:; partitions = hl.map(; lambda x: hl.interval(; start=hl.struct(**{f1: x.start}),; end=hl.struct(**{f1: x.end}),; includes_start=x.includes_start,; includes_end=x.includes_end,; ),; partitions,; ); else:; if not isinstance(point_type, hl.tstruct):; raise ValueError(f'partitions has wrong type: {point_type} must be struct or type of first row key field'); if not point_type._is_prefix_of(row_key_type):; raise ValueError(f'partitions type invalid: {point_type} must be prefix of {row_key_type}'). s = json.dumps(partitions.dtype._convert_to_json(hl.eval(partitions))); return s, partitions.dtype. def default_handler():; try:; from IPython.display import display. return display; except ImportError:; return print. def guess_cloud_spark_provider() -> Optional[Literal['dataproc', 'hdinsight']]:; if 'HAIL_DATAPROC' in os.environ:; return 'dataproc'; if 'AZURE_SPARK' in os.environ or 'hdinsight' in os.getenv('CLASSPATH', ''):; return 'hdinsight'; return None. def no_service_backend(unsupported_feature):; from hail import current_backend; from hail.backend.service_backend import ServiceBackend. if isinstance(current_backend(), ServiceBackend):; raise NotImplementedError(; f'{unsupported_feature!r} is not yet supported on the service backend.'; f'\n If this is a pressing need, please alert the team on the discussion'; f'\n forum to aid in prioritization: https://discuss.hail.is'; ). ANY_REGION = ['any_region'].  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8130,Modifiability,inherit,inherited,8130,"):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8318,Modifiability,inherit,inherited,8318,"if isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.form",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8486,Modifiability,inherit,inherited,8486,"ructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(p) for p in prop_matches)); ); if inherited_matches:; word = plural('inherited method', len(inherited_matches)); s.append(; '\n {} {}: {}'.format(; ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8885,Modifiability,extend,extend,8885,"ass_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(p) for p in prop_matches)); ); if inherited_matches:; word = plural('inherited method', len(inherited_matches)); s.append(; '\n {} {}: {}'.format(; class_name, word, ', '.join(""'{}'"".format(m) for m in inherited_matches); ); ); elif has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def get_nice_field_error(obj, item):; class_name, _, handler, has_describe = get_obj_metadata(obj). field_names = obj._fields.keys(); dd = defaultdict(lambda: []); for f in field_names:; dd[f.lower()].append(f). item_lower = i",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:9413,Modifiability,inherit,inherited,9413,"item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(p) for p in prop_matches)); ); if inherited_matches:; word = plural('inherited method', len(inherited_matches)); s.append(; '\n {} {}: {}'.format(; class_name, word, ', '.join(""'{}'"".format(m) for m in inherited_matches); ); ); elif has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def get_nice_field_error(obj, item):; class_name, _, handler, has_describe = get_obj_metadata(obj). field_names = obj._fields.keys(); dd = defaultdict(lambda: []); for f in field_names:; dd[f.lower()].append(f). item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, dd, n=5). s = [""{} instance has no field '{}'"".format(class_name, item)]; if field_matches:; s.append('\n Did you mean:'); for f in field_matches:; for orig_f in dd[f]:; s.append(""\n {}"".format(handler(orig_f))); if has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def check_collisions(caller, names, indices, override_protected_indices=None):; from hail.",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:12114,Modifiability,extend,extend,12114,"licate field {k!r}""). def get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}. bindings = []. def is_top_level_field(e):; return e in indices.source._fields_inverse. existing_key_fields = []; final_key = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = ht.key_by(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.key_by(ht.x.replace(' ', '_'))""; ). name = e._ir.name; final_key.append(name). if not is_top_level_field(e):; bindings.append((name, e)); else:; existing_key_fields.append(name). final_key.extend(named_exprs); bindings.extend(named_exprs.items()); check_collisions(caller, final_key, indices, override_protected_indices=override_protected_indices); return final_key, dict(bindings). def check_keys(caller, name, protected_key):; from hail.expr.expressions import ExpressionException. if name in protected_key:; msg = (; f""{caller!r}: cannot overwrite key field {name!r} with annotate, select or drop; ""; f""use key_by to modify keys.""; ); error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). def get_select_exprs(caller, exprs, named_exprs, indices, base_struct):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e i",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:12144,Modifiability,extend,extend,12144,"f get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}. bindings = []. def is_top_level_field(e):; return e in indices.source._fields_inverse. existing_key_fields = []; final_key = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = ht.key_by(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.key_by(ht.x.replace(' ', '_'))""; ). name = e._ir.name; final_key.append(name). if not is_top_level_field(e):; bindings.append((name, e)); else:; existing_key_fields.append(name). final_key.extend(named_exprs); bindings.extend(named_exprs.items()); check_collisions(caller, final_key, indices, override_protected_indices=override_protected_indices); return final_key, dict(bindings). def check_keys(caller, name, protected_key):; from hail.expr.expressions import ExpressionException. if name in protected_key:; msg = (; f""{caller!r}: cannot overwrite key field {name!r} with annotate, select or drop; ""; f""use key_by to modify keys.""; ); error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). def get_select_exprs(caller, exprs, named_exprs, indices, base_struct):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:14808,Modifiability,extend,extend,14808,"final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid for uid in all_uids if uid in table._fields]; return table.drop(*remaining_uids). return left, cleanup. def divide_null(num, denom):; from hail.expr import if_else, missing; from hail.expr.expressions.base_expression import unify_types_limited. typ = unify_types_limited(num.dtype, denom.dtype); assert typ is not None; return if_else(denom != 0, num / denom, missing(typ)). def lookup_bit(byte, which_bit):; return (byte >> which_bit) & 1. def timestamp_path(base, suffix=''):; return ''.join([base, '-', datetime.datetime.now().strftime(""%Y%m%d-%H%M""), suffix]). def upper_hex(n, num_digits=None):; if num_digits is None:; return ""{0:X}"".format(n); else:; return ""{0:0{1}X}"".format(n, num_digits). def escape_str(s, backticked=False):; sb = StringIO(). rewrite_dict = {'\b': '\\b', '\n': '\\n', '\t': '\\t', '\f': '\\f', '\r': '\\r'}. for ch in s:; chNum = ord(ch); if chNum > 0x7F:; sb.write(""\\u"" + upper_hex(chNum",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:1594,Performance,optimiz,optimized,1594,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:1620,Performance,perform,performance,1620,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:2801,Performance,optimiz,optimized,2801,"es Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; check_nonnegative_and_in_range('range_table', 'n', n); if n_partitions is not None:; check_positive_and_in_range('range_table', 'n_partitions', n_partitions). return hail.Table(hail.ir.TableRange(n, n_partitions)). def check_positive_and_in_range(caller, name, value):; if value <= 0:; raise ValueError(f""'{caller}': parameter '{name}' must be positive, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}': parameter '{name}' must be less than or equal to {hail.tint32.max_value}, "" f""found {value}""; ). def check_nonnegative_and_in_range(caller, name, value):; if value < 0:; raise ValueError(f""'{caller}': parameter '{name}' must be non-negative, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}'",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:2827,Performance,perform,performance,2827,"es Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; check_nonnegative_and_in_range('range_table', 'n', n); if n_partitions is not None:; check_positive_and_in_range('range_table', 'n_partitions', n_partitions). return hail.Table(hail.ir.TableRange(n, n_partitions)). def check_positive_and_in_range(caller, name, value):; if value <= 0:; raise ValueError(f""'{caller}': parameter '{name}' must be positive, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}': parameter '{name}' must be less than or equal to {hail.tint32.max_value}, "" f""found {value}""; ). def check_nonnegative_and_in_range(caller, name, value):; if value < 0:; raise ValueError(f""'{caller}': parameter '{name}' must be non-negative, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}'",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:13915,Safety,redund,redundant,13915," v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.append(name); if is_top_level_field(e):; select_fields.append(name); else:; insertions[name] = e; for k, e in named_exprs.items():; check_keys(caller, k, protected_key); final_fields.append(k); insertions[k] = e. check_collisions(caller, final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:7854,Security,access,access,7854,"; return 'MatrixTable', MatrixTable, table_error(obj), True; elif isinstance(obj, GroupedMatrixTable):; return 'GroupedMatrixTable', GroupedMatrixTable, table_error(obj._parent), True; elif isinstance(obj, Table):; return 'Table', Table, table_error(obj), True; elif isinstance(obj, GroupedTable):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:1561,Testability,test,testing,1561,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:2768,Testability,test,testing,2768,"es Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; check_nonnegative_and_in_range('range_table', 'n', n); if n_partitions is not None:; check_positive_and_in_range('range_table', 'n_partitions', n_partitions). return hail.Table(hail.ir.TableRange(n, n_partitions)). def check_positive_and_in_range(caller, name, value):; if value <= 0:; raise ValueError(f""'{caller}': parameter '{name}' must be positive, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}': parameter '{name}' must be less than or equal to {hail.tint32.max_value}, "" f""found {value}""; ). def check_nonnegative_and_in_range(caller, name, value):; if value < 0:; raise ValueError(f""'{caller}': parameter '{name}' must be non-negative, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}'",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:6278,Testability,assert,assert,6278,"rror:; pass. storage_level = enumeration(; 'NONE',; 'DISK_ONLY',; 'DISK_ONLY_2',; 'MEMORY_ONLY',; 'MEMORY_ONLY_2',; 'MEMORY_ONLY_SER',; 'MEMORY_ONLY_SER_2',; 'MEMORY_AND_DISK',; 'MEMORY_AND_DISK_2',; 'MEMORY_AND_DISK_SER',; 'MEMORY_AND_DISK_SER_2',; 'OFF_HEAP',; ). def run_command(args):; import subprocess as sp. try:; sp.check_output(args, stderr=sp.STDOUT); except sp.CalledProcessError as e:; print(e.output); raise e. def hl_plural(orig, n, alternate=None):; if alternate is None:; plural = orig + 's'; else:; plural = alternate; return hl.if_else(n == 1, orig, plural). def plural(orig, n, alternate=None):; if n == 1:; return orig; elif alternate:; return alternate; else:; return orig + 's'. def get_obj_metadata(obj):; from hail.expr.expressions import ArrayStructExpression, SetStructExpression, StructExpression; from hail.matrixtable import GroupedMatrixTable, MatrixTable; from hail.table import GroupedTable, Table; from hail.utils import Struct. def table_error(index_obj):; def fmt_field(field):; assert field in index_obj._fields; inds = index_obj[field]._indices; if inds == index_obj._global_indices:; return ""'{}' [globals]"".format(field); elif inds == index_obj._row_indices:; return ""'{}' [row]"".format(field); elif inds == index_obj._col_indices: # Table will never get here; return ""'{}' [col]"".format(field); else:; assert inds == index_obj._entry_indices; return ""'{}' [entry]"".format(field). return fmt_field. def struct_error(s):; def fmt_field(field):; assert field in s._fields; return ""'{}'"".format(field). return fmt_field. if isinstance(obj, MatrixTable):; return 'MatrixTable', MatrixTable, table_error(obj), True; elif isinstance(obj, GroupedMatrixTable):; return 'GroupedMatrixTable', GroupedMatrixTable, table_error(obj._parent), True; elif isinstance(obj, Table):; return 'Table', Table, table_error(obj), True; elif isinstance(obj, GroupedTable):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Str",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:6606,Testability,assert,assert,6606,"stderr=sp.STDOUT); except sp.CalledProcessError as e:; print(e.output); raise e. def hl_plural(orig, n, alternate=None):; if alternate is None:; plural = orig + 's'; else:; plural = alternate; return hl.if_else(n == 1, orig, plural). def plural(orig, n, alternate=None):; if n == 1:; return orig; elif alternate:; return alternate; else:; return orig + 's'. def get_obj_metadata(obj):; from hail.expr.expressions import ArrayStructExpression, SetStructExpression, StructExpression; from hail.matrixtable import GroupedMatrixTable, MatrixTable; from hail.table import GroupedTable, Table; from hail.utils import Struct. def table_error(index_obj):; def fmt_field(field):; assert field in index_obj._fields; inds = index_obj[field]._indices; if inds == index_obj._global_indices:; return ""'{}' [globals]"".format(field); elif inds == index_obj._row_indices:; return ""'{}' [row]"".format(field); elif inds == index_obj._col_indices: # Table will never get here; return ""'{}' [col]"".format(field); else:; assert inds == index_obj._entry_indices; return ""'{}' [entry]"".format(field). return fmt_field. def struct_error(s):; def fmt_field(field):; assert field in s._fields; return ""'{}'"".format(field). return fmt_field. if isinstance(obj, MatrixTable):; return 'MatrixTable', MatrixTable, table_error(obj), True; elif isinstance(obj, GroupedMatrixTable):; return 'GroupedMatrixTable', GroupedMatrixTable, table_error(obj._parent), True; elif isinstance(obj, Table):; return 'Table', Table, table_error(obj), True; elif isinstance(obj, GroupedTable):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', S",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:6747,Testability,assert,assert,6747,"lternate is None:; plural = orig + 's'; else:; plural = alternate; return hl.if_else(n == 1, orig, plural). def plural(orig, n, alternate=None):; if n == 1:; return orig; elif alternate:; return alternate; else:; return orig + 's'. def get_obj_metadata(obj):; from hail.expr.expressions import ArrayStructExpression, SetStructExpression, StructExpression; from hail.matrixtable import GroupedMatrixTable, MatrixTable; from hail.table import GroupedTable, Table; from hail.utils import Struct. def table_error(index_obj):; def fmt_field(field):; assert field in index_obj._fields; inds = index_obj[field]._indices; if inds == index_obj._global_indices:; return ""'{}' [globals]"".format(field); elif inds == index_obj._row_indices:; return ""'{}' [row]"".format(field); elif inds == index_obj._col_indices: # Table will never get here; return ""'{}' [col]"".format(field); else:; assert inds == index_obj._entry_indices; return ""'{}' [entry]"".format(field). return fmt_field. def struct_error(s):; def fmt_field(field):; assert field in s._fields; return ""'{}'"".format(field). return fmt_field. if isinstance(obj, MatrixTable):; return 'MatrixTable', MatrixTable, table_error(obj), True; elif isinstance(obj, GroupedMatrixTable):; return 'GroupedMatrixTable', GroupedMatrixTable, table_error(obj._parent), True; elif isinstance(obj, Table):; return 'Table', Table, table_error(obj), True; elif isinstance(obj, GroupedTable):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; clas",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:14092,Testability,assert,assert,14092,"_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.append(name); if is_top_level_field(e):; select_fields.append(name); else:; insertions[name] = e; for k, e in named_exprs.items():; check_keys(caller, k, protected_key); final_fields.append(k); insertions[k] = e. check_collisions(caller, final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid for uid in all_uids if uid in table._fields]; return table.drop(*remaining_uids). return left, cleanup. def divide_null(num, denom):; from hail.expr import if_else, missing; from hail.expr.expressions.base_expression ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:15191,Testability,assert,assert,15191,"s import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid for uid in all_uids if uid in table._fields]; return table.drop(*remaining_uids). return left, cleanup. def divide_null(num, denom):; from hail.expr import if_else, missing; from hail.expr.expressions.base_expression import unify_types_limited. typ = unify_types_limited(num.dtype, denom.dtype); assert typ is not None; return if_else(denom != 0, num / denom, missing(typ)). def lookup_bit(byte, which_bit):; return (byte >> which_bit) & 1. def timestamp_path(base, suffix=''):; return ''.join([base, '-', datetime.datetime.now().strftime(""%Y%m%d-%H%M""), suffix]). def upper_hex(n, num_digits=None):; if num_digits is None:; return ""{0:X}"".format(n); else:; return ""{0:0{1}X}"".format(n, num_digits). def escape_str(s, backticked=False):; sb = StringIO(). rewrite_dict = {'\b': '\\b', '\n': '\\n', '\t': '\\t', '\f': '\\f', '\r': '\\r'}. for ch in s:; chNum = ord(ch); if chNum > 0x7F:; sb.write(""\\u"" + upper_hex(chNum, 4)); elif chNum < 32:; if ch in rewrite_dict:; sb.write(rewrite_dict[ch]); elif chNum > 0xF:; sb.write(""\\u00"" + upper_hex(chNum)); else:; sb.write(""\\u000"" + upper_hex(chNum)); elif ch == '""':; if backticked:; sb.write('""'); else:; sb.write('\\""'); elif ch == '`':; if backticked:; sb.write(""\\`""); else:; sb.write(""`""); elif ch == '\\':; sb.write('\\\\'); else:; sb.write(ch). escaped = sb.getvalue(); sb.clos",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:1573,Usability,learn,learning,1573,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/misc.html:2780,Usability,learn,learning,2780,"es Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; check_nonnegative_and_in_range('range_table', 'n', n); if n_partitions is not None:; check_positive_and_in_range('range_table', 'n_partitions', n_partitions). return hail.Table(hail.ir.TableRange(n, n_partitions)). def check_positive_and_in_range(caller, name, value):; if value <= 0:; raise ValueError(f""'{caller}': parameter '{name}' must be positive, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}': parameter '{name}' must be less than or equal to {hail.tint32.max_value}, "" f""found {value}""; ). def check_nonnegative_and_in_range(caller, name, value):; if value < 0:; raise ValueError(f""'{caller}': parameter '{name}' must be non-negative, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}'",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/utils/struct.html:3675,Deployability,update,updated,3675,"lf):; return str(self). def __str__(self):; if all(k.isidentifier() for k in self._fields):; return 'Struct(' + ', '.join(f'{k}={v!r}' for k, v in self._fields.items()) + ')'; return 'Struct(**{' + ', '.join(f'{k!r}: {v!r}' for k, v in self._fields.items()) + '})'. def __eq__(self, other):; return self._fields == other._fields if isinstance(other, Struct) else NotImplemented. def __hash__(self):; return 37 + hash(tuple(sorted(self._fields.items()))). def __iter__(self):; return iter(self._fields). def __dir__(self):; super_dir = super().__dir__(); return super_dir + list(self._fields.keys()). def annotate(self, **kwargs):; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; d = OrderedDict(self.items()); for k, v in kwargs.items():; d[k] = v; return Struct(**d). @typecheck_method(fields=str, kwargs=anytype); def select(self, *fields, **kwargs):; """"""Select existing fields and compute new ones. Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `kwargs` arguments are new fields to add. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.; named_exprs : keyword args; New field. Returns; -------; :class:`.Struct`; Struct containing specified",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
https://hail.is/docs/0.2/_modules/hail/utils/struct.html:7306,Deployability,patch,patch,7306,"ds: varargs of :class:`str`; Fields to drop. Returns; -------; :class:`.Struct`; Struct without certain fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5, bar=2, apple=10). Drop one field from `s`. >>> s.drop('bar'); Struct(food=8, fruit=5, apple=10). Drop two fields from `s`. >>> s.drop('food', 'fruit'); Struct(bar=2, apple=10); """"""; d = OrderedDict((k, v) for k, v in self.items() if k not in args); return Struct(**d). @typecheck(struct=Struct); def to_dict(struct):; return dict(struct.items()). _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, stream, indent, allowance, context, level, *args, **kwargs):; if isinstance(obj, Struct):; rep = self._repr(obj, context, level); max_width = self._width - indent - allowance; if len(rep) <= max_width:; stream.write(rep); return. stream.write('Struct('); indent += len('Struct('); if all(k.isidentifier() for k in obj):; n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(k); stream.write('='); this_indent = indent + len(k) + len('='); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); else:; stream.write('**{'); indent += len('**{'); n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(repr(k)); stream.write(': '); this_indent = indent + len(repr(k)) + len(': '); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); stream.write('}'); stream.write(')'); else:; _old_printer._format(self, obj, stream, indent, allowance, context, level, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
https://hail.is/docs/0.2/_modules/hail/utils/struct.html:7360,Deployability,update,updated,7360,"ds: varargs of :class:`str`; Fields to drop. Returns; -------; :class:`.Struct`; Struct without certain fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5, bar=2, apple=10). Drop one field from `s`. >>> s.drop('bar'); Struct(food=8, fruit=5, apple=10). Drop two fields from `s`. >>> s.drop('food', 'fruit'); Struct(bar=2, apple=10); """"""; d = OrderedDict((k, v) for k, v in self.items() if k not in args); return Struct(**d). @typecheck(struct=Struct); def to_dict(struct):; return dict(struct.items()). _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, stream, indent, allowance, context, level, *args, **kwargs):; if isinstance(obj, Struct):; rep = self._repr(obj, context, level); max_width = self._width - indent - allowance; if len(rep) <= max_width:; stream.write(rep); return. stream.write('Struct('); indent += len('Struct('); if all(k.isidentifier() for k in obj):; n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(k); stream.write('='); this_indent = indent + len(k) + len('='); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); else:; stream.write('**{'); indent += len('**{'); n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(repr(k)); stream.write(': '); this_indent = indent + len(repr(k)) + len(': '); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); stream.write('}'); stream.write(')'); else:; _old_printer._format(self, obj, stream, indent, allowance, context, level, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
https://hail.is/docs/0.2/_modules/hail/utils/struct.html:1718,Safety,avoid,avoid,1718,"t_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = state. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). @typecheck_method(item=str); def __getitem__(self, item):; return self._get_field(item). def __setattr__(self, key, value):; if key in self._fields:; raise ValueError(""Structs are immutable, cannot overwrite a field.""); else:; super().__setattr__(key, value). def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]; elif item in self._fields:; return self._fields[item]; else:; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __repr__(self):; return str(self). def __str__(self):; if a",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
https://hail.is/docs/0.2/_modules/hail/utils/struct.html:939,Security,access,accessing,939,". Hail | ; hail.utils.struct. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.struct. Source code for hail.utils.struct; import pprint; from collections import OrderedDict; from collections.abc import Mapping; from typing import Any, Dict. from hail.typecheck import anytype, typecheck, typecheck_method; from hail.utils.misc import get_nice_attr_error, get_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = st",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
https://hail.is/docs/0.2/_modules/hail/utils/struct.html:1132,Security,access,accessed,1132,"tch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.struct. Source code for hail.utils.struct; import pprint; from collections import OrderedDict; from collections.abc import Mapping; from typing import Any, Dict. from hail.typecheck import anytype, typecheck, typecheck_method; from hail.utils.misc import get_nice_attr_error, get_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = state. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; el",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
https://hail.is/docs/0.2/_modules/hail/utils/struct.html:3080,Security,hash,hash,3080,"fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). @typecheck_method(item=str); def __getitem__(self, item):; return self._get_field(item). def __setattr__(self, key, value):; if key in self._fields:; raise ValueError(""Structs are immutable, cannot overwrite a field.""); else:; super().__setattr__(key, value). def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]; elif item in self._fields:; return self._fields[item]; else:; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __repr__(self):; return str(self). def __str__(self):; if all(k.isidentifier() for k in self._fields):; return 'Struct(' + ', '.join(f'{k}={v!r}' for k, v in self._fields.items()) + ')'; return 'Struct(**{' + ', '.join(f'{k!r}: {v!r}' for k, v in self._fields.items()) + '})'. def __eq__(self, other):; return self._fields == other._fields if isinstance(other, Struct) else NotImplemented. def __hash__(self):; return 37 + hash(tuple(sorted(self._fields.items()))). def __iter__(self):; return iter(self._fields). def __dir__(self):; super_dir = super().__dir__(); return super_dir + list(self._fields.keys()). def annotate(self, **kwargs):; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; d = OrderedDict(",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:2036,Availability,down,download,2036,"al/ensembl_gene_annotations.txt',; 'HGDP_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_pop_and_sex_annotations.tsv',; 'HGDP_matrix_table': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_subset.vcf.bgz',; 'HGDP_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_gene_annotations.tsv',; 'movie_lens_100k': 'https://files.grouplens.org/datasets/movielens/ml-100k.zip',; }. tmp_dir: str = None. def init_temp_dir():; global tmp_dir; if tmp_dir is None:; tmp_dir = new_local_temp_dir(). def _dir_exists(fs, path):; return fs.exists(path) and fs.is_dir(path). def _file_exists(fs, path):; return fs.exists(path) and fs.is_file(path). def _copy_to_tmp(fs, src, extension=None):; dst = new_temp_file(extension=extension); fs.copy(src, dst); return dst. [docs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:2866,Availability,down,downloading,2866,"ocs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16).write(matrix_table_path, overwrite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:3347,Availability,down,downloading,3347,", '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16).write(matrix_table_path, overwrite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('1KG files found'). [docs]def get_hgdp(output_dir, overwrite: bool = False):; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``Tr",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:3604,Availability,down,downloading,3604,"ts(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16).write(matrix_table_path, overwrite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('1KG files found'). [docs]def get_hgdp(output_dir, overwrite: bool = False):; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, 'HGDP.mt'); vcf_path = os.path.join(output_dir, 'HGDP.vcf.bgz'); s",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:4222,Availability,down,download,4222,"rite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('1KG files found'). [docs]def get_hgdp(output_dir, overwrite: bool = False):; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, 'HGDP.mt'); vcf_path = os.path.join(output_dir, 'HGDP.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, 'HGDP_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:5058,Availability,down,downloading,5058,"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, 'HGDP.mt'); vcf_path = os.path.join(output_dir, 'HGDP.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, 'HGDP_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); h",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:5574,Availability,down,downloading,5574,"tput_dir, 'HGDP.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, 'HGDP_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If `",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:5833,Availability,down,downloading,5833,"e_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:6332,Availability,down,download,6332,"hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:6969,Availability,down,downloading,6969,"py(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.data'); assert os.path.exists(user_table_path); assert os.path.exists(movie_table_path); assert os.path.exists(ratings_table_path). user_cluster_readable = _copy_to_tmp(fs, local_path_uri(user_table_path), extension='txt'); movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:8872,Deployability,release,release,8872,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:8894,Deployability,release,release,8894,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:8967,Deployability,release,release,8967,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:8989,Deployability,release,release,8989,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:9533,Deployability,update,updated,9533,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!').  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:7347,Testability,assert,assert,7347,"t 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.data'); assert os.path.exists(user_table_path); assert os.path.exists(movie_table_path); assert os.path.exists(ratings_table_path). user_cluster_readable = _copy_to_tmp(fs, local_path_uri(user_table_path), extension='txt'); movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:7387,Testability,assert,assert,7387,"te <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.data'); assert os.path.exists(user_table_path); assert os.path.exists(movie_table_path); assert os.path.exists(ratings_table_path). user_cluster_readable = _copy_to_tmp(fs, local_path_uri(user_table_path), extension='txt'); movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'im",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:7428,Testability,assert,assert,7428,"ens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.data'); assert os.path.exists(user_table_path); assert os.path.exists(movie_table_path); assert os.path.exists(ratings_table_path). user_cluster_readable = _copy_to_tmp(fs, local_path_uri(user_table_path), extension='txt'); movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {user",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
https://hail.is/docs/0.2/_modules/hail/vds/functions.html:3577,Deployability,update,updated,3577,"[9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; -----; The `number` parameter matches the `VCF specification <https://samtools.github.io/hts-specs/VCFv4.3.pdf>`__; number definitions:. - ``A`` indicates one value per allele, excluding the reference.; - ``R`` indicates one value per allele, including the reference.; - ``G`` indicates one value per unique diploid genotype. Warning; -------; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample -- with 100,000 samples, nearly 50GB per variant!. See Also; --------; :func:`.lgt_to_gt`. Parameters; ----------; array : :class:`.ArrayExpression`; Array to reindex.; local_alleles : :class:`.ArrayExpression`; Local alleles array.; n_alleles : :class:`.Int32Expression`; Total number of alleles to reindex to.; fill_value; Value to fill in at global indices with no data.; number : :class:`str`; One of 'A', 'R', 'G'. Returns; -------; :class:`.ArrayExpression`; """"""; try:; fill_value = hl.coercer_from_dtype(array.dtype.element_type).coerce(fill_value); except Exception as e:; raise ValueError(f'fill_value type {fill_value.dtype} is incompatible with array type {array.dtype}') from e. if number == 'G':; return _func(""local_to_global_g"", array.dtype, array, local_alleles, n_alleles, fill_value); elif number == 'R':; omit_first = False; elif number == 'A':; omit_first = True; else:; raise ValueError(f'unrecognized number {number}'). return _func(""local_to_global_a_r"", array.dtype, array, local_alleles, n_alleles, fill_value, hl.bool(omit_first)).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/functions.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:12182,Availability,checkpoint,checkpointing,12182,"comparison. Returns; -------; :class:`.Table`; """""". rg = mt.interval.start.dtype.reference_genome. if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_x = rg.x_contigs[0]; if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_y = rg.y_contigs[0]. mt = mt.annotate_rows(contig=mt.interval.start.contig); mt = mt.annotate_cols(__mean_dp=hl.agg.group_by(mt.contig, hl.agg.sum(mt.sum_dp) / hl.agg.sum(mt.interval_size))). mean_dp_dict = mt.__mean_dp; auto_dp = mean_dp_dict.get(normalization_contig, 0.0); x_dp = mean_dp_dict.get(chr_x, 0.0); y_dp = mean_dp_dict.get(chr_y, 0.0); per_sample = mt.transmute_cols(; autosomal_mean_dp=auto_dp,; x_mean_dp=x_dp,; x_ploidy=2 * x_dp / auto_dp,; y_mean_dp=y_dp,; y_ploidy=2 * y_dp / auto_dp,; ); info(""'impute_sex_chromosome_ploidy': computing and checkpointing coverage and karyotype metrics""); return per_sample.cols().checkpoint(new_temp_file('impute_sex_karyotype', extension='ht')). [docs]@typecheck(; vds=VariantDataset,; calling_intervals=oneof(Table, expr_array(expr_interval(expr_locus()))),; normalization_contig=str,; use_variant_dataset=bool,; ); def impute_sex_chromosome_ploidy(; vds: VariantDataset, calling_intervals, normalization_contig: str, use_variant_dataset: bool = False; ) -> Table:; """"""Impute sex chromosome ploidy from depth of reference or variant data within calling intervals. Returns a :class:`.Table` with sample ID keys, with the following fields:. - ``autosomal_mean_dp`` (*float64*): Mean depth on calling intervals on normalization contig.; - ``x_mean_dp`` (*float64*): Mean depth on calling intervals on X chromosome.; - ``x_ploidy`` (*float64*): Estimated ploidy on X chromosome. Equal to ``2 * x_mean_dp / autosomal_mean_dp``.; - ``y_mean_dp`` (",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:12255,Availability,checkpoint,checkpoint,12255,"ome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_x = rg.x_contigs[0]; if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_y = rg.y_contigs[0]. mt = mt.annotate_rows(contig=mt.interval.start.contig); mt = mt.annotate_cols(__mean_dp=hl.agg.group_by(mt.contig, hl.agg.sum(mt.sum_dp) / hl.agg.sum(mt.interval_size))). mean_dp_dict = mt.__mean_dp; auto_dp = mean_dp_dict.get(normalization_contig, 0.0); x_dp = mean_dp_dict.get(chr_x, 0.0); y_dp = mean_dp_dict.get(chr_y, 0.0); per_sample = mt.transmute_cols(; autosomal_mean_dp=auto_dp,; x_mean_dp=x_dp,; x_ploidy=2 * x_dp / auto_dp,; y_mean_dp=y_dp,; y_ploidy=2 * y_dp / auto_dp,; ); info(""'impute_sex_chromosome_ploidy': computing and checkpointing coverage and karyotype metrics""); return per_sample.cols().checkpoint(new_temp_file('impute_sex_karyotype', extension='ht')). [docs]@typecheck(; vds=VariantDataset,; calling_intervals=oneof(Table, expr_array(expr_interval(expr_locus()))),; normalization_contig=str,; use_variant_dataset=bool,; ); def impute_sex_chromosome_ploidy(; vds: VariantDataset, calling_intervals, normalization_contig: str, use_variant_dataset: bool = False; ) -> Table:; """"""Impute sex chromosome ploidy from depth of reference or variant data within calling intervals. Returns a :class:`.Table` with sample ID keys, with the following fields:. - ``autosomal_mean_dp`` (*float64*): Mean depth on calling intervals on normalization contig.; - ``x_mean_dp`` (*float64*): Mean depth on calling intervals on X chromosome.; - ``x_ploidy`` (*float64*): Estimated ploidy on X chromosome. Equal to ``2 * x_mean_dp / autosomal_mean_dp``.; - ``y_mean_dp`` (*float64*): Mean depth on calling intervals on chromosome.; - ``y_ploidy`` (*float64*): Estimated ploidy on Y chromosome. Equal to ``2 * y_mean_db / autosomal_mean_dp``. ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:14901,Availability,checkpoint,checkpoint,14901,"t_type),; key='interval',; ); else:; key_dtype = calling_intervals.key.dtype; if (; len(key_dtype) != 1; or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.referen",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:14935,Availability,down,downstream,14935,"t_type),; key='interval',; ); else:; key_dtype = calling_intervals.key.dtype; if (; len(key_dtype) != 1; or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.referen",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:14992,Availability,checkpoint,checkpointing,14992,"t_type),; key='interval',; ); else:; key_dtype = calling_intervals.key.dtype; if (; len(key_dtype) != 1; or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.referen",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:15065,Availability,checkpoint,checkpoint,15065,"or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.reference_data, kept_contig_filter),; hl.filter_intervals(vds.variant_data, kept_contig_filter),; ). if use_var",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:23796,Availability,error,error,23796,"erval boundaries. Results in a smaller result, but this filter mode; is more computationally expensive to evaluate.; keep : :obj:`bool`; Whether to keep, or filter out (default) rows that fall within any; interval in `intervals`. Returns; -------; :class:`.VariantDataset`; """"""; if split_reference_blocks and not keep:; raise ValueError(""'filter_intervals': cannot use 'split_reference_blocks' with keep=False""); return _parameterized_filter_intervals(; vds, intervals, keep=keep, mode='split_at_boundaries' if split_reference_blocks else 'variants_only'; ). [docs]@typecheck(vds=VariantDataset, filter_changed_loci=bool); def split_multi(vds: 'VariantDataset', *, filter_changed_loci: bool = False) -> 'VariantDataset':; """"""Split the multiallelic variants in a :class:`.VariantDataset`. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; filter_changed_loci : :obj:`bool`; If any REF/ALT pair changes locus under :func:`.min_rep`, filter that; variant instead of throwing an error. Returns; -------; :class:`.VariantDataset`; """"""; variant_data = hl.experimental.sparse_split_multi(vds.variant_data, filter_changed_loci=filter_changed_loci); reference_data = vds.reference_data. if 'LGT' in reference_data.entry:; if 'GT' in reference_data.entry:; reference_data = reference_data.drop('LGT'); else:; reference_data = reference_data.transmute_entries(GT=reference_data.LGT). return VariantDataset(reference_data=reference_data, variant_data=variant_data). @typecheck(ref=MatrixTable, intervals=Table); def segment_reference_blocks(ref: 'MatrixTable', intervals: 'Table') -> 'MatrixTable':; """"""Returns a matrix table of reference blocks segmented according to intervals. Loci outside the given intervals are discarded. Reference blocks that start before; but span an interval will appear at the interval start locus. Note; ----; Assumes disjoint intervals which do not span contigs. Requires start-inclusive intervals. Parameters; ----------; ref : :clas",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:33191,Deployability,patch,patch,33191,"reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None):; """"""Cap reference blocks at a maximum length in order to permit faster interval filtering. Examples; --------; Truncate reference blocks to 5 kilobases:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) # doctest: +SKIP. Truncate the longest 1% of reference blocks to the length of the 99th percentile block:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) # doctest: +SKIP. Notes; -----; After this function has been run, the reference blocks have a known maximum length `ref_block_max_length`,; stored in the global fields, which permits :func:`.vds.filter_intervals` to filter to intervals of the reference; data by reading `ref_block_max_length` bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work. It is also possible to patch an existing VDS to store the max reference block length with :func:`.vds.store_ref_block_max_length`. See Also; --------; :func:`.vds.store_ref_block_max_length`. Parameters; ----------; vds : :class:`.VariantDataset` or :class:`.MatrixTable`; max_ref_block_base_pairs; Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction; Fraction of reference block length distribution to truncate / winsorize. Returns; -------; :class:`.VariantDataset` or :class:`.MatrixTable`; """"""; if isinstance(ds, VariantDataset):; rd = ds.reference_data; else:; rd = ds. fd_name = hl.vds.VariantDataset.ref_block_max_length_field; if fd_name in rd.globals:; rd = rd.drop(fd_name). if int(ref_block_winsorize_fraction is None) + int(max_ref_block_base_pairs is None) != 1:; raise ValueError(; 'truncate_reference_blocks: require exactly one of ""max_ref_block_base_pairs"", ""ref_block_winsorize_fraction""'; ). if ref_block_winsorize_fraction is not None:; assert (; ref_block_winsorize_fraction > 0 and ref_block_winsoriz",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:41193,Deployability,update,updated,41193,"(hl.missing(rd.entry.dtype), False), lambda acc: keep_last(acc, (elt, False)), keep_last; ),; ht.entries,; ),; ht.entries,; ).map(lambda tup: keep_last(tup[0], (tup[1], False))); ); ht_join = ht. ht = ht.key_by(); ht = ht.select(; to_shuffle=hl.enumerate(ht.prev_block).filter(; lambda idx_and_elt: hl.is_defined(idx_and_elt[1]) & idx_and_elt[1][1]; ); ); ht = ht.explode('to_shuffle'); rg = rd.locus.dtype.reference_genome; ht = ht.transmute(col_idx=ht.to_shuffle[0], entry=ht.to_shuffle[1][0]); ht_shuf = ht.key_by(; locus=hl.locus(hl.literal(rg.contigs)[ht.entry.contig_idx], ht.entry.start_pos, reference_genome=rg); ). ht_shuf = ht_shuf.collect_by_key(""new_starts""); # new_starts can contain multiple records for a collapsed ref block, one for each folded block.; # We want to keep the one with the highest END; ht_shuf = ht_shuf.select(; moved_blocks_dict=hl.group_by(lambda elt: elt.col_idx, ht_shuf.new_starts).map_values(; lambda arr: arr[hl.argmax(arr.map(lambda x: x.entry.END))].entry.drop('contig_idx', 'start_pos'); ); ). ht_joined = ht_join.join(ht_shuf.select_globals(), 'left'). def merge_f(tup):; (idx, original_entry) = tup. return (; hl.case(); .when(; ~(hl.coalesce(ht_joined.prev_block[idx][1], False)),; hl.coalesce(ht_joined.moved_blocks_dict.get(idx), original_entry.drop('contig_idx', 'start_pos')),; ); .or_missing(); ). ht_joined = ht_joined.annotate(new_entries=hl.enumerate(ht_joined.entries).map(lambda tup: merge_f(tup))); ht_joined = ht_joined.drop('moved_blocks_dict', 'entries', 'prev_block', 'contig_idx_row', 'start_pos_row'); new_rd = ht_joined._unlocalize_entries(; entries_field_name='new_entries', cols_field_name='cols', col_key=list(rd.col_key); ). rbml = hl.vds.VariantDataset.ref_block_max_length_field; if rbml in new_rd.globals:; new_rd = new_rd.drop(rbml). if isinstance(ds, VariantDataset):; return VariantDataset(reference_data=new_rd, variant_data=ds.variant_data); return new_rd.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:14916,Energy Efficiency,efficient,efficient,14916,"t_type),; key='interval',; ); else:; key_dtype = calling_intervals.key.dtype; if (; len(key_dtype) != 1; or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.referen",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:21561,Modifiability,extend,extend,21561,"antDataset` with those chromosomes; removed.; - ``keep_autosomes``: This argument expects the value ``True``, and returns a dataset without; sex and mitochondrial chromosomes. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset.; keep; Keep a specified list of contigs.; remove; Remove a specified list of contigs; keep_autosomes; If true, keep only autosomal chromosomes. Returns; -------; :class:`.VariantDataset`.; """""". n_args_passed = (keep is not None) + (remove is not None) + keep_autosomes; if n_args_passed == 0:; raise ValueError(""filter_chromosomes: expect one of 'keep', 'remove', or 'keep_autosomes' arguments""); if n_args_passed > 1:; raise ValueError(; ""filter_chromosomes: expect ONLY one of 'keep', 'remove', or 'keep_autosomes' arguments""; ""\n In order use 'keep_autosomes' with 'keep' or 'remove', call the function twice""; ). rg = vds.reference_genome. to_keep = []. if keep is not None:; keep = wrap_to_list(keep); to_keep.extend(keep); elif remove is not None:; remove = set(wrap_to_list(remove)); for c in rg.contigs:; if c not in remove:; to_keep.append(c); elif keep_autosomes:; to_remove = set(rg.x_contigs + rg.y_contigs + rg.mt_contigs); for c in rg.contigs:; if c not in to_remove:; to_keep.append(c). parsed_intervals = hl.literal(to_keep, hl.tarray(hl.tstr)).map(; lambda c: hl.parse_locus_interval(c, reference_genome=rg); ); return _parameterized_filter_intervals(vds, intervals=parsed_intervals, keep=True, mode='unchecked_filter_both'). [docs]@typecheck(; vds=VariantDataset,; intervals=oneof(Table, expr_array(expr_interval(expr_any))),; split_reference_blocks=bool,; keep=bool,; ); def filter_intervals(; vds: 'VariantDataset', intervals, *, split_reference_blocks: bool = False, keep: bool = True; ) -> 'VariantDataset':; """"""Filter intervals in a :class:`.VariantDataset`. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; intervals : :class:`.Table` or :class:`.ArrayExpression` of type :class:`.tint",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:27277,Modifiability,rewrite,rewrite,27277,"ambda idx_and_e: hl.rbind(; idx_and_e[0],; idx_and_e[1],; lambda idx, e: hl.coalesce(; joined._ref_entries[idx],; hl.or_missing((e.__contig_idx == joined.__contig_idx) & (e.END >= pos), e),; ),; ).drop('__contig_idx'); ),; ),; ); ); dense = dense.filter(dense._include_locus).drop('_interval_dup', '_include_locus', '__contig_idx'). # at this point, 'dense' is a table with dense rows of reference blocks, keyed by locus. refl_filtered = refl.annotate(**{interval_field: intervals[refl.locus]._interval_dup}). # remove rows that are not contained in an interval, and rows that are the start of an; # interval (interval starts come from the 'dense' table); refl_filtered = refl_filtered.filter(; hl.is_defined(refl_filtered[interval_field]) & (refl_filtered.locus != refl_filtered[interval_field].start); ). # union dense interval starts with filtered table; refl_filtered = refl_filtered.union(dense.transmute(_ref_entries=dense.dense_ref)). # rewrite reference blocks to end at the first of (interval end, reference block end); refl_filtered = refl_filtered.annotate(; interval_end=refl_filtered[interval_field].end.position - ~refl_filtered[interval_field].includes_end; ); refl_filtered = refl_filtered.annotate(; _ref_entries=refl_filtered._ref_entries.map(; lambda entry: entry.annotate(END=hl.min(entry.END, refl_filtered.interval_end)); ); ). return refl_filtered._unlocalize_entries('_ref_entries', '_ref_cols', list(ref.col_key)). [docs]@typecheck(; vds=VariantDataset,; intervals=Table,; gq_thresholds=sequenceof(int),; dp_thresholds=sequenceof(int),; dp_field=nullable(str),; ); def interval_coverage(; vds: VariantDataset,; intervals: Table,; gq_thresholds=(; 0,; 10,; 20,; ),; dp_thresholds=(0, 1, 10, 20, 30),; dp_field=None,; ) -> 'MatrixTable':; """"""Compute statistics about base coverage by interval. Returns a :class:`.MatrixTable` with interval row keys and sample column keys. Contains the following row fields:; - ``interval`` (*interval*): Genomic interval of interest.; - ``inte",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:2984,Testability,assert,assert,2984,"IXME(chrisvittal) consider changing END semantics on VDS to make this better; # see https://github.com/hail-is/hail/issues/13183 for why this is here and more discussion; # we assume that END <= contig.length; ref = ref.annotate_rows(_locus_global_pos=ref.locus.global_position(), _locus_pos=ref.locus.position); ref = ref.transmute_entries(_END_GLOBAL=ref._locus_global_pos + (ref.END - ref._locus_pos)). to_drop = 'alleles', 'rsid', 'ref_allele', '_locus_global_pos', '_locus_pos'; ref = ref.drop(*(x for x in to_drop if x in ref.row)); var = vds.variant_data; refl = ref.localize_entries('_ref_entries'); varl = var.localize_entries('_var_entries', '_var_cols'); varl = varl.annotate(_variant_defined=True); joined = varl.key_by('locus').join(refl, how='outer'); dr = joined.annotate(; dense_ref=hl.or_missing(; joined._variant_defined, hl.scan._densify(hl.len(joined._var_cols), joined._ref_entries); ); ); dr = dr.filter(dr._variant_defined). def coalesce_join(ref, var):; call_field = 'GT' if 'GT' in var else 'LGT'; assert call_field in var, var.dtype. if call_field not in ref:; ref_call_field = 'GT' if 'GT' in ref else 'LGT'; if ref_call_field not in ref:; ref = ref.annotate(**{call_field: hl.call(0, 0)}); else:; ref = ref.annotate(**{call_field: ref[ref_call_field]}). # call_field is now in both ref and var; ref_set, var_set = set(ref.dtype), set(var.dtype); shared_fields, var_fields = var_set & ref_set, var_set - ref_set. return hl.if_else(; hl.is_defined(var),; var.select(*shared_fields, *var_fields),; ref.select(*shared_fields, **{f: hl.missing(var[f].dtype) for f in var_fields}),; ). dr = dr.annotate(; _dense=hl.rbind(; dr._ref_entries,; lambda refs_at_this_row: hl.enumerate(hl.zip(dr._var_entries, dr.dense_ref)).map(; lambda tup: coalesce_join(; hl.coalesce(; refs_at_this_row[tup[0]],; hl.or_missing(tup[1][1]._END_GLOBAL >= dr.locus.global_position(), tup[1][1]),; ),; tup[1][0],; ); ),; ),; ). dr = dr._key_by_assert_sorted('locus', 'alleles'); fields_to_drop = ['_var_",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:34152,Testability,assert,assert,34152,"to store the max reference block length with :func:`.vds.store_ref_block_max_length`. See Also; --------; :func:`.vds.store_ref_block_max_length`. Parameters; ----------; vds : :class:`.VariantDataset` or :class:`.MatrixTable`; max_ref_block_base_pairs; Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction; Fraction of reference block length distribution to truncate / winsorize. Returns; -------; :class:`.VariantDataset` or :class:`.MatrixTable`; """"""; if isinstance(ds, VariantDataset):; rd = ds.reference_data; else:; rd = ds. fd_name = hl.vds.VariantDataset.ref_block_max_length_field; if fd_name in rd.globals:; rd = rd.drop(fd_name). if int(ref_block_winsorize_fraction is None) + int(max_ref_block_base_pairs is None) != 1:; raise ValueError(; 'truncate_reference_blocks: require exactly one of ""max_ref_block_base_pairs"", ""ref_block_winsorize_fraction""'; ). if ref_block_winsorize_fraction is not None:; assert (; ref_block_winsorize_fraction > 0 and ref_block_winsorize_fraction < 1; ), 'truncate_reference_blocks: ""ref_block_winsorize_fraction"" must be between 0 and 1 (e.g. 0.01 to truncate the top 1% of reference blocks)'; if ref_block_winsorize_fraction > 0.1:; warning(; f""'truncate_reference_blocks': ref_block_winsorize_fraction of {ref_block_winsorize_fraction} will lead to significant data duplication,""; f"" recommended values are <0.05.""; ); max_ref_block_base_pairs = rd.aggregate_entries(; hl.agg.approx_quantiles(rd.END - rd.locus.position + 1, 1 - ref_block_winsorize_fraction, k=200); ). assert (; max_ref_block_base_pairs > 0; ), 'truncate_reference_blocks: ""max_ref_block_base_pairs"" must be between greater than zero'; info(f""splitting VDS reference blocks at {max_ref_block_base_pairs} base pairs""). rd_under_limit = rd.filter_entries(rd.END - rd.locus.position < max_ref_block_base_pairs).localize_entries(; 'fixed_blocks', 'cols'; ). rd_over_limit = rd.filter_entries(rd.END - rd.locus.position >= max_ref_block_base_pairs).key_cols_by(; col_",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/methods.html:34754,Testability,assert,assert,34754," rd.drop(fd_name). if int(ref_block_winsorize_fraction is None) + int(max_ref_block_base_pairs is None) != 1:; raise ValueError(; 'truncate_reference_blocks: require exactly one of ""max_ref_block_base_pairs"", ""ref_block_winsorize_fraction""'; ). if ref_block_winsorize_fraction is not None:; assert (; ref_block_winsorize_fraction > 0 and ref_block_winsorize_fraction < 1; ), 'truncate_reference_blocks: ""ref_block_winsorize_fraction"" must be between 0 and 1 (e.g. 0.01 to truncate the top 1% of reference blocks)'; if ref_block_winsorize_fraction > 0.1:; warning(; f""'truncate_reference_blocks': ref_block_winsorize_fraction of {ref_block_winsorize_fraction} will lead to significant data duplication,""; f"" recommended values are <0.05.""; ); max_ref_block_base_pairs = rd.aggregate_entries(; hl.agg.approx_quantiles(rd.END - rd.locus.position + 1, 1 - ref_block_winsorize_fraction, k=200); ). assert (; max_ref_block_base_pairs > 0; ), 'truncate_reference_blocks: ""max_ref_block_base_pairs"" must be between greater than zero'; info(f""splitting VDS reference blocks at {max_ref_block_base_pairs} base pairs""). rd_under_limit = rd.filter_entries(rd.END - rd.locus.position < max_ref_block_base_pairs).localize_entries(; 'fixed_blocks', 'cols'; ). rd_over_limit = rd.filter_entries(rd.END - rd.locus.position >= max_ref_block_base_pairs).key_cols_by(; col_idx=hl.scan.count(); ); rd_over_limit = rd_over_limit.select_rows().select_cols().key_rows_by().key_cols_by(); es = rd_over_limit.entries(); es = es.annotate(new_start=hl.range(es.locus.position, es.END + 1, max_ref_block_base_pairs)); es = es.explode('new_start'); es = es.transmute(; locus=hl.locus(es.locus.contig, es.new_start, reference_genome=es.locus.dtype.reference_genome),; END=hl.min(es.new_start + max_ref_block_base_pairs - 1, es.END),; ); es = es.key_by(es.locus).collect_by_key(""new_blocks""); es = es.transmute(moved_blocks_dict=hl.dict(es.new_blocks.map(lambda x: (x.col_idx, x.drop('col_idx'))))). joined = rd_under_limit.join(es,",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html:13298,Deployability,update,updated,13298,"(GQ) scores.; dp_bins : :class:`tuple` of :obj:`int`; Tuple containing cutoffs for depth (DP) scores.; dp_field : :obj:`str`; Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns; -------; :class:`.Table`; Hail Table of results, keyed by sample.; """""". require_first_key_field_locus(vds.reference_data, 'sample_qc'); require_first_key_field_locus(vds.variant_data, 'sample_qc'). if dp_field is not None:; ref_dp_field_to_use = dp_field; elif 'DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'DP'; elif 'MIN_DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'MIN_DP'; else:; ref_dp_field_to_use = None. vmt = vds.variant_data; if 'GT' not in vmt.entry:; vmt = vmt.annotate_entries(GT=hl.vds.lgt_to_gt(vmt.LGT, vmt.LA)); allele_count, atypes = vmt_sample_qc_variant_annotations(global_gt=vmt.GT, alleles=vmt.alleles); variant_ac = Env.get_uid(); variant_atypes = Env.get_uid(); vmt = vmt.annotate_rows(**{variant_ac: allele_count, variant_atypes: atypes}); vmt_dp = vmt['DP'] if ref_dp_field_to_use is not None and 'DP' in vmt.entry else None; variant_results = vmt.select_cols(; **vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; variant_ac=vmt[variant_ac],; variant_atypes=vmt[variant_atypes],; dp=vmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). rmt = vds.reference_data; rmt_dp = rmt[ref_dp_field_to_use] if ref_dp_field_to_use is not None else None; reference_results = rmt.select_cols(; **rmt_sample_qc(; locus=rmt.locus,; gq=rmt.GQ,; end=rmt.END,; dp=rmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). joined = reference_results[variant_results.key]; dp_bins_field = {}; if ref_dp_field_to_use is not None:; dp_bins_field['dp_bins'] = hl.tuple(dp_bins); joined_results = variant_results.transmute(**combine_sample_qc(joined, variant_results.row)); joined_results = joined_results.annotate_globals(gq_bins=hl.tuple(gq_bins), **dp_bins_field); return joined_results.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/sample_qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2924,Availability,down,downstream,2924,".path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_block_max_length=False). if VariantDataset.ref_block_max_length_field in vds.reference_data.globals:; warning(f""VDS at {vds_path} already contains a global annotation with the max reference block length""); return; rd = vds.reference_data; rd = rd.annotate_rows(__start_pos=rd.locus.position); fs = hl.current_backend().fs; ref_block_max_len = rd.aggregate_entries(hl.agg.max(rd.END - rd.__start_pos + 1)); with fs.open(os.path.join(vds_path, extra_ref_globals_file), 'w') as f:; json.dump({VariantDataset.",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:7726,Availability,checkpoint,checkpoint,7726,"END field with non-reference genotype at '; ); + hl.str(mt.locus); + hl.str(' / '); + hl.str(mt.col_key[0]); ); ); rmt = rmt.select_entries(*(x for x in rmt.entry if x in used_ref_block_fields)); rmt = rmt.filter_rows(hl.agg.count() > 0). rmt = rmt.key_rows_by('locus').select_rows().select_cols(). if is_split:; rmt = rmt.distinct_by_row(). vmt = mt.filter_entries(hl.is_missing(mt.END)).drop('END')._key_rows_by_assert_sorted('locus', 'alleles'); vmt = vmt.filter_rows(hl.agg.count() > 0). return VariantDataset(rmt, vmt). def __init__(self, reference_data: MatrixTable, variant_data: MatrixTable):; self.reference_data: MatrixTable = reference_data; self.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_k",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8410,Availability,error,error,8410,"lf.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstr",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8662,Availability,error,error,8662,"*kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int3",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9015,Availability,error,error,9015,"reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9265,Availability,error,error,9265,"sary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9491,Availability,error,error,9491,"tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_m",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9638,Availability,error,error,9638,"""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((r",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9842,Availability,error,error,9842,"key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.loc",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:10091,Availability,error,error,10091,": array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:10358,Availability,error,error,10358,"t isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found recor",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:11021,Availability,error,error,11021,"first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.unio",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:11169,Availability,error,error,11169,"n {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) # doctest: +SKIP. """""". fd = hl.vds.VariantDataset.ref_block_max_length_field; mts = [vds.reference_data for vds in vdses]; n_",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:11375,Availability,error,error,11375,"t: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) # doctest: +SKIP. """""". fd = hl.vds.VariantDataset.ref_block_max_length_field; mts = [vds.reference_data for vds in vdses]; n_with_ref_max_len = len([mt for mt in mts if fd in mt.globals]); any_ref_max = n_with_ref_max_len > 0; all_ref_max = n_with_ref_max_len == len(mts). # if some mts have max ref len but not all, drop it;",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2442,Deployability,patch,patch,2442,"ntDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2832,Deployability,patch,patch,2832,"ax_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_block_max_length=False). if VariantDataset.ref_block_max_length_field in vds.reference_data.globals:; warning(f""VDS at {vds_path} already contains a global annotation with the max reference block length""); return; rd = vds.reference_data; rd = rd.annotate_rows(__start_pos=rd.locus.position); fs = hl.current_backend().fs; ref_block_max_len = rd.aggregate_entries(hl.agg.max(rd.END - rd.__start_pos + 1))",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:12847,Deployability,update,updated,12847,".locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) # doctest: +SKIP. """""". fd = hl.vds.VariantDataset.ref_block_max_length_field; mts = [vds.reference_data for vds in vdses]; n_with_ref_max_len = len([mt for mt in mts if fd in mt.globals]); any_ref_max = n_with_ref_max_len > 0; all_ref_max = n_with_ref_max_len == len(mts). # if some mts have max ref len but not all, drop it; if all_ref_max:; new_ref_mt = hl.MatrixTable.union_rows(*mts).annotate_globals(**{; fd: hl.max([mt.index_globals()[fd] for mt in mts]); }); else:; if any_ref_max:; mts = [mt.drop(fd) if fd in mt.globals else mt for mt in mts]; new_ref_mt = hl.MatrixTable.union_rows(*mts). new_var_mt = hl.MatrixTable.union_rows(*(vds.variant_data for vds in vdses)); return hl.vds.VariantDataset(new_ref_mt, new_var_mt).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2081,Performance,load,load,2081,"eturns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make inter",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:7475,Security,validat,validate,7475,"(f""\n {k!r}"" for k in mt.entry if k in used_ref_block_fields); ). rmt = mt.filter_entries(; hl.case(); .when(hl.is_missing(mt.END), False); .when(hl.is_defined(mt.END) & mt[gt_field].is_hom_ref(), True); .or_error(; hl.str(; 'cannot create VDS from merged representation -' ' found END field with non-reference genotype at '; ); + hl.str(mt.locus); + hl.str(' / '); + hl.str(mt.col_key[0]); ); ); rmt = rmt.select_entries(*(x for x in rmt.entry if x in used_ref_block_fields)); rmt = rmt.filter_rows(hl.agg.count() > 0). rmt = rmt.key_rows_by('locus').select_rows().select_cols(). if is_split:; rmt = rmt.distinct_by_row(). vmt = mt.filter_entries(hl.is_missing(mt.END)).drop('END')._key_rows_by_assert_sorted('locus', 'alleles'); vmt = vmt.filter_rows(hl.agg.count() > 0). return VariantDataset(rmt, vmt). def __init__(self, reference_data: MatrixTable, variant_data: MatrixTable):; self.reference_data: MatrixTable = reference_data; self.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8239,Security,validat,validate,8239,"riantDataset(rmt, vmt). def __init__(self, reference_data: MatrixTable, variant_data: MatrixTable):; self.reference_data: MatrixTable = reference_data; self.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8446,Security,validat,validate,8446," variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:1372,Testability,assert,assert,1372,"ces; Change Log And Version Policy. menu; Hail. Module code; hail.vds.variant_dataset. Source code for hail.vds.variant_dataset; import json; import os. import hail as hl; from hail.genetics import ReferenceGenome; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck_method; from hail.utils.java import info, warning. extra_ref_globals_file = 'extra_reference_globals.json'. [docs]def read_vds(; path,; *,; intervals=None,; n_partitions=None,; _assert_reference_type=None,; _assert_variant_type=None,; _warn_no_ref_block_max_length=True,; ) -> 'VariantDataset':; """"""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:1550,Testability,assert,assert,1550,".genetics import ReferenceGenome; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck_method; from hail.utils.java import info, warning. extra_ref_globals_file = 'extra_reference_globals.json'. [docs]def read_vds(; path,; *,; intervals=None,; n_partitions=None,; _assert_reference_type=None,; _assert_variant_type=None,; _warn_no_ref_block_max_length=True,; ) -> 'VariantDataset':; """"""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [doc",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:12871,Availability,down,downstream,12871,"aining two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y  values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; raw_res = _agg_func(; 'ApproxCDF',; [hl.float64(expr)],; tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)),; init_op_args=[k],; ); conv = {; tint32: lambda x: x.map(hl.int),; tint64: lambda x: x.map(hl.int64),; tfloat32: lambda x: x.map(hl.float32),; tfloat64: identity,; }; if _raw:; return raw_res; else:; raw_res = raw_res.annotate(items=conv[expr.dtype](raw_res['items'])); return _result_from_raw_cdf(raw_res). [docs]@typecheck(expr=expr_numeric, qs=expr_oneof(expr_numeric, expr_array(expr_numeric)), k=int); def approx_quantiles(expr, qs, k=100) -> Expression:; """,MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:12882,Availability,error,error,12882,"aining two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y  values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; raw_res = _agg_func(; 'ApproxCDF',; [hl.float64(expr)],; tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)),; init_op_args=[k],; ); conv = {; tint32: lambda x: x.map(hl.int),; tint64: lambda x: x.map(hl.int64),; tfloat32: lambda x: x.map(hl.float32),; tfloat64: identity,; }; if _raw:; return raw_res; else:; raw_res = raw_res.annotate(items=conv[expr.dtype](raw_res['items'])); return _result_from_raw_cdf(raw_res). [docs]@typecheck(expr=expr_numeric, qs=expr_oneof(expr_numeric, expr_array(expr_numeric)), k=int); def approx_quantiles(expr, qs, k=100) -> Expression:; """,MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44073,Availability,down,downsample,44073,"; bin_edges=hl.range(0, nbins + 1).map(lambda i: s + i * bs),; bin_freq=hl.range(0, nbins).map(lambda i: freq_dict.get(i, 0)),; n_smaller=freq_dict.get(-1, 0),; n_larger=freq_dict.get(nbins, 0),; ). def wrap_errors(s, e, nbins, freq_dict):; return (; hl.case(); .when(; nbins > 0,; hl.bind(; lambda bs: hl.case(); .when((bs > 0) & hl.is_finite(bs), result(s, nbins, bs, freq_dict)); .or_error(; ""'hist': start=""; + hl.str(s); + "" end=""; + hl.str(e); + "" bins=""; + hl.str(nbins); + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44256,Availability,down,downsampled,44256,"get(nbins, 0),; ). def wrap_errors(s, e, nbins, freq_dict):; return (; hl.case(); .when(; nbins > 0,; hl.bind(; lambda bs: hl.case(); .when((bs > 0) & hl.is_finite(bs), result(s, nbins, bs, freq_dict)); .or_error(; ""'hist': start=""; + hl.str(s); + "" end=""; + hl.str(e); + "" bins=""; + hl.str(nbins); + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [e",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44318,Availability,down,downsampled,44318,"eturn (; hl.case(); .when(; nbins > 0,; hl.bind(; lambda bs: hl.case(); .when((bs > 0) & hl.is_finite(bs), result(s, nbins, bs, freq_dict)); .or_error(; ""'hist': start=""; + hl.str(s); + "" end=""; + hl.str(e); + "" bins=""; + hl.str(nbins); + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [expr], tarray(expr.dtype), [n]). [docs]@typecheck(gp=expr_array",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44548,Availability,down,downsample,44548,"; + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [expr], tarray(expr.dtype), [n]). [docs]@typecheck(gp=expr_array(expr_float64)); def info_score(gp) -> StructExpression:; r""""""Compute the IMPUTE information score. Examples; --------; Calculate the info score per variant:. >>> gen_mt = hl.import_gen('data/example.gen', sample_file='data/example.sa",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44692,Availability,down,downsampled,44692,"ins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [expr], tarray(expr.dtype), [n]). [docs]@typecheck(gp=expr_array(expr_float64)); def info_score(gp) -> StructExpression:; r""""""Compute the IMPUTE information score. Examples; --------; Calculate the info score per variant:. >>> gen_mt = hl.import_gen('data/example.gen', sample_file='data/example.sample'); >>> gen_mt = gen_mt.annotate_rows(info_score = hl.agg.info_score(gen_mt.GP)). Calculate group-specific info scores per variant:. >>> gen_mt = hl.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:51671,Availability,error,error,51671," of genotype and age:. >>> ds_ann = ds.annotate_rows(linreg =; ... hl.agg.linreg(ds.pheno.blood_pressure,; ... [1,; ... ds.GT.n_alt_alleles(),; ... ds.pheno.age,; ... ds.GT.n_alt_alleles() * ds.pheno.age])). Warning; -------; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----; In relation to; `lm.summary <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/summary.lm.html>`__; in R, ``linreg(y, x = [1, mt.x1, mt.x2])`` computes; ``summary(lm(y ~ x1 + x2))`` and; ``linreg(y, x = [mt.x1, mt.x2], nested_dim=0)`` computes; ``summary(lm(y ~ x1 + x2 - 1))``. More generally, `nested_dim` defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; numb",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:51969,Availability,error,error,51969,"st be included; **explicitly** if desired. Notes; -----; In relation to; `lm.summary <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/summary.lm.html>`__; in R, ``linreg(y, x = [1, mt.x1, mt.x2])`` computes; ``summary(lm(y ~ x1 + x2))`` and; ``linreg(y, x = [mt.x1, mt.x2], nested_dim=0)`` computes; ``summary(lm(y ~ x1 + x2 - 1))``. More generally, `nested_dim` defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning;",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:54546,Availability,error,error,54546,"s.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nested_dim; covs_defined = hl.all(lambda cov: hl.is_defined(cov), x); tup = hl.agg.filter(covs_defined, hl.tuple([hl.agg.count_where(hl.is_defined(y)), hl.agg.sum(y * y)])); n = tup[0]; yty = tup[1]. def result_from_agg(linreg_res, n, k, k0, yty):; xty = linreg_res.xty; beta = linreg_res.beta; diag_inv = linreg_res.diag_inv; beta0 = linreg_res.beta0. def dot(a, b):; return hl.sum(a * b). d = n - k; rss = yty - dot(xty, beta); rse2 = rss / d # residual standard error squared; se = (rse2 * diag_inv) ** 0.5; t = beta / se; p = t.map(lambda ti: 2 * hl.pT(-hl.abs(ti), d, True, False)); rse = hl.sqrt(rse2). d0 = k - k0; xty0 = xty[:k0]; rss0 = yty - dot(xty0, beta0); r2 = 1 - rss / rss0; r2adj = 1 - (1 - r2) * (n - k0) / d; f = (rss0 - rss) * d / (rss * d0); p0 = hl.pF(f, d0, d, False, False). return hl.struct(; beta=beta,; standard_error=se,; t_stat=t,; p_value=p,; multiple_standard_error=rse,; multiple_r_squared=r2,; adjusted_r_squared=r2adj,; f_stat=f,; multiple_p_value=p0,; n=n,; ). global _result_from_linreg_agg_f; if _result_from_linreg_agg_f is None:; _result_from_linreg_agg_f = hl.experimental.define_function(; result_from_agg, res_type, hl.tint64, hl.tint32, hl.tint32, hl.tfloat64, _name=""linregResFromAgg""; ). return _result_from_linreg_agg_f(temp, n, k, k0, yty). [docs]@typecheck(x=expr_float64, y=expr_float64); def corr(x, y) -> Float64Expression:; """"""Computes the; `Pearson correlation coefficient <https://en.wikipedia.o",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:62023,Deployability,update,updated,62023,"ons(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (:class:`.Expression`) -> :class:`.Expression`); The function used to combine the current aggregator state with the next element you're aggregating over. Type is; `A => A`; comb_op : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); The function used to combine two aggregator states together and produce final result. Type is `(A, A) => A`.; """""". return _agg_func._fold(initial_value, seq_op, comb_op).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:582,Integrability,wrap,wraps,582,". Hail | ; hail.expr.aggregators.aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.aggregators.aggregators. Source code for hail.expr.aggregators.aggregators; import difflib; from functools import update_wrapper, wraps. import hail as hl; from hail import ir; from hail.expr import (; Aggregation,; ArrayExpression,; BooleanExpression,; DictExpression,; Expression,; ExpressionException,; Float64Expression,; Indices,; Int64Expression,; NDArrayNumericExpression,; NumericExpression,; SetExpression,; StringExpression,; StructExpression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; to_expr,; unify_all,; unify_types,; ); from hail.expr.expressions.typed_expressions import construct_variable; from hail.expr.functions import _quantile_from_cdf, _result_from_raw_cdf, float32, rbind; from hail.expr.types import (; hail_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import TypeChecker, func_spec, identity, nullable, oneof, sequenceof, typecheck, typecheck_method; from hail.utils import wrap_to_list; from hail.utils.java import Env. class AggregableChecker(TypeChecker):; def __init__(self, coercer):; self.coercer = coercer; super(AggregableChecker, self).__init__(). def expects(self):; return self.coercer.expects(). def format(self, arg):; return self.coercer.format(arg). def check(self, x, caller, param):; x = self.coercer.check(x, caller, param); if len(x._ir.search(lambda node: isinstance(nod",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:2186,Integrability,interface,interface,2186,"rt construct_variable; from hail.expr.functions import _quantile_from_cdf, _result_from_raw_cdf, float32, rbind; from hail.expr.types import (; hail_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import TypeChecker, func_spec, identity, nullable, oneof, sequenceof, typecheck, typecheck_method; from hail.utils import wrap_to_list; from hail.utils.java import Env. class AggregableChecker(TypeChecker):; def __init__(self, coercer):; self.coercer = coercer; super(AggregableChecker, self).__init__(). def expects(self):; return self.coercer.expects(). def format(self, arg):; return self.coercer.format(arg). def check(self, x, caller, param):; x = self.coercer.check(x, caller, param); if len(x._ir.search(lambda node: isinstance(node, ir.BaseApplyAggOp))) == 0:; raise ExpressionException(; f""{caller} must be placed outside of an aggregation. See ""; ""https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701""; ); return x. agg_expr = AggregableChecker. class AggFunc(object):; def __init__(self):; self._as_scan = False; self._agg_bindings = set(). def correct_prefix(self):; return ""scan"" if self._as_scan else ""agg"". def incorrect_prefix(self):; return ""agg"" if self._as_scan else ""scan"". def correct_plural(self):; return ""scans"" if self._as_scan else ""aggregations"". def incorrect_plural(self):; return ""aggregations"" if self._as_scan else ""scans"". def check_scan_agg_compatibility(self, caller, node):; if self._as_scan != isinstance(node, ir.ApplyScanOp):; raise ExpressionException(; ""'{correct}.{caller}' cannot contain {incorrect}"".format(; correct=self.correct_prefix(), caller=caller, incorrect=self.incorrect_plural(); ); ). @typecheck_method(; agg_op=str, seq_op_args=sequenceof(expr_any), ret_type=hail_type, init_op_args=sequenceof(expr_any); ); def __call__(self, agg_op, seq_op_args, ret_type, init_op_args=()):; indices, aggregations = unify_all(*seq_op_args, *init_op",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:52719,Integrability,depend,dependent,52719,"d standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.sta",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:53090,Integrability,depend,dependent,53090,"_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nest",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:58271,Integrability,wrap,wrap,58271,"Struct(beta=[34.25],; standard_error=[1.75],; t_stat=[19.571428571428573],; p_value=[0.03249975499062629],; multiple_standard_error=4.949747468305833,; multiple_r_squared=0.9973961101073441,; adjusted_r_squared=0.9947922202146882,; f_stat=383.0408163265306,; multiple_p_value=0.03249975499062629,; n=2); }. Compute call statistics stratified by population group and case status:. >>> ann = ds.annotate_rows(call_stats=hl.agg.group_by(hl.struct(pop=ds.pop, is_case=ds.is_case),; ... hl.agg.call_stats(ds.GT, ds.alleles))). Parameters; ----------; group : :class:`.Expression` or :obj:`list` of :class:`.Expression`; Group to stratify the result by.; agg_expr : :class:`.Expression`; Aggregation or scan expression to compute per grouping. Returns; -------; :class:`.DictExpression`; Dictionary where the keys are `group` and the values are the result of computing; `agg_expr` for each unique value of `group`.; """""". return _agg_func.group_by(group, agg_expr). @typecheck(expr=expr_any); def _prev_nonnull(expr) -> ArrayExpression:; wrap = expr.dtype in {tint32, tint64, tfloat32, tfloat64, tbool, tcall}; if wrap:; expr = hl.or_missing(hl.is_defined(expr), hl.tuple([expr])); r = _agg_func('PrevNonnull', [expr], expr.dtype, []); if wrap:; r = r[0]; return r. [docs]@typecheck(f=func_spec(1, expr_any), array=expr_array()); def array_agg(f, array):; """"""Aggregate an array element-wise using a user-specified aggregation function. Examples; --------; Start with a range table with an array of random boolean values:. >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction ``True`` per element:. >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) # doctest: +SKIP_OUTPUT_CHECK; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; -----; This function requires that all values of `array` have the same length. If; two values have different lengths, then an exception will be thrown. The `f` a",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:58347,Integrability,wrap,wrap,58347,"=[0.03249975499062629],; multiple_standard_error=4.949747468305833,; multiple_r_squared=0.9973961101073441,; adjusted_r_squared=0.9947922202146882,; f_stat=383.0408163265306,; multiple_p_value=0.03249975499062629,; n=2); }. Compute call statistics stratified by population group and case status:. >>> ann = ds.annotate_rows(call_stats=hl.agg.group_by(hl.struct(pop=ds.pop, is_case=ds.is_case),; ... hl.agg.call_stats(ds.GT, ds.alleles))). Parameters; ----------; group : :class:`.Expression` or :obj:`list` of :class:`.Expression`; Group to stratify the result by.; agg_expr : :class:`.Expression`; Aggregation or scan expression to compute per grouping. Returns; -------; :class:`.DictExpression`; Dictionary where the keys are `group` and the values are the result of computing; `agg_expr` for each unique value of `group`.; """""". return _agg_func.group_by(group, agg_expr). @typecheck(expr=expr_any); def _prev_nonnull(expr) -> ArrayExpression:; wrap = expr.dtype in {tint32, tint64, tfloat32, tfloat64, tbool, tcall}; if wrap:; expr = hl.or_missing(hl.is_defined(expr), hl.tuple([expr])); r = _agg_func('PrevNonnull', [expr], expr.dtype, []); if wrap:; r = r[0]; return r. [docs]@typecheck(f=func_spec(1, expr_any), array=expr_array()); def array_agg(f, array):; """"""Aggregate an array element-wise using a user-specified aggregation function. Examples; --------; Start with a range table with an array of random boolean values:. >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction ``True`` per element:. >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) # doctest: +SKIP_OUTPUT_CHECK; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; -----; This function requires that all values of `array` have the same length. If; two values have different lengths, then an exception will be thrown. The `f` argument should be a function taking one argument, an expression of; the element typ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:58472,Integrability,wrap,wrap,58472,"t=383.0408163265306,; multiple_p_value=0.03249975499062629,; n=2); }. Compute call statistics stratified by population group and case status:. >>> ann = ds.annotate_rows(call_stats=hl.agg.group_by(hl.struct(pop=ds.pop, is_case=ds.is_case),; ... hl.agg.call_stats(ds.GT, ds.alleles))). Parameters; ----------; group : :class:`.Expression` or :obj:`list` of :class:`.Expression`; Group to stratify the result by.; agg_expr : :class:`.Expression`; Aggregation or scan expression to compute per grouping. Returns; -------; :class:`.DictExpression`; Dictionary where the keys are `group` and the values are the result of computing; `agg_expr` for each unique value of `group`.; """""". return _agg_func.group_by(group, agg_expr). @typecheck(expr=expr_any); def _prev_nonnull(expr) -> ArrayExpression:; wrap = expr.dtype in {tint32, tint64, tfloat32, tfloat64, tbool, tcall}; if wrap:; expr = hl.or_missing(hl.is_defined(expr), hl.tuple([expr])); r = _agg_func('PrevNonnull', [expr], expr.dtype, []); if wrap:; r = r[0]; return r. [docs]@typecheck(f=func_spec(1, expr_any), array=expr_array()); def array_agg(f, array):; """"""Aggregate an array element-wise using a user-specified aggregation function. Examples; --------; Start with a range table with an array of random boolean values:. >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction ``True`` per element:. >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) # doctest: +SKIP_OUTPUT_CHECK; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; -----; This function requires that all values of `array` have the same length. If; two values have different lengths, then an exception will be thrown. The `f` argument should be a function taking one argument, an expression of; the element type of `array`, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; :func:`array_agg` is an array of e",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:60205,Integrability,wrap,wraps,60205," The `f` argument should be a function taking one argument, an expression of; the element type of `array`, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; :func:`array_agg` is an array of elements of the return type of `f`. Parameters; ----------; f :; Aggregation function to apply to each element of the exploded array.; array : :class:`.ArrayExpression`; Array to aggregate. Returns; -------; :class:`.ArrayExpression`; """"""; return _agg_func.array_agg(array, f). @typecheck(expr=expr_str); def _impute_type(expr):; ret_type = hl.dtype(; 'struct{anyNonMissing: bool,'; 'allDefined: bool,'; 'supportsBool: bool,'; 'supportsInt32: bool,'; 'supportsInt64: bool,'; 'supportsFloat64: bool}'; ). return _agg_func('ImputeType', [expr], ret_type, []). class ScanFunctions(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:60219,Integrability,wrap,wrapper,60219," The `f` argument should be a function taking one argument, an expression of; the element type of `array`, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; :func:`array_agg` is an array of elements of the return type of `f`. Parameters; ----------; f :; Aggregation function to apply to each element of the exploded array.; array : :class:`.ArrayExpression`; Array to aggregate. Returns; -------; :class:`.ArrayExpression`; """"""; return _agg_func.array_agg(array, f). @typecheck(expr=expr_str); def _impute_type(expr):; ret_type = hl.dtype(; 'struct{anyNonMissing: bool,'; 'allDefined: bool,'; 'supportsBool: bool,'; 'supportsInt32: bool,'; 'supportsInt64: bool,'; 'supportsFloat64: bool}'; ). return _agg_func('ImputeType', [expr], ret_type, []). class ScanFunctions(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:60542,Integrability,wrap,wrapper,60542,"ation function to apply to each element of the exploded array.; array : :class:`.ArrayExpression`; Array to aggregate. Returns; -------; :class:`.ArrayExpression`; """"""; return _agg_func.array_agg(array, f). @typecheck(expr=expr_str); def _impute_type(expr):; ret_type = hl.dtype(; 'struct{anyNonMissing: bool,'; 'allDefined: bool,'; 'supportsBool: bool,'; 'supportsInt32: bool,'; 'supportsInt64: bool,'; 'supportsFloat64: bool}'; ). return _agg_func('ImputeType', [expr], ret_type, []). class ScanFunctions(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:60562,Integrability,wrap,wrapper,60562,"ation function to apply to each element of the exploded array.; array : :class:`.ArrayExpression`; Array to aggregate. Returns; -------; :class:`.ArrayExpression`; """"""; return _agg_func.array_agg(array, f). @typecheck(expr=expr_str); def _impute_type(expr):; ret_type = hl.dtype(; 'struct{anyNonMissing: bool,'; 'allDefined: bool,'; 'supportsBool: bool,'; 'supportsInt32: bool,'; 'supportsInt64: bool,'; 'supportsFloat64: bool}'; ). return _agg_func('ImputeType', [expr], ret_type, []). class ScanFunctions(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:11544,Modifiability,variab,variables,11544,"nt_type. var = Env.get_uid(base='agg'); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); aggregated = f(ref). if not aggregated._aggregations:; raise ExpressionException(; ""'hl.aggregate_local_array' "" ""must take a mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); if isinstance(array.dtype, tarray):; stream = ir.toStream(array._ir); else:; stream = array._ir; return construct_expr(; ir.StreamAgg(stream, var, aggregated._ir),; aggregated.dtype,; Indices(indices.source, indices.axes),; array._aggregations,; ). _agg_func = AggFunc(). def _check_agg_bindings(expr, bindings):; bound_references = {; ref.name; for ref in expr._ir.search(; lambda x: (; isinstance(x, ir.Ref); and not isinstance(x, ir.TopLevelReference); and not x.name.startswith('__uid_scan'); and not x.name.startswith('__uid_agg'); and not x.name == '__rng_state'; ); ); }; free_variables = bound_references - expr._ir.bound_variables - bindings; if free_variables:; raise ExpressionException(; ""dynamic variables created by 'hl.bind' or lambda methods like 'hl.map' may not be aggregated""; ). [docs]@typecheck(expr=expr_numeric, k=int, _raw=bool); def approx_cdf(expr, k=100, *, _raw=False):; """"""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y  values(i)`. An alternative intuition is that the summary encodes a compressed",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:53100,Modifiability,variab,variable,53100,"_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nest",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:53215,Modifiability,variab,variables,53215,"_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nested_dim; covs_defined = hl.all(lambda cov: hl.is_defined(cov), x); tup = hl.agg.filter(covs_defined, hl.tuple([hl.a",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31278,Performance,perform,performs,31278,"egate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; return hl.bind(; lambda n: hl.if_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are define",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31997,Performance,perform,perform,31997," row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32627,Performance,perform,perform,32627,"4`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_weinberg_test(; counts.get(0, 0), counts.get(1, 0), counts.get(2, 0), one_sided=one_sided; ),; ). [docs]@typecheck(f=func_spec(1, agg_expr(expr_any)), array_agg_expr=expr_oneof(expr_array(), expr_set())); def explode(f, array_agg_expr) -> Expression:; """"""Explode an array or set expression to aggregate the elements of all records. Examples; --------; Compute the mean of all elements in fields `C1`, `C2`, and `C3`:. >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:10319,Security,access,accessing,10319,"type; var = Env.get_uid(); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); self._agg_bindings.add(var); aggregated = f(ref); _check_agg_bindings(aggregated, self._agg_bindings); self._agg_bindings.remove(var). if not self._as_scan and not aggregated._aggregations:; raise ExpressionException(; f""'hl.{self.correct_prefix()}.array_agg' "" f""must take mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); aggregations = hl.utils.LinkedList(Aggregation); if not self._as_scan:; aggregations = aggregations.push(Aggregation(array, aggregated)); return construct_expr(; ir.AggArrayPerElement(array._ir, var, 'unused', aggregated._ir, self._as_scan),; tarray(aggregated.dtype),; Indices(indices.source, aggregated._indices.axes),; aggregations,; ). @property; def context(self):; if self._as_scan:; return 'scan'; else:; return 'agg'. def _aggregate_local_array(array, f):; """"""Compute a summary of an array using aggregators. Useful for accessing; functionality that exists in `hl.agg` but not elsewhere, like `hl.agg.call_stats`. Parameters; ----------; array; f. Returns; -------; Aggregation result.; """"""; elt = array.dtype.element_type. var = Env.get_uid(base='agg'); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); aggregated = f(ref). if not aggregated._aggregations:; raise ExpressionException(; ""'hl.aggregate_local_array' "" ""must take a mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); if isinstance(array.dtype, tarray):; stream = ir.toStream(array._ir); else:; stream = array._ir; return construct_expr(; ir.StreamAgg(stream, var, aggregated._ir),; aggregated.dtype,; Indices(indices.source, indices.axes),; array._aggregations,; ). _agg_func = AggFunc(). def _check_agg_bindings(expr, bindings):; bound_references = {; ref.name; for ref in expr._ir.search(; lambda x: (; isinstance(x, ir.Ref); and not isinstance(x, ir.TopLevelReference); and not x.name.startswith('__uid_scan'); and",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:19372,Testability,test,test,19372,"table1.aggregate(hl.agg.count_where(table1.HT > 68)); 2. Parameters; ----------; condition : :class:`.BooleanExpression`; Criteria for inclusion. Returns; -------; :class:`.Expression` of type :py:data:`.tint64`; Total number of records where `condition` is ``True``.; """""". return _agg_func('Sum', [hl.int64(condition)], tint64). [docs]@typecheck(condition=expr_bool); def any(condition) -> BooleanExpression:; """"""Returns ``True`` if `condition` is ``True`` for any record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(any_over_70 = hl.agg.any(table1.HT > 70)); ... .show()); +-----+-------------+; | SEX | any_over_70 |; +-----+-------------+; | str | bool |; +-----+-------------+; | ""F"" | False |; | ""M"" | True |; +-----+-------------+. Notes; -----; If there are no records to aggregate, the result is ``False``. Missing records are not considered. If every record is missing,; the result is also ``False``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; return count_where(condition) > 0. [docs]@typecheck(condition=expr_bool); def all(condition) -> BooleanExpression:; """"""Returns ``True`` if `condition` is ``True`` for every record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; -----; If there are no records to aggregate, the result is ``True``. Missing records are not considered. If every record is missing,; the result is also ``True``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; return count_where(~condition) == 0. [docs]@typecheck(expr=expr_any, weight=nullable(expr_numeric)); def counter(expr, *, weight=None) -> DictEx",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:20156,Testability,test,test,20156,"re are no records to aggregate, the result is ``False``. Missing records are not considered. If every record is missing,; the result is also ``False``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; return count_where(condition) > 0. [docs]@typecheck(condition=expr_bool); def all(condition) -> BooleanExpression:; """"""Returns ``True`` if `condition` is ``True`` for every record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; -----; If there are no records to aggregate, the result is ``True``. Missing records are not considered. If every record is missing,; the result is also ``True``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; return count_where(~condition) == 0. [docs]@typecheck(expr=expr_any, weight=nullable(expr_numeric)); def counter(expr, *, weight=None) -> DictExpression:; """"""Count the occurrences of each unique record and return a dictionary. Examples; --------; Count the number of individuals for each unique `SEX` value:. >>> table1.aggregate(hl.agg.counter(table1.SEX)); {'F': 2, 'M': 2}; <BLANKLINE>. Compute the total height for each unique `SEX` value:. >>> table1.aggregate(hl.agg.counter(table1.SEX, weight=table1.HT)); {'F': 130, 'M': 137}; <BLANKLINE>. Note that when counting a set-typed field, the values become :class:`.frozenset` s because; Python does not permit the keys of a dictionary to be mutable:. >>> table1.aggregate(hl.agg.counter(hl.set({table1.SEX}), weight=table1.HT)); {frozenset({'F'}): 130, frozenset({'M'}): 137}; <BLANKLINE>. Notes; -----; If you need a more complex grouped aggregation than :func:`counter`; su",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:30899,Testability,test,test,30899,"ression. Returns; -------; :class:`.Expression` of type :py:data:`.tint64` or :py:data:`.tfloat64`; Product of records of `expr`.; """""". return _agg_func('Product', [expr], expr.dtype). [docs]@typecheck(predicate=expr_bool); def fraction(predicate) -> Float64Expression:; """"""Compute the fraction of records where `predicate` is ``True``. Examples; --------; Compute the fraction of rows where `SEX` is ""F"" and `HT` > 65:. >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; return hl.bind(; lambda n: hl.if_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31291,Testability,test,test,31291,"egate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; return hl.bind(; lambda n: hl.if_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are define",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31662,Testability,test,test,31662,"_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess hetero",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32021,Testability,test,test,32021," row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32169,Testability,test,test,32169," row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32318,Testability,test,test,32318,".hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_weinberg_test(; counts.get(0, 0), counts.get(1, 0), counts.get(2, 0), one_sided=one_sided; ),; ). [docs]@typecheck(f=func_spec(1, agg_expr(expr_any)), array_agg_expr=expr_oneof(expr_array(), expr_set())); def explode(f, array_agg_expr) -> Expr",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32528,Testability,test,test,32528," - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_weinberg_test(; counts.get(0, 0), counts.get(1, 0), counts.get(2, 0), one_sided=one_sided; ),; ). [docs]@typecheck(f=func_spec(1, agg_expr(expr_any)), array_agg_expr=expr_oneof(expr_array(), expr_set())); def explode(f, array_agg_expr) -> Expression:; """"""Explode an array or set expression to aggregate the elements of all records. Examples; --------; Compute the mean of all elements in fields `C1`, `C2`, and `C3`:. >>> table1.aggregate(hl.agg.expl",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32645,Testability,test,test,32645,"4`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_weinberg_test(; counts.get(0, 0), counts.get(1, 0), counts.get(2, 0), one_sided=one_sided; ),; ). [docs]@typecheck(f=func_spec(1, agg_expr(expr_any)), array_agg_expr=expr_oneof(expr_array(), expr_set())); def explode(f, array_agg_expr) -> Expression:; """"""Explode an array or set expression to aggregate the elements of all records. Examples; --------; Compute the mean of all elements in fields `C1`, `C2`, and `C3`:. >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:52328,Testability,test,test,52328,"nes the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:52366,Testability,test,test,52366,"th the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:12464,Usability,intuit,intuition,12464,"s:; raise ExpressionException(; ""dynamic variables created by 'hl.bind' or lambda methods like 'hl.map' may not be aggregated""; ). [docs]@typecheck(expr=expr_numeric, k=int, _raw=bool); def approx_cdf(expr, k=100, *, _raw=False):; """"""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y  values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; raw_res = _agg_func(; 'ApproxCDF',; [hl.float64(expr)],; tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)),; init_op_args=[k],; ); conv = {; tint32: lambda x: x.map(hl.int),; tint64: lambda x: x.map(hl.int64),; tfloat32: lambda x: x.map(hl.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:23430,Availability,error,error,23430,", aggregations). @property; def dtype(self) -> HailType:; """"""The data type of the expression. Returns; -------; :class:`.HailType`. """"""; return self._type. def __bool__(self):; raise TypeError(; ""'Expression' objects cannot be converted to a 'bool'. Use 'hl.if_else' instead of Python if statements.""; ). def __len__(self):; raise TypeError(""'Expression' objects have no static length: use 'hl.len' for the length of collections""). def __contains__(self, item):; class_name = type(self).__name__; raise TypeError(f""`{class_name}` objects don't support the `in` operator.""). def __hash__(self):; return super(Expression, self).__hash__(). def __repr__(self):; return f'<{self.__class__.__name__} of type {self.dtype}>'. [docs] def __eq__(self, other):; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; return self._compare_op(""=="", other). [docs] def __ne__(self, other):; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; return self._compare_op(""!="", other). def _to_table(self, name):; name, ds = self._to_relational(name); if isinstance(ds, hail.MatrixTable):; entries = ds.key_cols_by().entries(); entries = entr",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:23992,Availability,error,error,23992,"perator.""). def __hash__(self):; return super(Expression, self).__hash__(). def __repr__(self):; return f'<{self.__class__.__name__} of type {self.dtype}>'. [docs] def __eq__(self, other):; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; return self._compare_op(""=="", other). [docs] def __ne__(self, other):; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; return self._compare_op(""!="", other). def _to_table(self, name):; name, ds = self._to_relational(name); if isinstance(ds, hail.MatrixTable):; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return name, entries.select(name); else:; if len(ds.key) != 0:; ds = ds.order_by(*ds.key); return name, ds.select(name). def _to_relational(self, fallback_name):; source = self._indices.source; axes = self._indices.axes; if not self._aggregations.empty():; raise NotImplementedError('cannot convert aggregated expression to table'). if source is None:; return fallback_name, hl.Table.parallelize([hl.struct(**{fallback_name: self})], n_partitions=1). name = source._fields_inverse.get(self); top_level = name is not None; if not top_le",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:35943,Deployability,update,updated,35943,"lf), hl.agg.count()),; self._summary_aggs(),; )). def _summarize(self, agg_res=None, *, name=None, header=None, top=False):; src = self._indices.source; summary_header = None; if src is None or len(self._indices.axes) == 0:; raise ValueError(""Cannot summarize a scalar expression""); if agg_res is None:; count, agg_res = self._aggregation_method()(hl.tuple((hl.agg.count(), self._all_summary_aggs()))); summary_header = f'{count} records.'; sum_fields, nested = self._summary_fields(agg_res, top); summary = Summary(self._type, agg_res[0], sum_fields, nested, header=summary_header); if name is None and header is None:; return summary; else:; return NamedSummary(summary, name, header). [docs] def summarize(self, handler=None):; """"""Compute and print summary information about the expression. .. include:: _templates/experimental.rst; """""". src = self._indices.source; if self in src._fields:; field_name = src._fields_inverse[self]; prefix = field_name; elif self._ir.is_nested_field:; prefix = self._ir.name; else:; prefix = '<expr>'. if handler is None:; handler = hl.utils.default_handler(); handler(self._summarize(name=prefix)). def _selector_and_agg_method(self):; src = self._indices.source; assert src is not None; assert len(self._indices.axes) > 0; if isinstance(src, hl.MatrixTable):; if self._indices == src._row_indices:; return src.select_rows, lambda t: t.aggregate_rows; elif self._indices == src._col_indices:; return src.select_cols, lambda t: t.aggregate_cols; else:; return src.select_entries, lambda t: t.aggregate_entries; else:; return src.select, lambda t: t.aggregate. def _aggregation_method(self):; return self._selector_and_agg_method()[1](self._indices.source). def _persist(self):; src = self._indices.source; if src is not None:; raise ValueError(""Can only persist a scalar (no Table/MatrixTable source)""); expr = Env.backend().persist_expression(self); assert expr.dtype == self.dtype; return expr.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:16254,Integrability,depend,dependencies,16254," ts):; return t0. return None. def unify_exprs(*exprs: 'Expression') -> Tuple:; assert len(exprs) > 0; types = {e.dtype for e in exprs}. # all types are the same; if len(types) == 1:; return (*exprs, True). for t in types:; c = expressions.coercer_from_dtype(t); if all(c.can_coerce(e.dtype) for e in exprs):; return (*tuple([c.coerce(e) for e in exprs]), True). # cannot coerce all types to the same type; return (*exprs, False). [docs]class Expression(object):; """"""Base class for Hail expressions."""""". __array_ufunc__ = None # disable NumPy coercions, so Hail coercions take priority. @typecheck_method(x=ir.IR, type=nullable(HailType), indices=Indices, aggregations=linked_list(Aggregation)); def __init__(; self, x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; self._ir: ir.IR = x; self._type = type; self._indices = indices; self._aggregations = aggregations; self._summary = None. [docs] def describe(self, handler=print):; """"""Print information about type, index, and dependencies.""""""; if self._aggregations:; agg_indices = set(); for a in self._aggregations:; agg_indices = agg_indices.union(a.indices.axes); agg_tag = ' (aggregated)'; agg_str = (; f'Includes aggregation with index {list(agg_indices)}\n'; f' (Aggregation index may be promoted based on context)'; ); else:; agg_tag = ''; agg_str = ''. bar = '--------------------------------------------------------'; s = (; '{bar}\n'; 'Type:\n'; ' {t}\n'; '{bar}\n'; 'Source:\n'; ' {src}\n'; 'Index:\n'; ' {inds}{agg_tag}{maybe_bar}{agg}\n'; '{bar}'.format(; bar=bar,; t=self.dtype.pretty(indent=4),; src=self._indices.source,; inds=list(self._indices.axes),; maybe_bar='\n' + bar + '\n' if agg_str else '',; agg_tag=agg_tag,; agg=agg_str,; ); ); handler(s). [docs] def __lt__(self, other):; return self._compare_op(""<"", other). [docs] def __le__(self, other):; return self._compare_op(""<="", other). [docs] def __gt__(self, other):; return self._compare_op("">"", other). [docs] d",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:8658,Testability,assert,assert,8658,"nstance(t, (tlocus, tinterval)):; return; if isinstance(t, tstruct):; for k, vt in t.items():; try:; raise_for_holes(vt); except ExpressionException as exc:; raise ExpressionException(f'cannot impute field {k}') from exc; return; if isinstance(t, ttuple):; for k, vt in enumerate(t):; try:; raise_for_holes(vt); except ExpressionException as exc:; raise ExpressionException(f'cannot impute {k}th element') from exc; return; if isinstance(t, (tarray, tset)):; try:; raise_for_holes(t.element_type); except ExpressionException as exc:; raise ExpressionException('cannot impute array elements') from exc; return; if isinstance(t, tdict):; try:; raise_for_holes(t.key_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict keys') from exc; try:; raise_for_holes(t.value_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got her",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:8996,Testability,assert,assert,8996,"'cannot impute {k}th element') from exc; return; if isinstance(t, (tarray, tset)):; try:; raise_for_holes(t.element_type); except ExpressionException as exc:; raise ExpressionException('cannot impute array elements') from exc; return; if isinstance(t, tdict):; try:; raise_for_holes(t.key_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict keys') from exc; try:; raise_for_holes(t.value_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got here; return e; elif isinstance(dtype, tstruct):; new_fields = []; found_expr = False; for f, t in dtype.items():; value = _to_expr(e[f], t); found_expr = found_expr or isinstance(value, Expression); new_fields.append(value). if not found_expr:; return e; else:; exprs = [; new_fields[i] if isinstance(new_fields[i], Expression) else hl.literal(new_fields[i], dtype[i]); for i i",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:9304,Testability,assert,assert,9304," raise_for_holes(t.key_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict keys') from exc; try:; raise_for_holes(t.value_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got here; return e; elif isinstance(dtype, tstruct):; new_fields = []; found_expr = False; for f, t in dtype.items():; value = _to_expr(e[f], t); found_expr = found_expr or isinstance(value, Expression); new_fields.append(value). if not found_expr:; return e; else:; exprs = [; new_fields[i] if isinstance(new_fields[i], Expression) else hl.literal(new_fields[i], dtype[i]); for i in range(len(new_fields)); ]; fields = {name: expr for name, expr in zip(dtype.keys(), exprs)}; from .typed_expressions import StructExpression. return StructExpression._from_fields(fields). elif isinstance(dtype, tarray):; elements = []; found_expr = False; for eleme",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:9518,Testability,assert,assert,9518," ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got here; return e; elif isinstance(dtype, tstruct):; new_fields = []; found_expr = False; for f, t in dtype.items():; value = _to_expr(e[f], t); found_expr = found_expr or isinstance(value, Expression); new_fields.append(value). if not found_expr:; return e; else:; exprs = [; new_fields[i] if isinstance(new_fields[i], Expression) else hl.literal(new_fields[i], dtype[i]); for i in range(len(new_fields)); ]; fields = {name: expr for name, expr in zip(dtype.keys(), exprs)}; from .typed_expressions import StructExpression. return StructExpression._from_fields(fields). elif isinstance(dtype, tarray):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:10496,Testability,assert,assert,10496,"e:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got here; return e; elif isinstance(dtype, tstruct):; new_fields = []; found_expr = False; for f, t in dtype.items():; value = _to_expr(e[f], t); found_expr = found_expr or isinstance(value, Expression); new_fields.append(value). if not found_expr:; return e; else:; exprs = [; new_fields[i] if isinstance(new_fields[i], Expression) else hl.literal(new_fields[i], dtype[i]); for i in range(len(new_fields)); ]; fields = {name: expr for name, expr in zip(dtype.keys(), exprs)}; from .typed_expressions import StructExpression. return StructExpression._from_fields(fields). elif isinstance(dtype, tarray):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.MakeArray([e._ir for e in exprs], None); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tset):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.ToSet(ir.toStream(ir.MakeArray([e._ir for e in exprs], None))); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, ttuple):; elements = []; found_expr = False; assert len(e) == len(dtype.types); for i in range(",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:11060,Testability,assert,assert,11060," ]; fields = {name: expr for name, expr in zip(dtype.keys(), exprs)}; from .typed_expressions import StructExpression. return StructExpression._from_fields(fields). elif isinstance(dtype, tarray):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.MakeArray([e._ir for e in exprs], None); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tset):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.ToSet(ir.toStream(ir.MakeArray([e._ir for e in exprs], None))); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, ttuple):; elements = []; found_expr = False; assert len(e) == len(dtype.types); for i in range(len(e)):; value = _to_expr(e[i], dtype.types[i]); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; exprs = [; elements[i] if isinstance(elements[i], Expression) else hl.literal(elements[i], dtype.types[i]); for i in range(len(elements)); ]; indices, aggregations = unify_all(*exprs); x = ir.MakeTuple([expr._ir for expr in exprs]); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tdict):; keys = []; values = []; found_expr = False; for k, v in e.item",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:11464,Testability,assert,assert,11464,"e, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.MakeArray([e._ir for e in exprs], None); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tset):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.ToSet(ir.toStream(ir.MakeArray([e._ir for e in exprs], None))); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, ttuple):; elements = []; found_expr = False; assert len(e) == len(dtype.types); for i in range(len(e)):; value = _to_expr(e[i], dtype.types[i]); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; exprs = [; elements[i] if isinstance(elements[i], Expression) else hl.literal(elements[i], dtype.types[i]); for i in range(len(elements)); ]; indices, aggregations = unify_all(*exprs); x = ir.MakeTuple([expr._ir for expr in exprs]); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tdict):; keys = []; values = []; found_expr = False; for k, v in e.items():; k_ = _to_expr(k, dtype.key_type); v_ = _to_expr(v, dtype.value_type); found_expr = found_expr or isinstance(k_, Expression); found_expr = found_expr or isinstance(v_, Expression); keys.append(k_); values.append(v_); if not found_expr:; return e; else:; assert len(keys) > 0; # Here I use `to_expr` to call `lit` the keys and values sep",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:12337,Testability,assert,assert,12337,"expr(x, dtype, indices, aggregations); elif isinstance(dtype, ttuple):; elements = []; found_expr = False; assert len(e) == len(dtype.types); for i in range(len(e)):; value = _to_expr(e[i], dtype.types[i]); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; exprs = [; elements[i] if isinstance(elements[i], Expression) else hl.literal(elements[i], dtype.types[i]); for i in range(len(elements)); ]; indices, aggregations = unify_all(*exprs); x = ir.MakeTuple([expr._ir for expr in exprs]); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tdict):; keys = []; values = []; found_expr = False; for k, v in e.items():; k_ = _to_expr(k, dtype.key_type); v_ = _to_expr(v, dtype.value_type); found_expr = found_expr or isinstance(k_, Expression); found_expr = found_expr or isinstance(v_, Expression); keys.append(k_); values.append(v_); if not found_expr:; return e; else:; assert len(keys) > 0; # Here I use `to_expr` to call `lit` the keys and values separately.; # I anticipate a common mode is statically-known keys and Expression; # values.; key_array = to_expr(keys, tarray(dtype.key_type)); value_array = to_expr(values, tarray(dtype.value_type)); return hl.dict(hl.zip(key_array, value_array)); elif isinstance(dtype, hl.tndarray):; return hl.nd.array(e); else:; raise NotImplementedError(dtype). def unify_all(*exprs) -> Tuple[Indices, LinkedList]:; if len(exprs) == 0:; return Indices(), LinkedList(Aggregation); try:; new_indices = Indices.unify(*[e._indices for e in exprs]); except ExpressionException:; # source mismatch; from collections import defaultdict. sources = defaultdict(lambda: []); for e in exprs:; from .expression_utils import get_refs. for name, inds in get_refs(e, *[e for a in e._aggregations for e in a.exprs]).items():; sources[inds.source].append(str(name)); raise ExpressionException(; ""Cannot combine expressions from different source objects.""; ""\n Found fie",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:13852,Testability,assert,assert,13852," = Indices.unify(*[e._indices for e in exprs]); except ExpressionException:; # source mismatch; from collections import defaultdict. sources = defaultdict(lambda: []); for e in exprs:; from .expression_utils import get_refs. for name, inds in get_refs(e, *[e for a in e._aggregations for e in a.exprs]).items():; sources[inds.source].append(str(name)); raise ExpressionException(; ""Cannot combine expressions from different source objects.""; ""\n Found fields from {n} objects:{fields}"".format(; n=len(sources), fields=''.join(""\n {}: {}"".format(src, fds) for src, fds in sources.items()); ); ) from None; first, rest = exprs[0], exprs[1:]; aggregations = first._aggregations; for e in rest:; aggregations = aggregations.push(*e._aggregations); return new_indices, aggregations. def unify_types_limited(*ts):; type_set = set(ts); if len(type_set) == 1:; # only one distinct class; return next(iter(type_set)); elif all(is_numeric(t) for t in ts):; # assert there are at least 2 numeric types; assert len(type_set) > 1; if tfloat64 in type_set:; return tfloat64; elif tfloat32 in type_set:; return tfloat32; elif tint64 in type_set:; return tint64; else:; assert type_set == {tint32, tbool}; return tint32; else:; return None. def unify_types(*ts):; limited_unify = unify_types_limited(*ts); if limited_unify is not None:; return limited_unify; elif all(isinstance(t, tarray) for t in ts):; et = unify_types_limited(*(t.element_type for t in ts)); if et is not None:; return tarray(et); else:; return None; else:; return None. def super_unify_types(*ts):; ts = [t for t in ts if t is not None]; if len(ts) == 0:; return None; t0 = ts[0]; if all(is_numeric(t) for t in ts):; return unify_types_limited(*ts); if any(not isinstance(t, type(t0)) for t in ts):; return None; if isinstance(t0, tarray):; et = super_unify_types(*[t.element_type for t in ts]); return tarray(et); if isinstance(t0, tset):; et = super_unify_types(*[t.element_type for t in ts]); return tset(et); if isinstance(t0, tdict):; kt = s",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:13895,Testability,assert,assert,13895," = Indices.unify(*[e._indices for e in exprs]); except ExpressionException:; # source mismatch; from collections import defaultdict. sources = defaultdict(lambda: []); for e in exprs:; from .expression_utils import get_refs. for name, inds in get_refs(e, *[e for a in e._aggregations for e in a.exprs]).items():; sources[inds.source].append(str(name)); raise ExpressionException(; ""Cannot combine expressions from different source objects.""; ""\n Found fields from {n} objects:{fields}"".format(; n=len(sources), fields=''.join(""\n {}: {}"".format(src, fds) for src, fds in sources.items()); ); ) from None; first, rest = exprs[0], exprs[1:]; aggregations = first._aggregations; for e in rest:; aggregations = aggregations.push(*e._aggregations); return new_indices, aggregations. def unify_types_limited(*ts):; type_set = set(ts); if len(type_set) == 1:; # only one distinct class; return next(iter(type_set)); elif all(is_numeric(t) for t in ts):; # assert there are at least 2 numeric types; assert len(type_set) > 1; if tfloat64 in type_set:; return tfloat64; elif tfloat32 in type_set:; return tfloat32; elif tint64 in type_set:; return tint64; else:; assert type_set == {tint32, tbool}; return tint32; else:; return None. def unify_types(*ts):; limited_unify = unify_types_limited(*ts); if limited_unify is not None:; return limited_unify; elif all(isinstance(t, tarray) for t in ts):; et = unify_types_limited(*(t.element_type for t in ts)); if et is not None:; return tarray(et); else:; return None; else:; return None. def super_unify_types(*ts):; ts = [t for t in ts if t is not None]; if len(ts) == 0:; return None; t0 = ts[0]; if all(is_numeric(t) for t in ts):; return unify_types_limited(*ts); if any(not isinstance(t, type(t0)) for t in ts):; return None; if isinstance(t0, tarray):; et = super_unify_types(*[t.element_type for t in ts]); return tarray(et); if isinstance(t0, tset):; et = super_unify_types(*[t.element_type for t in ts]); return tset(et); if isinstance(t0, tdict):; kt = s",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:14057,Testability,assert,assert,14057," = Indices.unify(*[e._indices for e in exprs]); except ExpressionException:; # source mismatch; from collections import defaultdict. sources = defaultdict(lambda: []); for e in exprs:; from .expression_utils import get_refs. for name, inds in get_refs(e, *[e for a in e._aggregations for e in a.exprs]).items():; sources[inds.source].append(str(name)); raise ExpressionException(; ""Cannot combine expressions from different source objects.""; ""\n Found fields from {n} objects:{fields}"".format(; n=len(sources), fields=''.join(""\n {}: {}"".format(src, fds) for src, fds in sources.items()); ); ) from None; first, rest = exprs[0], exprs[1:]; aggregations = first._aggregations; for e in rest:; aggregations = aggregations.push(*e._aggregations); return new_indices, aggregations. def unify_types_limited(*ts):; type_set = set(ts); if len(type_set) == 1:; # only one distinct class; return next(iter(type_set)); elif all(is_numeric(t) for t in ts):; # assert there are at least 2 numeric types; assert len(type_set) > 1; if tfloat64 in type_set:; return tfloat64; elif tfloat32 in type_set:; return tfloat32; elif tint64 in type_set:; return tint64; else:; assert type_set == {tint32, tbool}; return tint32; else:; return None. def unify_types(*ts):; limited_unify = unify_types_limited(*ts); if limited_unify is not None:; return limited_unify; elif all(isinstance(t, tarray) for t in ts):; et = unify_types_limited(*(t.element_type for t in ts)); if et is not None:; return tarray(et); else:; return None; else:; return None. def super_unify_types(*ts):; ts = [t for t in ts if t is not None]; if len(ts) == 0:; return None; t0 = ts[0]; if all(is_numeric(t) for t in ts):; return unify_types_limited(*ts); if any(not isinstance(t, type(t0)) for t in ts):; return None; if isinstance(t0, tarray):; et = super_unify_types(*[t.element_type for t in ts]); return tarray(et); if isinstance(t0, tset):; et = super_unify_types(*[t.element_type for t in ts]); return tset(et); if isinstance(t0, tdict):; kt = s",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:15296,Testability,assert,assert,15296,"):; et = unify_types_limited(*(t.element_type for t in ts)); if et is not None:; return tarray(et); else:; return None; else:; return None. def super_unify_types(*ts):; ts = [t for t in ts if t is not None]; if len(ts) == 0:; return None; t0 = ts[0]; if all(is_numeric(t) for t in ts):; return unify_types_limited(*ts); if any(not isinstance(t, type(t0)) for t in ts):; return None; if isinstance(t0, tarray):; et = super_unify_types(*[t.element_type for t in ts]); return tarray(et); if isinstance(t0, tset):; et = super_unify_types(*[t.element_type for t in ts]); return tset(et); if isinstance(t0, tdict):; kt = super_unify_types(*[t.key_type for t in ts]); vt = super_unify_types(*[t.value_type for t in ts]); return tdict(kt, vt); if isinstance(t0, tstruct):; keys = [k for t in ts for k in t.fields]; kvs = {k: super_unify_types(*[t.get(k, None) for t in ts]) for k in keys}; return tstruct(**kvs); if all(t0 == t for t in ts):; return t0. return None. def unify_exprs(*exprs: 'Expression') -> Tuple:; assert len(exprs) > 0; types = {e.dtype for e in exprs}. # all types are the same; if len(types) == 1:; return (*exprs, True). for t in types:; c = expressions.coercer_from_dtype(t); if all(c.can_coerce(e.dtype) for e in exprs):; return (*tuple([c.coerce(e) for e in exprs]), True). # cannot coerce all types to the same type; return (*exprs, False). [docs]class Expression(object):; """"""Base class for Hail expressions."""""". __array_ufunc__ = None # disable NumPy coercions, so Hail coercions take priority. @typecheck_method(x=ir.IR, type=nullable(HailType), indices=Indices, aggregations=linked_list(Aggregation)); def __init__(; self, x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; self._ir: ir.IR = x; self._type = type; self._indices = indices; self._aggregations = aggregations; self._summary = None. [docs] def describe(self, handler=print):; """"""Print information about type, index, and dependencies.""""""; if self._aggrega",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:18178,Testability,assert,assert,18178,"compare_op("">"", other). [docs] def __ge__(self, other):; return self._compare_op("">="", other). def __nonzero__(self):; raise ExpressionException(; ""The truth value of an expression is undefined\n""; "" Hint: instead of 'if x', use 'hl.if_else(x, ...)'\n""; "" Hint: instead of 'x and y' or 'x or y', use 'x & y' or 'x | y'\n""; "" Hint: instead of 'not x', use '~x'""; ). def __iter__(self):; raise ExpressionException(f""{self!r} object is not iterable""). def _compare_op(self, op, other):; other = to_expr(other); left, right, success = unify_exprs(self, other); if not success:; raise TypeError(; f""Invalid '{op}' comparison, cannot compare expressions "" f""of type '{self.dtype}' and '{other.dtype}'""; ); res = left._bin_op(op, right, hl.tbool); return res. def _is_scalar(self):; return self._indices.source is None. def _promote_scalar(self, typ):; if typ == tint32:; return hail.int32(self); elif typ == tint64:; return hail.int64(self); elif typ == tfloat32:; return hail.float32(self); else:; assert typ == tfloat64; return hail.float64(self). def _promote_numeric(self, typ):; coercer = expressions.coercer_from_dtype(typ); if isinstance(typ, tarray) and not isinstance(self.dtype, tarray):; return coercer.ec.coerce(self); elif isinstance(typ, tndarray) and not isinstance(self.dtype, tndarray):; return coercer.ec.coerce(self); else:; return coercer.coerce(self). @staticmethod; def _div_ret_type_f(t):; assert is_numeric(t); if t in {tint32, tint64}:; return tfloat64; else:; # Float64 or Float32; return t. def _bin_op_numeric_unify_types(self, name, other):; def numeric_proxy(t):; if t == tbool:; return tint32; else:; return t. def scalar_type(t):; if isinstance(t, tarray):; return numeric_proxy(t.element_type); elif isinstance(t, tndarray):; return numeric_proxy(t.element_type); else:; return numeric_proxy(t). t = unify_types(scalar_type(self.dtype), scalar_type(other.dtype)); if t is None:; raise NotImplementedError(""'{}' {} '{}'"".format(self.dtype, name, other.dtype)). if isinstance",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:18592,Testability,assert,assert,18592,"terable""). def _compare_op(self, op, other):; other = to_expr(other); left, right, success = unify_exprs(self, other); if not success:; raise TypeError(; f""Invalid '{op}' comparison, cannot compare expressions "" f""of type '{self.dtype}' and '{other.dtype}'""; ); res = left._bin_op(op, right, hl.tbool); return res. def _is_scalar(self):; return self._indices.source is None. def _promote_scalar(self, typ):; if typ == tint32:; return hail.int32(self); elif typ == tint64:; return hail.int64(self); elif typ == tfloat32:; return hail.float32(self); else:; assert typ == tfloat64; return hail.float64(self). def _promote_numeric(self, typ):; coercer = expressions.coercer_from_dtype(typ); if isinstance(typ, tarray) and not isinstance(self.dtype, tarray):; return coercer.ec.coerce(self); elif isinstance(typ, tndarray) and not isinstance(self.dtype, tndarray):; return coercer.ec.coerce(self); else:; return coercer.coerce(self). @staticmethod; def _div_ret_type_f(t):; assert is_numeric(t); if t in {tint32, tint64}:; return tfloat64; else:; # Float64 or Float32; return t. def _bin_op_numeric_unify_types(self, name, other):; def numeric_proxy(t):; if t == tbool:; return tint32; else:; return t. def scalar_type(t):; if isinstance(t, tarray):; return numeric_proxy(t.element_type); elif isinstance(t, tndarray):; return numeric_proxy(t.element_type); else:; return numeric_proxy(t). t = unify_types(scalar_type(self.dtype), scalar_type(other.dtype)); if t is None:; raise NotImplementedError(""'{}' {} '{}'"".format(self.dtype, name, other.dtype)). if isinstance(self.dtype, tarray) or isinstance(other.dtype, tarray):; return tarray(t); elif isinstance(self.dtype, tndarray):; return tndarray(t, self.ndim); elif isinstance(other.dtype, tndarray):; return tndarray(t, other.ndim). return t. def _bin_op_numeric(self, name, other, ret_type_f=None):; other = to_expr(other); unified_type = self._bin_op_numeric_unify_types(name, other); me = self._promote_numeric(unified_type); other = other._promote_",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:25718,Testability,assert,assert,25718,"():; raise NotImplementedError('cannot convert aggregated expression to table'). if source is None:; return fallback_name, hl.Table.parallelize([hl.struct(**{fallback_name: self})], n_partitions=1). name = source._fields_inverse.get(self); top_level = name is not None; if not top_level:; name = fallback_name; named_self = {name: self}; if len(axes) == 0:; x = source.select_globals(**named_self); ds = hl.Table.parallelize([x.index_globals()], n_partitions=1); elif isinstance(source, hail.Table):; if top_level and name in source.key:; named_self = {}; ds = source.select(**named_self).select_globals(); elif isinstance(source, hail.MatrixTable):; if self._indices == source._row_indices:; if top_level and name in source.row_key:; named_self = {}; ds = source.select_rows(**named_self).select_globals().rows(); elif self._indices == source._col_indices:; if top_level and name in source.col_key:; named_self = {}; ds = source.select_cols(**named_self).select_globals().key_cols_by().cols(); else:; assert self._indices == source._entry_indices; ds = source.select_entries(**named_self).select_globals().select_cols().select_rows(); return name, ds. [docs] @typecheck_method(; n=nullable(int),; width=nullable(int),; truncate=nullable(int),; types=bool,; handler=nullable(anyfunc),; n_rows=nullable(int),; n_cols=nullable(int),; ); def show(self, n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None):; """"""Print the first few records of the expression to the console. If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records. Examples; --------. >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; -----; The output can be passed piped to a",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:26803,Testability,log,logging,26803,"amed_self).select_globals().select_cols().select_rows(); return name, ds. [docs] @typecheck_method(; n=nullable(int),; width=nullable(int),; truncate=nullable(int),; types=bool,; handler=nullable(anyfunc),; n_rows=nullable(int),; n_cols=nullable(int),; ); def show(self, n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None):; """"""Print the first few records of the expression to the console. If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records. Examples; --------. >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; -----; The output can be passed piped to another output source using the `handler` argument:. >>> ht.foo.show(handler=lambda x: logging.info(x)) # doctest: +SKIP. Parameters; ----------; n : :obj:`int`; Maximum number of rows to show.; width : :obj:`int`; Horizontal width at which to break columns.; truncate : :obj:`int`, optional; Truncate each field to the given number of characters. If; ``None``, truncate fields to the given `width`.; types : :obj:`bool`; Print an extra header line with the type of each field.; """"""; kwargs = {; 'n': n,; 'width': width,; 'truncate': truncate,; 'types': types,; 'handler': handler,; 'n_rows': n_rows,; 'n_cols': n_cols,; }; if kwargs.get('n_rows') is None:; kwargs['n_rows'] = kwargs['n']; del kwargs['n']; _, ds = self._to_relational_preserving_rows_and_cols('<expr>'); return ds.show(**{k: v for k, v in kwargs.items() if v is not None}). def _to_relational_preserving_rows_and_cols(self, fallback_name):; source = self._indices.source; if isinstance(source, hl.Table):; if self is source.row:; return None, source; if self is source.key:; return None, source.select(); if isins",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:30910,Testability,assert,assert,30910,"mn header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:. >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus	alleles	{""s"":0,""family"":""fam1""}	{""s"":1,""family"":""fam1""}	{""s"":2,""family"":""fam1""}	{""s"":3,""family"":""fam1""}; 1:1	[""A"",""C""]	0/1	0/0	0/1	0/0; 1:2	[""A"",""C""]	1/1	0/1	0/1	0/1; 1:3	[""A"",""C""]	0/0	0/1	0/0	0/0; 1:4	[""A"",""C""]	0/1	1/1	0/1	0/1. Parameters; ----------; path : :class:`str`; The path to which to export.; delimiter : :class:`str`; The string for delimiting columns.; missing : :class:`str`; The string to output for missing values.; header : :obj:`bool`; When ``True`` include a header line.; """"""; uid = Env.get_uid(); self_name, ds = self._to_relational_preserving_rows_and_cols(uid); if isinstance(ds, hl.Table):; ds.export(output=path, delimiter=delimiter, header=header); else:; assert len(self._indices.axes) == 2; entries, cols = Env.get_uid(), Env.get_uid(); t = ds.select_cols().localize_entries(entries, cols); t = t.order_by(*t.key); output_col_name = Env.get_uid(); entry_array = t[entries]; if self_name:; entry_array = hl.map(lambda x: x[self_name], entry_array); entry_array = hl.map(lambda x: hl.if_else(hl.is_missing(x), missing, hl.str(x)), entry_array); file_contents = t.select(; **{k: hl.str(t[k]) for k in ds.row_key}, **{output_col_name: hl.delimit(entry_array, delimiter)}; ); if header:; col_key = t[cols]; if len(ds.col_key) == 1:; col_key = hl.map(lambda x: x[0], col_key); column_names = hl.map(hl.str, col_key).collect(_localize=False)[0]; header_table = (; hl.utils.range_table(1); .key_by(); .select(**{k: k for k in ds.row_key}, **{output_col_name: hl.delimit(column_names, delimiter)}); ); file_contents = header_table.union(file_contents); file_contents.export(path, delimiter=delimiter, header=False). [docs] @typecheck_metho",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:35171,Testability,assert,assert,35171,"lf), hl.agg.count()),; self._summary_aggs(),; )). def _summarize(self, agg_res=None, *, name=None, header=None, top=False):; src = self._indices.source; summary_header = None; if src is None or len(self._indices.axes) == 0:; raise ValueError(""Cannot summarize a scalar expression""); if agg_res is None:; count, agg_res = self._aggregation_method()(hl.tuple((hl.agg.count(), self._all_summary_aggs()))); summary_header = f'{count} records.'; sum_fields, nested = self._summary_fields(agg_res, top); summary = Summary(self._type, agg_res[0], sum_fields, nested, header=summary_header); if name is None and header is None:; return summary; else:; return NamedSummary(summary, name, header). [docs] def summarize(self, handler=None):; """"""Compute and print summary information about the expression. .. include:: _templates/experimental.rst; """""". src = self._indices.source; if self in src._fields:; field_name = src._fields_inverse[self]; prefix = field_name; elif self._ir.is_nested_field:; prefix = self._ir.name; else:; prefix = '<expr>'. if handler is None:; handler = hl.utils.default_handler(); handler(self._summarize(name=prefix)). def _selector_and_agg_method(self):; src = self._indices.source; assert src is not None; assert len(self._indices.axes) > 0; if isinstance(src, hl.MatrixTable):; if self._indices == src._row_indices:; return src.select_rows, lambda t: t.aggregate_rows; elif self._indices == src._col_indices:; return src.select_cols, lambda t: t.aggregate_cols; else:; return src.select_entries, lambda t: t.aggregate_entries; else:; return src.select, lambda t: t.aggregate. def _aggregation_method(self):; return self._selector_and_agg_method()[1](self._indices.source). def _persist(self):; src = self._indices.source; if src is not None:; raise ValueError(""Can only persist a scalar (no Table/MatrixTable source)""); expr = Env.backend().persist_expression(self); assert expr.dtype == self.dtype; return expr.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:35195,Testability,assert,assert,35195,"lf), hl.agg.count()),; self._summary_aggs(),; )). def _summarize(self, agg_res=None, *, name=None, header=None, top=False):; src = self._indices.source; summary_header = None; if src is None or len(self._indices.axes) == 0:; raise ValueError(""Cannot summarize a scalar expression""); if agg_res is None:; count, agg_res = self._aggregation_method()(hl.tuple((hl.agg.count(), self._all_summary_aggs()))); summary_header = f'{count} records.'; sum_fields, nested = self._summary_fields(agg_res, top); summary = Summary(self._type, agg_res[0], sum_fields, nested, header=summary_header); if name is None and header is None:; return summary; else:; return NamedSummary(summary, name, header). [docs] def summarize(self, handler=None):; """"""Compute and print summary information about the expression. .. include:: _templates/experimental.rst; """""". src = self._indices.source; if self in src._fields:; field_name = src._fields_inverse[self]; prefix = field_name; elif self._ir.is_nested_field:; prefix = self._ir.name; else:; prefix = '<expr>'. if handler is None:; handler = hl.utils.default_handler(); handler(self._summarize(name=prefix)). def _selector_and_agg_method(self):; src = self._indices.source; assert src is not None; assert len(self._indices.axes) > 0; if isinstance(src, hl.MatrixTable):; if self._indices == src._row_indices:; return src.select_rows, lambda t: t.aggregate_rows; elif self._indices == src._col_indices:; return src.select_cols, lambda t: t.aggregate_cols; else:; return src.select_entries, lambda t: t.aggregate_entries; else:; return src.select, lambda t: t.aggregate. def _aggregation_method(self):; return self._selector_and_agg_method()[1](self._indices.source). def _persist(self):; src = self._indices.source; if src is not None:; raise ValueError(""Can only persist a scalar (no Table/MatrixTable source)""); expr = Env.backend().persist_expression(self); assert expr.dtype == self.dtype; return expr.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:35857,Testability,assert,assert,35857,"lf), hl.agg.count()),; self._summary_aggs(),; )). def _summarize(self, agg_res=None, *, name=None, header=None, top=False):; src = self._indices.source; summary_header = None; if src is None or len(self._indices.axes) == 0:; raise ValueError(""Cannot summarize a scalar expression""); if agg_res is None:; count, agg_res = self._aggregation_method()(hl.tuple((hl.agg.count(), self._all_summary_aggs()))); summary_header = f'{count} records.'; sum_fields, nested = self._summary_fields(agg_res, top); summary = Summary(self._type, agg_res[0], sum_fields, nested, header=summary_header); if name is None and header is None:; return summary; else:; return NamedSummary(summary, name, header). [docs] def summarize(self, handler=None):; """"""Compute and print summary information about the expression. .. include:: _templates/experimental.rst; """""". src = self._indices.source; if self in src._fields:; field_name = src._fields_inverse[self]; prefix = field_name; elif self._ir.is_nested_field:; prefix = self._ir.name; else:; prefix = '<expr>'. if handler is None:; handler = hl.utils.default_handler(); handler(self._summarize(name=prefix)). def _selector_and_agg_method(self):; src = self._indices.source; assert src is not None; assert len(self._indices.axes) > 0; if isinstance(src, hl.MatrixTable):; if self._indices == src._row_indices:; return src.select_rows, lambda t: t.aggregate_rows; elif self._indices == src._col_indices:; return src.select_cols, lambda t: t.aggregate_cols; else:; return src.select_entries, lambda t: t.aggregate_entries; else:; return src.select, lambda t: t.aggregate. def _aggregation_method(self):; return self._selector_and_agg_method()[1](self._indices.source). def _persist(self):; src = self._indices.source; if src is not None:; raise ValueError(""Can only persist a scalar (no Table/MatrixTable source)""); expr = Env.backend().persist_expression(self); assert expr.dtype == self.dtype; return expr.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1013,Availability,error,error,1013,"il.expr.expressions.expression_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpec",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1150,Availability,error,errors,1150,"ocs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'str",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1424,Availability,error,errors,1424,". menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expecte",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1647,Availability,error,error,1647,"from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expressi",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:2487,Availability,error,errors,2487,"urce mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expression {strictness}indexed by {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}{agg}"".format(; caller=caller,; strictness=strictness,; expected=list(expected_axes),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:3774,Availability,error,errors,3774,"),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4206,Availability,error,errors,4206,"n_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4346,Availability,error,errors,4346,"ions:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(M",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4364,Availability,error,errors,4364,"ions:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(M",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4373,Availability,error,error,4373,"ions:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(M",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4406,Availability,error,errors,4406,"refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(MakeTuple([ir]), timed=True)[0]. [docs]@typech",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:9670,Deployability,update,updated,9670,"le_source(caller, expr):; from hail import MatrixTable. source = expr._indices.source; if not isinstance(source, MatrixTable):; raise ValueError(; ""{}: Expect an expression of 'MatrixTable', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def table_source(caller, expr):; from hail import Table. source = expr._indices.source; if not isinstance(source, Table):; raise ValueError(; ""{}: Expect an expression of 'Table', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def raise_unless_entry_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be entry-indexed"" f"", found no indices (no source)""); if expr._indices != expr._indices.source._entry_indices:; raise ExpressionException(; f""{caller}: expression must be entry-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_row_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be row-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._row_indices:; raise ExpressionException(; f""{caller}: expression must be row-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_column_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be column-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._col_indices:; raise ExpressionException(; f""{caller}: expression must be column-indexed"" f"", found indices ({list(expr._indices.axes)}).""; ).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:2461,Testability,assert,assert,2461,"urce mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expression {strictness}indexed by {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}{agg}"".format(; caller=caller,; strictness=strictness,; expected=list(expected_axes),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:3367,Testability,assert,assert,3367,"name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expression {strictness}indexed by {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}{agg}"".format(; caller=caller,; strictness=strictness,; expected=list(expected_axes),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:3748,Testability,assert,assert,3748,"ist(expected_axes),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (A",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:5543,Usability,learn,learning,5543," stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(MakeTuple([ir]), timed=True)[0]. [docs]@typecheck(expression=expr_any); def eval(expression):; """"""Evaluate a Hail expression, returning the result. This method is extremely useful for learning about Hail expressions and; understanding how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.Table` or :class:`.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; Any; """"""; return eval_timed(expression)[0]. @typecheck(expression=expr_any); def eval_typed(expression):; """"""Evaluate a Hail expression, returning the result and the type of the result. This method is extremely useful for learning about Hail expressions and understanding; how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.hail.Table` or :class:`.hail.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval_typed(hl.if_else(x % 2 == 0, 'Even', 'Odd')); ('Even', dtype('str",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:6231,Usability,learn,learning,6231,"ion._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(MakeTuple([ir]), timed=True)[0]. [docs]@typecheck(expression=expr_any); def eval(expression):; """"""Evaluate a Hail expression, returning the result. This method is extremely useful for learning about Hail expressions and; understanding how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.Table` or :class:`.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; Any; """"""; return eval_timed(expression)[0]. @typecheck(expression=expr_any); def eval_typed(expression):; """"""Evaluate a Hail expression, returning the result and the type of the result. This method is extremely useful for learning about Hail expressions and understanding; how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.hail.Table` or :class:`.hail.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval_typed(hl.if_else(x % 2 == 0, 'Even', 'Odd')); ('Even', dtype('str')). Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (any, :class:`.HailType`); Result of evaluating `expression`, and its type. """"""; return eval(expression), expression.dtype. def _get_refs(expr: Expression, builder: Dict[str, Indices]) -> None:; from hail.ir import GetField, TopLevelReference. for ir in expr._ir.search(; lambda a: (isinstance(a, GetField) and not a.name.startswith('__uid') and isinstance(a.o, TopLevelReference)); ):; src = expr._indices.source; builder[ir.name] = src._indices_from_ref[ir.o.name]. def extract_refs_by_indices(exprs, indices):; """"""Re",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:36943,Availability,error,error,36943,"== {1, 2, 3}; True; >>> hl.eval(hl.flatten(a.b.inner)) == {1, 2, 3}; True. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """""". return self.map(lambda x: x[item]). [docs]class DictExpression(Expression):; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(DictExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tdict); self._kc = coercer_from_dtype(type.key_type); self._vc = coercer_from_dtype(type.value_type). [docs] @typecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""; if not self._kc.can_coerce(item.dtype):; raise Ty",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:37056,Availability,error,error,37056," ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """""". return self.map(lambda x: x[item]). [docs]class DictExpression(Expression):; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(DictExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tdict); self._kc = coercer_from_dtype(type.key_type); self._vc = coercer_from_dtype(type.value_type). [docs] @typecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""'DictExpression.contains' encountered an invalid key type\n""; "" dict key typ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:44553,Availability,error,error,44553,"kedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.dtype.items()):; if isinstance(self._ir, ir.MakeStruct):; expr = construct_expr(self._ir.fields[i][1], t, self._indices, self._aggregations); elif isinstance(self._ir, ir.SelectedTopLevelReference):; expr = construct_expr(; ir.ProjectedTopLevelReference(self._ir.ref.name, f, t), t, self._indices, self._aggregations; ); elif isinstance(self._ir, ir.SelectFields):; expr = construct_expr(ir.GetField(self._ir.old, f), t, self._indices, self._aggregations); else:; expr = construct_expr(ir.GetField(self._ir, f), t, self._indices, self._aggregations); self._set_field(f, expr). def _set_field(self, key, value):; if key not in self._fields:; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; if key in self.__dict__ or hasattr(super(), key):; self._warn_on_shadowed_name.add(key); else:; self.__dict__[key] = value; self._fields[key] = value. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13923,Deployability,pipeline,pipeline,13923,"rn construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(na",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:47645,Deployability,update,updated,47645,"sertions_dict.get(field); if e is None:; e = self._fields[field]; return e.dtype. new_type = hl.tstruct(**{f: get_type(f) for f in field_order}); indices, aggregations = unify_all(self, *insertions_dict.values()); return construct_expr(; ir.InsertFields.construct_with_deduplication(; self._ir, [(field, expr._ir) for field, expr in insertions_dict.items()], field_order; ),; new_type,; indices,; aggregations,; ). [docs] @typecheck_method(named_exprs=expr_any); def annotate(self, **named_exprs):; """"""Add new fields or recompute existing fields. Examples; --------. >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; -----; If an expression in `named_exprs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Fields to add. Returns; -------; :class:`.StructExpression`; Struct with new or updated fields.; """"""; new_types = {n: t for (n, t) in self.dtype.items()}. for f, e in named_exprs.items():; new_types[f] = e.dtype. result_type = tstruct(**new_types); indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]). return construct_expr(; ir.InsertFields.construct_with_deduplication(; self._ir, list(map(lambda x: (x[0], x[1]._ir), named_exprs.items())), None; ),; result_type,; indices,; aggregations,; ). [docs] @typecheck_method(fields=str, named_exprs=expr_any); def select(self, *fields, **named_exprs):; """"""Select existing fields and compute new ones. Examples; --------. >>> hl.eval(struct.select('a', c=['bar', 'baz'])); Struct(a=5, c=['bar', 'baz']). Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `named_exprs` arguments are new field expressions. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.;",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:118675,Deployability,update,updated,118675,"ices, aggregations=LinkedList); def construct_expr(; x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; if type is None:; return Expression(x, None, indices, aggregations); x.assign_type(type); if isinstance(type, tarray) and is_numeric(type.element_type):; return ArrayNumericExpression(x, type, indices, aggregations); elif isinstance(type, tarray):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return ArrayStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tset):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return SetStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tndarray) and is_numeric(type.element_type):; return NDArrayNumericExpression(x, type, indices, aggregations); elif type in scalars:; return scalars[type](x, type, indices, aggregations); elif type.__class__ in typ_to_expr:; return typ_to_expr[type.__class__](x, type, indices, aggregations); else:; raise NotImplementedError(type). @typecheck(name=str, type=HailType, indices=Indices); def construct_reference(name, type, indices):; assert isinstance(type, hl.tstruct); x = ir.SelectedTopLevelReference(name, type); return construct_expr(x, type, indices). @typecheck(name=str, type=HailType, indices=Indices, aggregations=LinkedList); def construct_variable(name, type, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation)):; return construct_expr(ir.Ref(name, type), type, indices, aggregations).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13998,Energy Efficiency,efficient,efficient,13998,"rn construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(na",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:25273,Energy Efficiency,power,power,25273,"(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by an array or a scalar using floor division. Examples; --------. >>> hl.eval(a1 // 2); [0, 0, 1, 1, 2, 2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Positionally compute the left modulo the right. Examples; --------. >>> hl.eval(a1 % 2); [0, 1, 0, 1, 0, 1]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, other):; """"""Positionally raise to the power of an array or a scalar. Examples; --------. >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('**', other, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class SetExpression(CollectionExpression):; """"""Expression of type :class:`.tset`. >>> s1 = hl.literal({1, 2, 3}); >>> s2 = hl.literal({1, 3, 5}). See Also; --------; :class:`.CollectionExpression`; """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(SetExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tset); self._ec = ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59199,Energy Efficiency,power,power,59199,"turn self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Divide two numbers with floor division. Examples; --------. >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; The floor of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(ot",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59252,Energy Efficiency,power,power,59252,"turn self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Divide two numbers with floor division. Examples; --------. >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; The floor of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(ot",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59413,Energy Efficiency,power,power,59413," >>> hl.eval(y // 2); 2.0. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; The floor of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59585,Energy Efficiency,power,power,59585,"of the left number divided by the right.; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The ``&`` and ``|`` operators have higher priority than comparison; operators like ``==``, ``<``, or ``>``. Parentheses are often; necessar",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:59632,Energy Efficiency,power,power,59632,"eric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). [docs] def __mod__(self, other):; """"""Compute the left modulo the right number. Examples; --------. >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters; ----------; other : :class:`.NumericExpression`; Dividend. Returns; -------; :class:`.NumericExpression`; Remainder after dividing the left by the right.; """"""; return self._bin_op_numeric('%', other). def __rmod__(self, other):; return self._bin_op_numeric_reverse('%', other). [docs] def __pow__(self, power, modulo=None):; """"""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""; return self._bin_op_numeric('**', power, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class BooleanExpression(NumericExpression):; """"""Expression of type :py:data:`.tbool`. >>> t = hl.literal(True); >>> f = hl.literal(False); >>> na = hl.missing(hl.tbool). >>> hl.eval(t); True. >>> hl.eval(f); False. >>> hl.eval(na); None. """""". @typecheck_method(other=expr_bool); def __rand__(self, other):; return self.__and__(other). @typecheck_method(other=expr_bool); def __ror__(self, other):; return self.__or__(other). [docs] @typecheck_method(other=expr_bool); def __and__(self, other):; """"""Return ``True`` if the left and right arguments are ``True``. Examples; --------. >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The ``&`` and ``|`` operators have higher priority than comparison; operators like ``==``, ``<``, or ``>``. Parentheses are often; necessary:. >>> x = hl.literal(5). >>> hl.eval((x < 10) & (x > 2)); True. Para",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:44559,Integrability,message,message,44559,"kedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.dtype.items()):; if isinstance(self._ir, ir.MakeStruct):; expr = construct_expr(self._ir.fields[i][1], t, self._indices, self._aggregations); elif isinstance(self._ir, ir.SelectedTopLevelReference):; expr = construct_expr(; ir.ProjectedTopLevelReference(self._ir.ref.name, f, t), t, self._indices, self._aggregations; ); elif isinstance(self._ir, ir.SelectFields):; expr = construct_expr(ir.GetField(self._ir.old, f), t, self._indices, self._aggregations); else:; expr = construct_expr(ir.GetField(self._ir, f), t, self._indices, self._aggregations); self._set_field(f, expr). def _set_field(self, key, value):; if key not in self._fields:; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; if key in self.__dict__ or hasattr(super(), key):; self._warn_on_shadowed_name.add(key); else:; self.__dict__[key] = value; self._fields[key] = value. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17202,Modifiability,extend,extend,17202,"eturn x(elt). else:. def f(elt, x):; return elt == x. return hl.bind(lambda a: hl.range(0, a.length()).filter(lambda i: f(a[i], x)).first(), self). [docs] @typecheck_method(item=expr_any); def append(self, item):; """"""Append an element to the array and return the result. Examples; --------. >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; ----; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the c",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17307,Modifiability,extend,extend,17307,".first(), self). [docs] @typecheck_method(item=expr_any); def append(self, item):; """"""Append an element to the array and return the result. Examples; --------. >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; ----; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argu",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17598,Modifiability,extend,extend,17598,"; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`. Returns; -------; :class:`.ArrayExpression`.; """"""; return self._to_stream().scan(lambda x, y: f(x, y), zero).to_array(). [docs] @typecheck_method(group_size=expr_int32); def grouped(self, group_size):; """"""Partition an array into fixed size subarrays. Examples; --------; >>> a = hl.array([0, 1,",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17754,Modifiability,extend,extend,17754,"nt to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`. Returns; -------; :class:`.ArrayExpression`.; """"""; return self._to_stream().scan(lambda x, y: f(x, y), zero).to_array(). [docs] @typecheck_method(group_size=expr_int32); def grouped(self, group_size):; """"""Partition an array into fixed size subarrays. Examples; --------; >>> a = hl.array([0, 1, 2, 3, 4]). >>> hl.eval(a.grouped(2)); [[0, 1], [2, 3], [4]]. Parameters; ----------; group_size : :class:`",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89118,Performance,load,load,89118," hl.eval(locus.in_autosome_or_par()); True. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isAutosomalOrPseudoAutosomal"", tbool). [docs] def in_mito(self):; """"""Returns ``True`` if the locus is on mitochondrial DNA. Examples; --------. >>> hl.eval(locus.in_mito()); False. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isMitochondrial"", tbool). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def sequence_context(self, before=0, after=0):; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89667,Performance,load,loaded,89667,"re=0, after=0):; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; -----; The returned interval is inclusive of both the `start` and `end`; endpoints. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include before the locus. Truncates at 1.; after : :class:`.Expressio",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89697,Performance,load,load,89697,"mples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; -----; The returned interval is inclusive of both the `start` and `end`; endpoints. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include before the locus. Truncates at 1.; after : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include after the locus. ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:107559,Performance,perform,performed,107559,"xpression`; Value or ndarray to divide by. Returns; -------; :class:`.NDArrayNumericExpression`; NDArray of positional quotients.; """"""; return self._bin_op_numeric(""/"", other, self._div_ret_type_f). def __rtruediv__(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by a ndarray or a scalar using floor division. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). def __rmatmul__(self, other):; if not isinstance(other, NDArrayNumericExpression):; other = hl.nd.array(other); return other.__matmul__(self). [docs] def __matmul__(self, other):; """"""Matrix multiplication: `a @ b`, semantically equivalent to `NumPy` matmul. If `a` and `b` are vectors,; the vector dot product is performed, returning a `NumericExpression`. If `a` and `b` are both 2-dimensional; matrices, this performs normal matrix multiplication. If `a` and `b` have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if `a` has shape `(3, 4, 5)` and `b` has shape `(3, 5, 6)`, `a` is treated; as a stack of three matrices of shape `(4, 5)` and `b` as a stack of three matrices of shape `(5, 6)`. `a @ b`; would then have shape `(3, 4, 6)`. Notes; -----; The last dimension of `a` and the second to last dimension of `b` (or only dimension if `b` is a vector); must have the same length. The dimensions to the left of the last two dimensions of `a` and `b` (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters; ----------; other : :class:`numpy.ndarray` :class:`.NDArrayNu",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:107657,Performance,perform,performs,107657," NDArray of positional quotients.; """"""; return self._bin_op_numeric(""/"", other, self._div_ret_type_f). def __rtruediv__(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by a ndarray or a scalar using floor division. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). def __rmatmul__(self, other):; if not isinstance(other, NDArrayNumericExpression):; other = hl.nd.array(other); return other.__matmul__(self). [docs] def __matmul__(self, other):; """"""Matrix multiplication: `a @ b`, semantically equivalent to `NumPy` matmul. If `a` and `b` are vectors,; the vector dot product is performed, returning a `NumericExpression`. If `a` and `b` are both 2-dimensional; matrices, this performs normal matrix multiplication. If `a` and `b` have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if `a` has shape `(3, 4, 5)` and `b` has shape `(3, 5, 6)`, `a` is treated; as a stack of three matrices of shape `(4, 5)` and `b` as a stack of three matrices of shape `(5, 6)`. `a @ b`; would then have shape `(3, 4, 6)`. Notes; -----; The last dimension of `a` and the second to last dimension of `b` (or only dimension if `b` is a vector); must have the same length. The dimensions to the left of the last two dimensions of `a` and `b` (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters; ----------; other : :class:`numpy.ndarray` :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression` or :class:`.NumericExpres",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42629,Safety,safe,safer,42629,"top):; k = construct_variable(Env.get_uid(), self.dtype.key_type, indices=self._indices); v = construct_variable(Env.get_uid(), self.dtype.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13231,Security,access,accessing,13231,"se TypeError(; ""array expects key to be type 'slice' or expression of type 'int32', ""; ""found expression of type '{}'"".format(item._type); ); else:; return self._method(""indexArray"", self.dtype.element_type, item). @typecheck_method(start=nullable(expr_int32), stop=nullable(expr_int32), step=nullable(expr_int32)); def _slice(self, start=None, stop=None, step=None):; indices, aggregations = unify_all(self, *(x for x in (start, stop, step) if x is not None)); if step is None:; step = hl.int(1); if start is None:; start = hl.if_else(step >= 0, 0, -1); if stop is not None:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop._ir, step._ir); else:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop, step._ir). return construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @dep",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42371,Security,access,accessible,42371,"---; :class:`.ArrayExpression`; All key/value pairs in the dictionary.; """"""; return hl.array(self). def _extra_summary_fields(self, agg_result):; return {; 'Min Size': agg_result[0],; 'Max Size': agg_result[1],; 'Mean Size': agg_result[2],; }. def _nested_summary(self, agg_result, top):; k = construct_variable(Env.get_uid(), self.dtype.key_type, indices=self._indices); v = construct_variable(Env.get_uid(), self.dtype.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42435,Security,access,access,42435,"ary.; """"""; return hl.array(self). def _extra_summary_fields(self, agg_result):; return {; 'Min Size': agg_result[0],; 'Max Size': agg_result[1],; 'Mean Size': agg_result[2],; }. def _nested_summary(self, agg_result, top):; k = construct_variable(Env.get_uid(), self.dtype.key_type, indices=self._indices); v = construct_variable(Env.get_uid(), self.dtype.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @t",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42790,Security,access,accessible,42790,"e.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.d",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42888,Security,access,access,42888,"agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.dtype.items()):; if isinstance(self._ir, ir.MakeStruct):; expr = construct_expr(self._ir.fields[i][1], t, self._indices,",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:45057,Security,access,access,45057,"(self._ir.ref.name, f, t), t, self._indices, self._aggregations; ); elif isinstance(self._ir, ir.SelectFields):; expr = construct_expr(ir.GetField(self._ir.old, f), t, self._indices, self._aggregations); else:; expr = construct_expr(ir.GetField(self._ir, f), t, self._indices, self._aggregations); self._set_field(f, expr). def _set_field(self, key, value):; if key not in self._fields:; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; if key in self.__dict__ or hasattr(super(), key):; self._warn_on_shadowed_name.add(key); else:; self.__dict__[key] = value; self._fields[key] = value. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.Expression`; Struct field.; """"""; if isinstance(item, str):; return self._get_field(item); if isinstance(item, int):; return self._get_field(self.dtype.fields[item]); else:; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return self.select(*self.dtype.fields[item.start : item.stop : item.ste",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:4009,Testability,assert,assert,4009,"octest: +SKIP_OUTPUT_CHECK; {'Bob'}. Notes; -----; Returns a same-type expression; evaluated on a :class:`.SetExpression`, returns a; :class:`.SetExpression`. Evaluated on an :class:`.ArrayExpression`,; returns an :class:`.ArrayExpression`. Parameters; ----------; f : function ( (arg) -> :class:`.BooleanExpression`); Function to evaluate for each element of the collection. Must return a; :class:`.BooleanExpression`. Returns; -------; :class:`.CollectionExpression`; Expression of the same type as the callee.; """"""; # FIXME: enable doctest. def unify_ret(t):; if t != tbool:; raise TypeError(""'filter' expects 'f' to return an expression of type 'bool', found '{}'"".format(t)); return hl.tarray(self._type.element_type). def transform_ir(array, name, body):; return ir.toArray(ir.StreamFilter(ir.toStream(array), name, body)). array_filter = hl.array(self)._ir_lambda_method(transform_ir, f, self.dtype.element_type, unify_ret). if isinstance(self.dtype, tset):; return hl.set(array_filter); else:; assert isinstance(self.dtype, tarray), self.dtype; return array_filter. [docs] @typecheck_method(f=func_spec(1, expr_bool)); def find(self, f):; """"""Returns the first element where `f` returns ``True``. Examples; --------. >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; -----; If `f` returns ``False`` for every element, then the result is missing. Parameters; ----------; f : function ( (arg) -> :class:`.BooleanExpression`); Function to evaluate for each element of the collection. Must return a; :class:`.BooleanExpression`. Returns; -------; :class:`.Expression`; Expression whose type is the element type of the collection.; """""". # FIXME this should short-circuit; return self.fold(; lambda accum, x: hl.if_else(hl.is_missing(accum) & f(x), x, accum), hl.missing(self._type.element_type); ). [docs] @typecheck_method(f=func_spec(1, expr_any)); def flatmap(self, f):; """"""Map each element of the collection to a new collection, and flatten",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:6324,Testability,assert,assert,6324,"ters; ----------; f : function ( (arg) -> :class:`.CollectionExpression`); Function from the element type of the collection to the type of the; collection. For instance, `flatmap` on a ``set<str>`` should take; a ``str`` and return a ``set``. Returns; -------; :class:`.CollectionExpression`; """"""; expected_type, s = (tarray, 'array') if isinstance(self._type, tarray) else (tset, 'set'); value_type = f(construct_variable(Env.get_uid(), self.dtype.element_type)).dtype. if not isinstance(value_type, expected_type):; raise TypeError(; ""'flatmap' expects 'f' to return an expression of type '{}', found '{}'"".format(s, value_type); ). def f2(x):; return hl.array(f(x)) if isinstance(value_type, tset) else f(x). def transform_ir(array, name, body):; return ir.toArray(ir.StreamFlatMap(ir.toStream(array), name, ir.ToStream(body))). array_flatmap = hl.array(self)._ir_lambda_method(transform_ir, f2, self.dtype.element_type, identity). if isinstance(self.dtype, tset):; return hl.set(array_flatmap); assert isinstance(self.dtype, tarray), self.dtype; return array_flatmap. [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def fold(self, f, zero):; """"""Reduces the collection with the given function `f`, provided the initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`. Returns; -------; :class:`.Expression`.; """"""; collection = self; if not isinstance(collection, ArrayExpression):; collection = hl.array(collection); return collection._to_stream().fold(lambda x, y: f(x, y), zero). [docs] @typecheck_method(f=func_spec(1, expr_bool)); def all(self, f):; """"""Returns ``True`` if `f` returns ``True`` for every element. Examples; --------. >>> hl.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:9572,Testability,assert,assert,9572,"ct_expr(; ir.GroupByKey(ir.toStream(keyed._ir)),; tdict(types[0], tarray(types[1])),; keyed._indices,; keyed._aggregations,; ). [docs] @typecheck_method(f=func_spec(1, expr_any)); def map(self, f):; """"""Transform each element of a collection. Examples; --------. >>> hl.eval(a.map(lambda x: x ** 3)); [1.0, 8.0, 27.0, 64.0, 125.0]. >>> hl.eval(s3.map(lambda x: x.length())); {3, 5, 7}. Parameters; ----------; f : function ( (arg) -> :class:`.Expression`); Function to transform each element of the collection. Returns; -------; :class:`.CollectionExpression`.; Collection where each element has been transformed according to `f`.; """""". def transform_ir(array, name, body):; a = ir.toArray(ir.StreamMap(ir.toStream(array), name, body)); if isinstance(self.dtype, tset):; a = ir.ToSet(ir.toStream(a)); return a. array_map = hl.array(self)._ir_lambda_method(; transform_ir, f, self._type.element_type, lambda t: self._type.__class__(t); ). if isinstance(self._type, tset):; return hl.set(array_map); assert isinstance(self._type, tarray); return array_map. [docs] @typecheck_method(f=anyfunc); def starmap(self, f):; r""""""Transform each element of a collection of tuples. Examples; --------. >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters; ----------; f : function ( (\*args) -> :class:`.Expression`); Function to transform each element of the collection. Returns; -------; :class:`.CollectionExpression`.; Collection where each element has been transformed according to `f`.; """""". return self.map(lambda e: f(*e)). [docs] def length(self):; """"""Returns the size of a collection. Examples; --------. >>> hl.eval(a.length()); 5. >>> hl.eval(s3.length()); 3. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; The number of elements in the collection.; """"""; return self.size(). [docs] def size(self):; """"""Returns the size of a collection. Examples; --------. >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns; -------; :class:`.Expression` of",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13829,Testability,test,test,13829,"step >= 0, 0, -1); if stop is not None:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop._ir, step._ir); else:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop, step._ir). return construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empt",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:26214,Testability,assert,assert,26214,"docs] def __pow__(self, other):; """"""Positionally raise to the power of an array or a scalar. Examples; --------. >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""; return self._bin_op_numeric('**', other, lambda _: tfloat64). def __rpow__(self, other):; return self._bin_op_numeric_reverse('**', other, lambda _: tfloat64). [docs]class SetExpression(CollectionExpression):; """"""Expression of type :class:`.tset`. >>> s1 = hl.literal({1, 2, 3}); >>> s2 = hl.literal({1, 3, 5}). See Also; --------; :class:`.CollectionExpression`; """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(SetExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tset); self._ec = coercer_from_dtype(type.element_type). [docs] @typecheck_method(item=expr_any); def add(self, item):; """"""Returns a new set including `item`. Examples; --------. >>> hl.eval(s1.add(10)) # doctest: +SKIP_OUTPUT_CHECK; {1, 2, 3, 10}. Parameters; ----------; item : :class:`.Expression`; Value to add. Returns; -------; :class:`.SetExpression`; Set with `item` added.; """"""; if not self._ec.can_coerce(item.dtype):; raise TypeError(; ""'SetExpression.add' expects 'item' to be the same type as its elements\n""; "" set element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self.dtype.element_type, item.dtype); ); return self._method(""add"", self.dtype, self._ec.coerce(item)). [docs] @typecheck_method(item=expr_any); def remove(self, item):; """"""Returns a new set excluding `item`. Examples; --------. >>> hl.eval(s1.remove(1)); {2, 3}. Parameters; ----------; item : :class:`.Expression`; Value to remove. Returns; -------; :class:`.SetExpression`; Set with ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:27820,Testability,test,test,27820,"tem': '{}'"".format(self.dtype.element_type, item.dtype); ); return self._method(""add"", self.dtype, self._ec.coerce(item)). [docs] @typecheck_method(item=expr_any); def remove(self, item):; """"""Returns a new set excluding `item`. Examples; --------. >>> hl.eval(s1.remove(1)); {2, 3}. Parameters; ----------; item : :class:`.Expression`; Value to remove. Returns; -------; :class:`.SetExpression`; Set with `item` removed.; """"""; if not self._ec.can_coerce(item.dtype):; raise TypeError(; ""'SetExpression.remove' expects 'item' to be the same type as its elements\n""; "" set element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self.dtype.element_type, item.dtype); ); return self._method(""remove"", self._type, self._ec.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns ``True`` if `item` is in the set. Examples; --------. >>> hl.eval(s1.contains(1)); True. >>> hl.eval(s1.contains(10)); False. Parameters; ----------; item : :class:`.Expression`; Value for inclusion test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is in the set.; """"""; if not self._ec.can_coerce(item.dtype):; raise TypeError(; ""'SetExpression.contains' expects 'item' to be the same type as its elements\n""; "" set element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self.dtype.element_type, item.dtype); ); return self._method(""contains"", tbool, self._ec.coerce(item)). [docs] @typecheck_method(s=expr_set()); def difference(self, s):; """"""Return the set of elements in the set that are not present in set `s`. Examples; --------. >>> hl.eval(s1.difference(s2)); {2}. >>> hl.eval(s2.difference(s1)); {5}. Parameters; ----------; s : :class:`.SetExpression`; Set expression of the same type. Returns; -------; :class:`.SetExpression`; Set of elements not in `s`.; """"""; if not s._type.element_type == self._type.element_type:; raise TypeError(; ""'SetExpression.difference' expects 's' to be the same type\n""; "" set type: '{}'\n""; "" type of 's': '{}'"".format(",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:36628,Testability,assert,assert,36628," """"""Get a field from each struct in this set. Examples; --------. >>> x = hl.set({hl.struct(a='foo', b=3), hl.struct(a='bar', b=4)}); >>> hl.eval(x.a) == {'foo', 'bar'}; True. >>> a = hl.set({hl.struct(b={hl.struct(inner=1),; ... hl.struct(inner=2)}),; ... hl.struct(b={hl.struct(inner=3)})}); >>> hl.eval(hl.flatten(a.b).inner) == {1, 2, 3}; True; >>> hl.eval(hl.flatten(a.b.inner)) == {1, 2, 3}; True. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """""". return self.map(lambda x: x[item]). [docs]class DictExpression(Expression):; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(DictExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tdict); self._kc = coercer_from_dtype(type.key_type); self._vc = coercer_from_dtype(type.value_type). [docs] @typecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:37764,Testability,test,test,37764,"ypecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""'DictExpression.contains' encountered an invalid key type\n""; "" dict key type: '{}'\n""; "" type of 'item': '{}'"".format(self._type.key_type, item.dtype); ); return self._method(""contains"", tbool, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any, default=nullable(expr_any)); def get(self, item, default=None):; """"""Returns the value associated with key `k` or a default value if that key is not present. Examples; --------. >>> hl.eval(d.get('Alice')); 43. >>> hl.eval(d.get('Anne')); None. >>> hl.eval(d.get('Anne', 0)); 0. Parameters; ----------; item : :class:`.Expression`; Key.; default : :class:`.Expression`; Default value. Must be same type as dictionary values. Returns; -------; :class:`.Expression`; The value associated with `item`, or `default`.; """"""; if not self._kc.can_co",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:45809,Testability,assert,assert,45809,"se:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.Expression`; Struct field.; """"""; if isinstance(item, str):; return self._get_field(item); if isinstance(item, int):; return self._get_field(self.dtype.fields[item]); else:; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return self.select(*self.dtype.fields[item.start : item.stop : item.step]). def __iter__(self):; return iter(self._fields). def __contains__(self, item):; return item in self._fields. def __hash__(self):; return object.__hash__(self). [docs] def __eq__(self, other):; """"""Check each field for equality. Parameters; ----------; other : :class:`.Expression`; An expression of the same type.; """"""; return Expression.__eq__(self, other). [docs] def __ne__(self, other):; return Expression.__ne__(self, other). def __nonzero__(self):; return Expression.__nonzero__(self). def _annotate_ordered(self, insertions_dict, field_order):; def get_type(field):; e = insertions_dict.get(field); if e is None:; e = self._fields[field]; return e.dtype. new_type = hl.tstruct(**{f: get_type(f) for f in field_order}); indices, aggregations = u",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:45867,Testability,assert,assert,45867,"etattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.Expression`; Struct field.; """"""; if isinstance(item, str):; return self._get_field(item); if isinstance(item, int):; return self._get_field(self.dtype.fields[item]); else:; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return self.select(*self.dtype.fields[item.start : item.stop : item.step]). def __iter__(self):; return iter(self._fields). def __contains__(self, item):; return item in self._fields. def __hash__(self):; return object.__hash__(self). [docs] def __eq__(self, other):; """"""Check each field for equality. Parameters; ----------; other : :class:`.Expression`; An expression of the same type.; """"""; return Expression.__eq__(self, other). [docs] def __ne__(self, other):; return Expression.__ne__(self, other). def __nonzero__(self):; return Expression.__nonzero__(self). def _annotate_ordered(self, insertions_dict, field_order):; def get_type(field):; e = insertions_dict.get(field); if e is None:; e = self._fields[field]; return e.dtype. new_type = hl.tstruct(**{f: get_type(f) for f in field_order}); indices, aggregations = unify_all(self, *insertions_dict.values()); return construct_exp",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:45923,Testability,assert,assert,45923,"bute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.Expression`; Struct field.; """"""; if isinstance(item, str):; return self._get_field(item); if isinstance(item, int):; return self._get_field(self.dtype.fields[item]); else:; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return self.select(*self.dtype.fields[item.start : item.stop : item.step]). def __iter__(self):; return iter(self._fields). def __contains__(self, item):; return item in self._fields. def __hash__(self):; return object.__hash__(self). [docs] def __eq__(self, other):; """"""Check each field for equality. Parameters; ----------; other : :class:`.Expression`; An expression of the same type.; """"""; return Expression.__eq__(self, other). [docs] def __ne__(self, other):; return Expression.__ne__(self, other). def __nonzero__(self):; return Expression.__nonzero__(self). def _annotate_ordered(self, insertions_dict, field_order):; def get_type(field):; e = insertions_dict.get(field); if e is None:; e = self._fields[field]; return e.dtype. new_type = hl.tstruct(**{f: get_type(f) for f in field_order}); indices, aggregations = unify_all(self, *insertions_dict.values()); return construct_expr(; ir.InsertFields.construct_with_deduplication(; self.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:53172,Testability,assert,assert,53172,"else '.'; return {f'{sep}{k}': f._summarize(agg_result[k]) for k, f in self.items()}. def _summary_aggs(self):; return hl.struct(**{k: f._all_summary_aggs() for k, f in self.items()}). [docs] def get(self, k, default=None):; """"""See :meth:`StructExpression.__getitem__`""""""; return super().get(k, default). [docs] def items(self):; """"""A list of pairs of field name and expression for said field.""""""; return super().items(). [docs] def keys(self):; """"""The list of field names.""""""; return super().keys(). [docs] def values(self):; """"""A list of expressions for each field.""""""; return super().values(). [docs]class TupleExpression(Expression, Sequence):; """"""Expression of type :class:`.ttuple`. >>> tup = hl.literal((""a"", 1, [1, 2, 3])); """""". [docs] @typecheck_method(item=oneof(int, slice)); def __getitem__(self, item):; """"""Index into the tuple. Examples; --------. >>> hl.eval(tup[1]); 1. Parameters; ----------; item : :obj:`int`; Element index. Returns; -------; :class:`.Expression`; """"""; if isinstance(item, slice):; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return hl.or_missing(; hl.is_defined(self), hl.tuple([self[i] for i in range(len(self))[item.start : item.stop : item.step]]); ); if not 0 <= item < len(self):; raise IndexError(""Out of bounds index, {}. Tuple length is {}."".format(item, len(self))); return construct_expr(ir.GetTupleElement(self._ir, item), self.dtype.types[item], self._indices). [docs] def __len__(self):; """"""Returns the length of the tuple. Examples; --------. >>> len(tup); 3. Returns; -------; :obj:`int`; """"""; return len(self.dtype.types). def __bool__(self):; return bool(len(self)). def __iter__(self):; for i in range(len(self)):; yield self[i]. def _nested_summary(self, agg_result, top):; return {f'[{i}]': self[i]._summarize(agg_result[i]) for i in range(len(self))}. def _summary_aggs(self):; return hl.tuple([self[i]._all_summary_a",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:53230,Testability,assert,assert,53230,".items()}. def _summary_aggs(self):; return hl.struct(**{k: f._all_summary_aggs() for k, f in self.items()}). [docs] def get(self, k, default=None):; """"""See :meth:`StructExpression.__getitem__`""""""; return super().get(k, default). [docs] def items(self):; """"""A list of pairs of field name and expression for said field.""""""; return super().items(). [docs] def keys(self):; """"""The list of field names.""""""; return super().keys(). [docs] def values(self):; """"""A list of expressions for each field.""""""; return super().values(). [docs]class TupleExpression(Expression, Sequence):; """"""Expression of type :class:`.ttuple`. >>> tup = hl.literal((""a"", 1, [1, 2, 3])); """""". [docs] @typecheck_method(item=oneof(int, slice)); def __getitem__(self, item):; """"""Index into the tuple. Examples; --------. >>> hl.eval(tup[1]); 1. Parameters; ----------; item : :obj:`int`; Element index. Returns; -------; :class:`.Expression`; """"""; if isinstance(item, slice):; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return hl.or_missing(; hl.is_defined(self), hl.tuple([self[i] for i in range(len(self))[item.start : item.stop : item.step]]); ); if not 0 <= item < len(self):; raise IndexError(""Out of bounds index, {}. Tuple length is {}."".format(item, len(self))); return construct_expr(ir.GetTupleElement(self._ir, item), self.dtype.types[item], self._indices). [docs] def __len__(self):; """"""Returns the length of the tuple. Examples; --------. >>> len(tup); 3. Returns; -------; :obj:`int`; """"""; return len(self.dtype.types). def __bool__(self):; return bool(len(self)). def __iter__(self):; for i in range(len(self)):; yield self[i]. def _nested_summary(self, agg_result, top):; return {f'[{i}]': self[i]._summarize(agg_result[i]) for i in range(len(self))}. def _summary_aggs(self):; return hl.tuple([self[i]._all_summary_aggs() for i in range(len(self))]). [docs] def count(self, value):; """"""Do no",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:53286,Testability,assert,assert,53286,"k: f._all_summary_aggs() for k, f in self.items()}). [docs] def get(self, k, default=None):; """"""See :meth:`StructExpression.__getitem__`""""""; return super().get(k, default). [docs] def items(self):; """"""A list of pairs of field name and expression for said field.""""""; return super().items(). [docs] def keys(self):; """"""The list of field names.""""""; return super().keys(). [docs] def values(self):; """"""A list of expressions for each field.""""""; return super().values(). [docs]class TupleExpression(Expression, Sequence):; """"""Expression of type :class:`.ttuple`. >>> tup = hl.literal((""a"", 1, [1, 2, 3])); """""". [docs] @typecheck_method(item=oneof(int, slice)); def __getitem__(self, item):; """"""Index into the tuple. Examples; --------. >>> hl.eval(tup[1]); 1. Parameters; ----------; item : :obj:`int`; Element index. Returns; -------; :class:`.Expression`; """"""; if isinstance(item, slice):; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return hl.or_missing(; hl.is_defined(self), hl.tuple([self[i] for i in range(len(self))[item.start : item.stop : item.step]]); ); if not 0 <= item < len(self):; raise IndexError(""Out of bounds index, {}. Tuple length is {}."".format(item, len(self))); return construct_expr(ir.GetTupleElement(self._ir, item), self.dtype.types[item], self._indices). [docs] def __len__(self):; """"""Returns the length of the tuple. Examples; --------. >>> len(tup); 3. Returns; -------; :obj:`int`; """"""; return len(self.dtype.types). def __bool__(self):; return bool(len(self)). def __iter__(self):; for i in range(len(self)):; yield self[i]. def _nested_summary(self, agg_result, top):; return {f'[{i}]': self[i]._summarize(agg_result[i]) for i in range(len(self))}. def _summary_aggs(self):; return hl.tuple([self[i]._all_summary_aggs() for i in range(len(self))]). [docs] def count(self, value):; """"""Do not use this method. This only exists for compatibility wi",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:101669,Testability,assert,assert,101669,"pe not in [hl.tint32, hl.tint64]:; raise TypeError(f""Argument {i} of reshape needs to be an integer, got {tuple_field_type}.""); shape_ir = hl.or_missing(hl.is_defined(shape), hl.tuple([hl.int64(i) for i in shape]))._ir; ndim = len(shape); else:; wrapped_shape = wrap_to_list(shape); ndim = len(wrapped_shape); shape_ir = hl.tuple(wrapped_shape)._ir. return construct_expr(; ir.NDArrayReshape(self._ir, shape_ir), tndarray(self._type.element_type, ndim), indices, aggregations; ). [docs] @typecheck_method(f=func_spec(1, expr_any)); def map(self, f):; """"""Applies an element-wise operation on an NDArray. Parameters; ----------; f : function ( (arg) -> :class:`.Expression`); Function to transform each element of the NDArray. Returns; -------; :class:`.NDArrayExpression`.; NDArray where each element has been transformed according to `f`.; """""". element_type = self._type.element_type; ndarray_map = self._ir_lambda_method(ir.NDArrayMap, f, element_type, lambda t: tndarray(t, self.ndim)). assert isinstance(self._type, tndarray); return ndarray_map. [docs] @typecheck_method(other=oneof(expr_ndarray(), list), f=func_spec(2, expr_any)); def map2(self, other, f):; """"""Applies an element-wise binary operation on two NDArrays. Parameters; ----------; other : class:`.NDArrayExpression`, :class:`.ArrayExpression`, numpy NDarray,; or nested python list/tuples. Both NDArrays must be the same shape or; broadcastable into common shape.; f : function ((arg1, arg2)-> :class:`.Expression`); Function to be applied to each element of both NDArrays. Returns; -------; :class:`.NDArrayExpression`.; Element-wise result of applying `f` to each index in NDArrays.; """""". if isinstance(other, (list, np.ndarray)):; other = hl.nd.array(other). self_broadcast, other_broadcast = self._broadcast_to_same_ndim(other). element_type1 = self_broadcast._type.element_type; element_type2 = other_broadcast._type.element_type; ndarray_map2 = self_broadcast._ir_lambda_method2(; other_broadcast, ir.NDArrayMap2, f, element_t",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:102748,Testability,assert,assert,102748,"r=oneof(expr_ndarray(), list), f=func_spec(2, expr_any)); def map2(self, other, f):; """"""Applies an element-wise binary operation on two NDArrays. Parameters; ----------; other : class:`.NDArrayExpression`, :class:`.ArrayExpression`, numpy NDarray,; or nested python list/tuples. Both NDArrays must be the same shape or; broadcastable into common shape.; f : function ((arg1, arg2)-> :class:`.Expression`); Function to be applied to each element of both NDArrays. Returns; -------; :class:`.NDArrayExpression`.; Element-wise result of applying `f` to each index in NDArrays.; """""". if isinstance(other, (list, np.ndarray)):; other = hl.nd.array(other). self_broadcast, other_broadcast = self._broadcast_to_same_ndim(other). element_type1 = self_broadcast._type.element_type; element_type2 = other_broadcast._type.element_type; ndarray_map2 = self_broadcast._ir_lambda_method2(; other_broadcast, ir.NDArrayMap2, f, element_type1, element_type2, lambda t: tndarray(t, self_broadcast.ndim); ). assert isinstance(self._type, tndarray); return ndarray_map2. def _broadcast_to_same_ndim(self, other):; if isinstance(other, NDArrayExpression):; if self.ndim < other.ndim:; return self._broadcast(other.ndim), other; elif self.ndim > other.ndim:; return self, other._broadcast(self.ndim). return self, other. def _broadcast(self, n_output_dims):; assert self.ndim < n_output_dims. # Right-align existing dimensions and start prepending new ones; # to the left: e.g. [0, 1] -> [3, 2, 0, 1]; # Based off numpy broadcasting with the assumption that everything; # can be thought to have an infinite number of 1-length dimensions; # prepended; old_dims = range(self.ndim); new_dims = range(self.ndim, n_output_dims); idx_mapping = list(reversed(new_dims)) + list(old_dims). return construct_expr(; ir.NDArrayReindex(self._ir, idx_mapping),; tndarray(self._type.element_type, n_output_dims),; self._indices,; self._aggregations,; ). [docs]class NDArrayNumericExpression(NDArrayExpression):; """"""Expression of type :cl",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:103096,Testability,assert,assert,103096,"adcastable into common shape.; f : function ((arg1, arg2)-> :class:`.Expression`); Function to be applied to each element of both NDArrays. Returns; -------; :class:`.NDArrayExpression`.; Element-wise result of applying `f` to each index in NDArrays.; """""". if isinstance(other, (list, np.ndarray)):; other = hl.nd.array(other). self_broadcast, other_broadcast = self._broadcast_to_same_ndim(other). element_type1 = self_broadcast._type.element_type; element_type2 = other_broadcast._type.element_type; ndarray_map2 = self_broadcast._ir_lambda_method2(; other_broadcast, ir.NDArrayMap2, f, element_type1, element_type2, lambda t: tndarray(t, self_broadcast.ndim); ). assert isinstance(self._type, tndarray); return ndarray_map2. def _broadcast_to_same_ndim(self, other):; if isinstance(other, NDArrayExpression):; if self.ndim < other.ndim:; return self._broadcast(other.ndim), other; elif self.ndim > other.ndim:; return self, other._broadcast(self.ndim). return self, other. def _broadcast(self, n_output_dims):; assert self.ndim < n_output_dims. # Right-align existing dimensions and start prepending new ones; # to the left: e.g. [0, 1] -> [3, 2, 0, 1]; # Based off numpy broadcasting with the assumption that everything; # can be thought to have an infinite number of 1-length dimensions; # prepended; old_dims = range(self.ndim); new_dims = range(self.ndim, n_output_dims); idx_mapping = list(reversed(new_dims)) + list(old_dims). return construct_expr(; ir.NDArrayReindex(self._ir, idx_mapping),; tndarray(self._type.element_type, n_output_dims),; self._indices,; self._aggregations,; ). [docs]class NDArrayNumericExpression(NDArrayExpression):; """"""Expression of type :class:`.tndarray` with a numeric element type. Numeric ndarrays support arithmetic both with scalar values and other; arrays. Arithmetic between two numeric ndarrays requires that the shapes of; each ndarray be either identical or compatible for broadcasting. Operations; are applied positionally (``nd1 * nd2`` will multiply ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:118241,Testability,assert,assert,118241,"ices, aggregations=LinkedList); def construct_expr(; x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; if type is None:; return Expression(x, None, indices, aggregations); x.assign_type(type); if isinstance(type, tarray) and is_numeric(type.element_type):; return ArrayNumericExpression(x, type, indices, aggregations); elif isinstance(type, tarray):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return ArrayStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tset):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return SetStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tndarray) and is_numeric(type.element_type):; return NDArrayNumericExpression(x, type, indices, aggregations); elif type in scalars:; return scalars[type](x, type, indices, aggregations); elif type.__class__ in typ_to_expr:; return typ_to_expr[type.__class__](x, type, indices, aggregations); else:; raise NotImplementedError(type). @typecheck(name=str, type=HailType, indices=Indices); def construct_reference(name, type, indices):; assert isinstance(type, hl.tstruct); x = ir.SelectedTopLevelReference(name, type); return construct_expr(x, type, indices). @typecheck(name=str, type=HailType, indices=Indices, aggregations=LinkedList); def construct_variable(name, type, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation)):; return construct_expr(ir.Ref(name, type), type, indices, aggregations).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html:8635,Deployability,update,updated,8635,"otate_fields[coords] = coord_expr; else:; coords = src._fields_inverse[coord_expr]. if isinstance(src, hl.MatrixTable):; new_src = src.annotate_rows(**annotate_fields); else:; new_src = src.annotate(**annotate_fields). locus_expr = new_src[locus]; if coord_expr is not None:; coord_expr = new_src[coords]. if coord_expr is None:; coord_expr = locus_expr.position. rg = locus_expr.dtype.reference_genome; contig_group_expr = hl.agg.group_by(hl.locus(locus_expr.contig, 1, reference_genome=rg), hl.agg.collect(coord_expr)). # check loci are in sorted order; last_pos = hl.fold(; lambda a, elt: (; hl.case(); .when(a <= elt, elt); .or_error(; hl.str(""locus_windows: 'locus_expr' global position must be in ascending order. ""); + hl.str(a); + hl.str("" was not less then or equal to ""); + hl.str(elt); ); ),; -1,; hl.agg.collect(; hl.case(); .when(hl.is_defined(locus_expr), locus_expr.global_position()); .or_error(""locus_windows: missing value for 'locus_expr'.""); ),; ); checked_contig_groups = (; hl.case().when(last_pos >= 0, contig_group_expr).or_error(""locus_windows: 'locus_expr' has length 0""); ). contig_groups = locus_expr._aggregation_method()(checked_contig_groups, _localize=False). coords = hl.sorted(hl.array(contig_groups)).map(lambda t: t[1]); starts_and_stops = hl._locus_windows_per_contig(coords, radius). if not _localize:; return starts_and_stops. starts, stops = hl.eval(starts_and_stops); return np.array(starts), np.array(stops). def _check_dims(a, name, ndim, min_size=1):; if len(a.shape) != ndim:; raise ValueError(f'{name} must be {ndim}-dimensional, ' f'found {a.ndim}'); for i in range(ndim):; if a.shape[i] < min_size:; raise ValueError(f'{name}.shape[{i}] must be at least ' f'{min_size}, found {a.shape[i]}'). def _ndarray_matmul_ndim(left, right):; if left == 1 and right == 1:; return 0; elif left == 1:; return right - 1; elif right == 1:; return left - 1; else:; assert left == right; return left.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html:8560,Testability,assert,assert,8560,"otate_fields[coords] = coord_expr; else:; coords = src._fields_inverse[coord_expr]. if isinstance(src, hl.MatrixTable):; new_src = src.annotate_rows(**annotate_fields); else:; new_src = src.annotate(**annotate_fields). locus_expr = new_src[locus]; if coord_expr is not None:; coord_expr = new_src[coords]. if coord_expr is None:; coord_expr = locus_expr.position. rg = locus_expr.dtype.reference_genome; contig_group_expr = hl.agg.group_by(hl.locus(locus_expr.contig, 1, reference_genome=rg), hl.agg.collect(coord_expr)). # check loci are in sorted order; last_pos = hl.fold(; lambda a, elt: (; hl.case(); .when(a <= elt, elt); .or_error(; hl.str(""locus_windows: 'locus_expr' global position must be in ascending order. ""); + hl.str(a); + hl.str("" was not less then or equal to ""); + hl.str(elt); ); ),; -1,; hl.agg.collect(; hl.case(); .when(hl.is_defined(locus_expr), locus_expr.global_position()); .or_error(""locus_windows: missing value for 'locus_expr'.""); ),; ); checked_contig_groups = (; hl.case().when(last_pos >= 0, contig_group_expr).or_error(""locus_windows: 'locus_expr' has length 0""); ). contig_groups = locus_expr._aggregation_method()(checked_contig_groups, _localize=False). coords = hl.sorted(hl.array(contig_groups)).map(lambda t: t[1]); starts_and_stops = hl._locus_windows_per_contig(coords, radius). if not _localize:; return starts_and_stops. starts, stops = hl.eval(starts_and_stops); return np.array(starts), np.array(stops). def _check_dims(a, name, ndim, min_size=1):; if len(a.shape) != ndim:; raise ValueError(f'{name} must be {ndim}-dimensional, ' f'found {a.ndim}'); for i in range(ndim):; if a.shape[i] < min_size:; raise ValueError(f'{name}.shape[{i}] must be at least ' f'{min_size}, found {a.shape[i]}'). def _ndarray_matmul_ndim(left, right):; if left == 1 and right == 1:; return 0; elif left == 1:; return right - 1; elif right == 1:; return left - 1; else:; assert left == right; return left.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6230,Availability,checkpoint,checkpoint,6230," 3))),; _e10=(; 4 * (p**3) * q * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * p * (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e20=(; (p**4) * ((X - 1) / X) * ((X - 2) / X) * ((X - 3) / X) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + (q**4) * ((Y - 1) / Y) * ((Y - 2) / Y) * ((Y - 3) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * (p**2) * (q**2) * ((X - 1) / X) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6573,Availability,checkpoint,checkpoint,6573,"1) / Y) * ((Y - 2) / Y) * ((Y - 3) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * (p**2) * (q**2) * ((X - 1) / X) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6664,Availability,checkpoint,checkpoint,6664,"4 * (p**2) * (q**2) * ((X - 1) / X) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6763,Availability,checkpoint,checkpoint,6763,"; ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.u",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6850,Availability,checkpoint,checkpoint,6850," (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6977,Availability,checkpoint,checkpoint,6977,"T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0)",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7118,Availability,checkpoint,checkpoint,7118,"T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.flo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7539,Availability,checkpoint,checkpoint,7539,"y_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7609,Availability,checkpoint,checkpoint,7609,"T = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7679,Availability,checkpoint,checkpoint,7679,"w_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = res",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7755,Availability,checkpoint,checkpoint,7755,".checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); resul",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7831,Availability,checkpoint,checkpoint,7831,"IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7907,Availability,checkpoint,checkpoint,7907,"ISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd=bound_result(result.ibd)); result = result.annotate(ibd=result.ibd.annotate",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:9258,Deployability,update,updated,9258,"s.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd=bound_result(result.ibd)); result = result.annotate(ibd=result.ibd.annotate(PI_HAT=result.ibd.Z1 / 2 + result.ibd.Z2)); result = result.filter((result.i < result.j) & (min <= result.ibd.PI_HAT) & (result.ibd.PI_HAT <= max)). samples = hl.literal(dataset.s.collect()); result = result.key_by(i=samples[hl.int32(result.i)], j=samples[hl.int32(result.j)]). return result.persist().  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:2169,Performance,perform,perform,2169,"by_descent(dataset, maf=None, bounded=True, min=None, max=None) -> Table:; """"""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :func:`.identity_by_descent` requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first. The resulting :class:`.Table` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... Parameters; ----------; dataset : :class:`.MatrixTable`; Variant-keyed and sample-keyed :class:`.MatrixTable` containing genotype information.; maf : :class:`.Float64Expression`, optional; Row-indexed expression for the minor allele frequency.; bounded : :obj:`bool`; Fo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:8848,Availability,checkpoint,checkpoint,8848,"able` whose rows and columns are keys are taken from; `call-expr`'s column keys. It has one entry field, `phi`.; """"""; mt = matrix_table_source('king/call_expr', call_expr); call = Env.get_uid(); mt = mt.annotate_entries(**{call: call_expr}). is_hom_ref = Env.get_uid(); is_het = Env.get_uid(); is_hom_var = Env.get_uid(); is_defined = Env.get_uid(); mt = mt.unfilter_entries(); mt = mt.select_entries(**{; is_hom_ref: hl.float(hl.or_else(mt[call].is_hom_ref(), 0)),; is_het: hl.float(hl.or_else(mt[call].is_het(), 0)),; is_hom_var: hl.float(hl.or_else(mt[call].is_hom_var(), 0)),; is_defined: hl.float(hl.is_defined(mt[call])),; }); ref = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_ref], block_size=block_size); het = hl.linalg.BlockMatrix.from_entry_expr(mt[is_het], block_size=block_size); var = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_var], block_size=block_size); defined = hl.linalg.BlockMatrix.from_entry_expr(mt[is_defined], block_size=block_size); ref_var = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[ki",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:9144,Availability,checkpoint,checkpoint,9144,"_hom_var = Env.get_uid(); is_defined = Env.get_uid(); mt = mt.unfilter_entries(); mt = mt.select_entries(**{; is_hom_ref: hl.float(hl.or_else(mt[call].is_hom_ref(), 0)),; is_het: hl.float(hl.or_else(mt[call].is_het(), 0)),; is_hom_var: hl.float(hl.or_else(mt[call].is_hom_var(), 0)),; is_defined: hl.float(hl.is_defined(mt[call])),; }); ref = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_ref], block_size=block_size); het = hl.linalg.BlockMatrix.from_entry_expr(mt[is_het], block_size=block_size); var = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_var], block_size=block_size); defined = hl.linalg.BlockMatrix.from_entry_expr(mt[is_defined], block_size=block_size); ref_var = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renam",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:9346,Availability,checkpoint,checkpoint,9346,"call].is_het(), 0)),; is_hom_var: hl.float(hl.or_else(mt[call].is_hom_var(), 0)),; is_defined: hl.float(hl.is_defined(mt[call])),; }); ref = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_ref], block_size=block_size); het = hl.linalg.BlockMatrix.from_entry_expr(mt[is_het], block_size=block_size); var = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_var], block_size=block_size); defined = hl.linalg.BlockMatrix.from_entry_expr(mt[is_defined], block_size=block_size); ref_var = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renaming, _ = deduplicate(list(col_key), already_used=set(col_key)); assert len(renaming) == len(col_key). kinship_between = kinship_between.key_rows_by(; **cols[kinship_between.row_idx].select(*col_key).ren",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:2844,Deployability,configurat,configurations,2844,"non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. :math:`X_{i,s}` is calculated by invoking; :meth:`~.CallExpression.n_alt_alleles` on the `call_expr`. The three counts above, :math:`N^{Aa}`, :math:`N^{Aa,Aa}`, and; :math:`N^{AA,aa}`, exclude variants where one or both individuals have; missing genotypes. In terms of the symbols above, we can define :math:`d`, the genetic distance; between two samples. We can interpret :math:`d` as an unnormalized; measurement of the genetic material not shared identically-by-descent:. .. math::. d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2. In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. +-------------------------------+----------+----------+----------+; |:math:`(X_{i,s} - X_{j,s})^2` |homref |het |homalt |; +-------------------------------+----------+----------+----------+; |homref |0 |1 |4 |; +-------------------------------+----------+----------+----------+; |het |1 |0 |1 |; +-------------------------------+----------+----------+----------+; |homalt |4 |1 |0 |; +-------------------------------+----------+----------+----------+. which leads to this expression for genetic distance:. .. math::. d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}. The first term, :math:`4 N^{AA,aa}_{i,j}`, accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:10801,Deployability,update,updated,10801,"ar = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renaming, _ = deduplicate(list(col_key), already_used=set(col_key)); assert len(renaming) == len(col_key). kinship_between = kinship_between.key_rows_by(; **cols[kinship_between.row_idx].select(*col_key).rename(dict(renaming)); ). kinship_between = kinship_between.annotate_entries(; min_n_hets=hl.min(kinship_between.n_hets_row, kinship_between.n_hets_col); ); return (; kinship_between.select_entries(; phi=(0.5); + (; (2 * kinship_between.het_hom_balance + -kinship_between.n_hets_row - kinship_between.n_hets_col); / (4 * kinship_between.min_n_hets); ); ); .select_rows(); .select_cols(); .select_globals(); ).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:2844,Modifiability,config,configurations,2844,"non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. :math:`X_{i,s}` is calculated by invoking; :meth:`~.CallExpression.n_alt_alleles` on the `call_expr`. The three counts above, :math:`N^{Aa}`, :math:`N^{Aa,Aa}`, and; :math:`N^{AA,aa}`, exclude variants where one or both individuals have; missing genotypes. In terms of the symbols above, we can define :math:`d`, the genetic distance; between two samples. We can interpret :math:`d` as an unnormalized; measurement of the genetic material not shared identically-by-descent:. .. math::. d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2. In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. +-------------------------------+----------+----------+----------+; |:math:`(X_{i,s} - X_{j,s})^2` |homref |het |homalt |; +-------------------------------+----------+----------+----------+; |homref |0 |1 |4 |; +-------------------------------+----------+----------+----------+; |het |1 |0 |1 |; +-------------------------------+----------+----------+----------+; |homalt |4 |1 |0 |; +-------------------------------+----------+----------+----------+. which leads to this expression for genetic distance:. .. math::. d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}. The first term, :math:`4 N^{AA,aa}_{i,j}`, accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:10215,Testability,assert,assert,10215,"ar = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renaming, _ = deduplicate(list(col_key), already_used=set(col_key)); assert len(renaming) == len(col_key). kinship_between = kinship_between.key_rows_by(; **cols[kinship_between.row_idx].select(*col_key).rename(dict(renaming)); ). kinship_between = kinship_between.annotate_entries(; min_n_hets=hl.min(kinship_between.n_hets_row, kinship_between.n_hets_col); ); return (; kinship_between.select_entries(; phi=(0.5); + (; (2 * kinship_between.het_hom_balance + -kinship_between.n_hets_row - kinship_between.n_hets_col); / (4 * kinship_between.min_n_hets); ); ); .select_rows(); .select_cols(); .select_globals(); ).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html:3481,Deployability,update,updated,3481," ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_generation_tup, idx):; prev_size = prev_generation_tup[1]; n_new = hl.int32(hl.floor(prev_size * generation_size_multiplier)); new_generation = hl.range(n_new).map(; lambda i: hl.struct(; s=hl.str('generation_') + hl.str(idx + 1) + hl.str('_idx_') + hl.str(i),; original=hl.missing('str'),; mother=hl.rand_int32(0, prev_size),; father=hl.rand_int32(0, prev_size),; ); ); return (new_generation, (prev_size + n_new) if keep_founders else n_new). ht = ht.annotate_globals(; generations=hl.range(n_rounds).scan(; lambda prev, idx: make_new_generation(prev, idx), (ht.generation_0, hl.len(ht.generation_0)); ); ). def simulate_mating_calls(prev_generation_calls, new_generation):; new_samples = new_generation.map(; lambda samp: hl.call(; prev_generation_calls[samp.mother][hl.rand_int32(0, 2)],; prev_generation_calls[samp.father][hl.rand_int32(0, 2)],; ); ); if keep_founders:; return prev_generation_calls.extend(new_samples); else:; return new_samples. ht = ht.annotate(; __new_entries=hl.fold(; lambda prev_calls, generation_metadata: simulate_mating_calls(prev_calls, generation_metadata[0]),; ht.__entries.GT,; ht.generations[1:],; ).map(lambda gt: hl.struct(GT=gt)); ); ht = ht.annotate_globals(; __new_cols=ht.generations.flatmap(lambda x: x[0]) if keep_founders else ht.generations[-1][0]; ); ht = ht.drop('__entries', '__cols', 'generation_0', 'generations'); return ht._unlocalize_entries('__new_entries', '__new_cols', list('s')).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html:2906,Modifiability,extend,extend,2906," ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_generation_tup, idx):; prev_size = prev_generation_tup[1]; n_new = hl.int32(hl.floor(prev_size * generation_size_multiplier)); new_generation = hl.range(n_new).map(; lambda i: hl.struct(; s=hl.str('generation_') + hl.str(idx + 1) + hl.str('_idx_') + hl.str(i),; original=hl.missing('str'),; mother=hl.rand_int32(0, prev_size),; father=hl.rand_int32(0, prev_size),; ); ); return (new_generation, (prev_size + n_new) if keep_founders else n_new). ht = ht.annotate_globals(; generations=hl.range(n_rounds).scan(; lambda prev, idx: make_new_generation(prev, idx), (ht.generation_0, hl.len(ht.generation_0)); ); ). def simulate_mating_calls(prev_generation_calls, new_generation):; new_samples = new_generation.map(; lambda samp: hl.call(; prev_generation_calls[samp.mother][hl.rand_int32(0, 2)],; prev_generation_calls[samp.father][hl.rand_int32(0, 2)],; ); ); if keep_founders:; return prev_generation_calls.extend(new_samples); else:; return new_samples. ht = ht.annotate(; __new_entries=hl.fold(; lambda prev_calls, generation_metadata: simulate_mating_calls(prev_calls, generation_metadata[0]),; ht.__entries.GT,; ht.generations[1:],; ).map(lambda gt: hl.struct(GT=gt)); ); ht = ht.annotate_globals(; __new_cols=ht.generations.flatmap(lambda x: x[0]) if keep_founders else ht.generations[-1][0]; ); ht = ht.drop('__entries', '__cols', 'generation_0', 'generations'); return ht._unlocalize_entries('__new_entries', '__new_cols', list('s')).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:3921,Availability,down,down,3921,"s=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:7681,Availability,avail,available,7681,"k^{(2)}_{ij}} \coloneqq; \frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their ki",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:9257,Availability,reliab,reliably,9257,"option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that :math:`g_{is}` is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first. The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the `statistics` parameter:. - `i` (``col_key.dtype``) -- First sample. (key field); - `j` (``col_key.dtype``) -- Second sample. (key field); - `kin` (:py:data:`.tfloat64`) -- Kinship estimate, :math:`\widehat{\phi_{ij}}`.; - `ibd2` (:py:data:`.tfloat64`) -- IBD2 estimate, :math:`\widehat{k^{(2)}_{ij}}`.; - `ibd0` (:py:data:`.tfloat64`) -- IBD0 estimate, :math:`\widehat{k^{(0)}_{ij}}`.; - `ibd1` (:py:data:`.tfloat64`) -- IBD1 estimate, :math:`\widehat{k^{(1)}_{ij}}`. Here ``col_key`` refers to the column key of the source matrix table,; and ``col_key.dtype`",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:10668,Availability,down,downstream,10668," 4, 5, or 6 fields below, depending on; the `statistics` parameter:. - `i` (``col_key.dtype``) -- First sample. (key field); - `j` (``col_key.dtype``) -- Second sample. (key field); - `kin` (:py:data:`.tfloat64`) -- Kinship estimate, :math:`\widehat{\phi_{ij}}`.; - `ibd2` (:py:data:`.tfloat64`) -- IBD2 estimate, :math:`\widehat{k^{(2)}_{ij}}`.; - `ibd0` (:py:data:`.tfloat64`) -- IBD0 estimate, :math:`\widehat{k^{(0)}_{ij}}`.; - `ibd1` (:py:data:`.tfloat64`) -- IBD1 estimate, :math:`\widehat{k^{(1)}_{ij}}`. Here ``col_key`` refers to the column key of the source matrix table,; and ``col_key.dtype`` is a struct containing the column key fields. There is one row for each pair of distinct samples (columns), where `i`; corresponds to the column of smaller column index. In particular, if the; same column key value exists for :math:`n` columns, then the resulting; table will have :math:`\binom{n-1}{2}` rows with both key fields equal to; that column key value. This may result in unexpected behavior in downstream; processing. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; min_individual_maf : :obj:`float`; The minimum individual-specific minor allele frequency.; If either individual-specific minor allele frequency for a pair of; individuals is below this threshold, then the variant will not; be used to estimate relatedness for the pair.; k : :obj:`int`, optional; If set, `k` principal component scores are computed and used.; Exactly one of `k` and `scores_expr` must be specified.; scores_expr : :class:`.ArrayNumericExpression`, optional; Column-indexed expression of principal component scores, with the same; source as `call_expr`. All array values must have the same positive length,; corresponding to the number of principal components, and all scores must; be non-missing. Exactly one of `k` and `scores_expr` must be specified.; min_kinship : :obj:`float`, optional; If set, pairs of samples with kinship lower than `min_kinship` ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:15274,Availability,checkpoint,checkpoint,15274,"lter(ht.i == ht.j, keep=False). col_keys = hl.literal(mt.select_cols().key_cols_by().cols().collect(), dtype=tarray(mt.col_key.dtype)); return ht.key_by(i=col_keys[ht.i], j=col_keys[ht.j]).persist(). def _bad_mu(mu: Float64Expression, maf: float) -> BooleanExpression:; """"""Check if computed value for estimated individual-specific allele; frequency (mu) is not valid for estimating relatedness. Parameters; ----------; mu : :class:`.Float64Expression`; Estimated individual-specific allele frequency.; maf : :obj:`float`; Minimum individual-specific minor allele frequency. Returns; -------; :class:`.BooleanExpression`; ``True`` if `mu` is not valid for relatedness estimation, else ``False``.; """"""; return (mu <= maf) | (mu >= (1.0 - maf)) | (mu <= 0.0) | (mu >= 1.0). def _gram(M: BlockMatrix) -> BlockMatrix:; """"""Compute Gram matrix, `M.T @ M`. Parameters; ----------; M : :class:`.BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; `M.T @ M`; """"""; return (M.T @ M).checkpoint(new_temp_file('pc_relate_bm/gram', 'bm')). def _dominance_encoding(g: Float64Expression, mu: Float64Expression) -> Float64Expression:; """"""Compute value for a single entry in dominance encoding of genotype matrix,; given the number of alternate alleles from the genotype matrix and the; estimated individual-specific allele frequency. Parameters; ----------; g : :class:`.Float64Expression`; Alternate allele count.; mu : :class:`.Float64Expression`; Estimated individual-specific allele frequency. Returns; -------; gd : :class:`.Float64Expression`; Dominance-coded entry for dominance-coded genotype matrix.; """"""; gd = (; hl.case(); .when(hl.is_nan(mu), 0.0); .when(g == 0.0, mu); .when(g == 1.0, 0.0); .when(g == 2.0, 1 - mu); .or_error('entries in genotype matrix must be 0.0, 1.0, or 2.0'); ); return gd. def _AtB_plus_BtA(A: BlockMatrix, B: BlockMatrix) -> BlockMatrix:; """"""Compute `(A.T @ B) + (B.T @ A)`, used in estimating IBD0 (k0). Parameters; ----------; A : :class:`.BlockMatrix`; B : :class:`.BlockMatri",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:16393,Availability,checkpoint,checkpoint,16393,":; """"""Compute value for a single entry in dominance encoding of genotype matrix,; given the number of alternate alleles from the genotype matrix and the; estimated individual-specific allele frequency. Parameters; ----------; g : :class:`.Float64Expression`; Alternate allele count.; mu : :class:`.Float64Expression`; Estimated individual-specific allele frequency. Returns; -------; gd : :class:`.Float64Expression`; Dominance-coded entry for dominance-coded genotype matrix.; """"""; gd = (; hl.case(); .when(hl.is_nan(mu), 0.0); .when(g == 0.0, mu); .when(g == 1.0, 0.0); .when(g == 2.0, 1 - mu); .or_error('entries in genotype matrix must be 0.0, 1.0, or 2.0'); ); return gd. def _AtB_plus_BtA(A: BlockMatrix, B: BlockMatrix) -> BlockMatrix:; """"""Compute `(A.T @ B) + (B.T @ A)`, used in estimating IBD0 (k0). Parameters; ----------; A : :class:`.BlockMatrix`; B : :class:`.BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; `(A.T @ B) + (B.T @ A)`; """"""; temp = (A.T @ B).checkpoint(new_temp_file()); return temp + temp.T. def _replace_nan(M: BlockMatrix, value: float) -> BlockMatrix:; """"""Replace NaN entries in a dense :class:`.BlockMatrix` with provided value. Parameters; ----------; M: :class:`.BlockMatrix`; value: :obj:`float`; Value to replace NaN entries with. Returns; -------; :class:`.BlockMatrix`; """"""; return M._map_dense(lambda x: hl.if_else(hl.is_nan(x), value, x)). @typecheck(; call_expr=expr_call,; min_individual_maf=numeric,; k=nullable(int),; scores_expr=nullable(expr_array(expr_float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def _pc_relate_bm(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = ""all"",; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; assert 0.0 <= min_individu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:19064,Availability,checkpoint,checkpoint,19064,"f k and scores_expr is not None:; raise ValueError(""pc_relate_bm: exactly one of 'k' and 'scores_expr' "" ""must be set, found both""); else:; raise ValueError(""pc_relate_bm: exactly one of 'k' and 'scores_expr' "" ""must be set, found neither""). n_missing = scores_table.aggregate(agg.count_where(hl.is_missing(scores_table.__scores))); if n_missing > 0:; raise ValueError(f'Found {n_missing} columns with missing scores array.'); pc_scores = hl.nd.array(scores_table.collect(_localize=False).map(lambda x: x.__scores)). # Define NaN for missing values, otherwise cannot convert expr to block matrix; nan = hl.float64(float('NaN')). # Create genotype matrix, set missing GT entries to NaN; mt = mt.select_entries(__gt=call_expr.n_alt_alleles()).unfilter_entries(); gt_with_nan_expr = hl.or_else(hl.float64(mt.__gt), nan); if not block_size:; block_size = BlockMatrix.default_block_size(); g = BlockMatrix.from_entry_expr(gt_with_nan_expr, block_size=block_size); g = g.checkpoint(new_temp_file('pc_relate_bm/g', 'bm')); sqrt_n_samples = hl.nd.array([hl.sqrt(g.shape[1])]). # Recover singular values, S0, as vector of column norms of pc_scores if necessary; if compute_S0:; S0 = (pc_scores ** hl.int32(2)).sum(0).map(lambda x: hl.sqrt(x)); else:; S0 = hl.nd.array(eigens).map(lambda x: hl.sqrt(x)); # Set first entry of S to sqrt(n), for intercept term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:19935,Availability,checkpoint,checkpoint,19935,".default_block_size(); g = BlockMatrix.from_entry_expr(gt_with_nan_expr, block_size=block_size); g = g.checkpoint(new_temp_file('pc_relate_bm/g', 'bm')); sqrt_n_samples = hl.nd.array([hl.sqrt(g.shape[1])]). # Recover singular values, S0, as vector of column norms of pc_scores if necessary; if compute_S0:; S0 = (pc_scores ** hl.int32(2)).sum(0).map(lambda x: hl.sqrt(x)); else:; S0 = hl.nd.array(eigens).map(lambda x: hl.sqrt(x)); # Set first entry of S to sqrt(n), for intercept term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:20414,Availability,checkpoint,checkpoint,20414,"pt term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing(hl.tfloat64), k1=hl.missing(hl.tfloat64), k2=hl.missing(hl.tfloat64)). if statistics in ['kin2', 'kin20', 'all']:; # Compute inbreeding coefficient and dominance encoding of GT matrix; f_i = (2.0 * phi.diagonal()) - 1.0; gd = g._apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:20658,Availability,checkpoint,checkpoint,20658,"ape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing(hl.tfloat64), k1=hl.missing(hl.tfloat64), k2=hl.missing(hl.tfloat64)). if statistics in ['kin2', 'kin20', 'all']:; # Compute inbreeding coefficient and dominance encoding of GT matrix; f_i = (2.0 * phi.diagonal()) - 1.0; gd = g._apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()[ht.i, ht.j].entry). if statistics in ['kin20', 'all']:; # Get the numerator used in IBD0 (k0) computation (IBS0), compute indicator matrices for homozygotes; hom_alt = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 2.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsit",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:20834,Availability,checkpoint,checkpoint,20834,"om_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing(hl.tfloat64), k1=hl.missing(hl.tfloat64), k2=hl.missing(hl.tfloat64)). if statistics in ['kin2', 'kin20', 'all']:; # Compute inbreeding coefficient and dominance encoding of GT matrix; f_i = (2.0 * phi.diagonal()) - 1.0; gd = g._apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()[ht.i, ht.j].entry). if statistics in ['kin20', 'all']:; # Get the numerator used in IBD0 (k0) computation (IBS0), compute indicator matrices for homozygotes; hom_alt = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 2.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); hom_ref = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 0.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:7764,Deployability,release,release,7764,"s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs b",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:23163,Deployability,update,updated,23163,"_apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()[ht.i, ht.j].entry). if statistics in ['kin20', 'all']:; # Get the numerator used in IBD0 (k0) computation (IBS0), compute indicator matrices for homozygotes; hom_alt = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 2.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); hom_ref = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 0.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); ibs0 = _AtB_plus_BtA(hom_alt, hom_ref). # Get the denominator used in IBD0 (k0) computation; mu2 = _replace_nan(mu**2.0, 0.0); one_minus_mu2 = _replace_nan((1.0 - mu) ** 2.0, 0.0); k0_denom = _AtB_plus_BtA(mu2, one_minus_mu2). # Compute IBD0 (k0) estimates, correct the estimates where phi <= k0_cutoff; k0 = ibs0 / k0_denom; k0_cutoff = 2.0 ** (-5.0 / 2.0); ht = ht.annotate(k0=k0.entries()[ht.i, ht.j].entry); ht = ht.annotate(k0=hl.if_else(ht.kin <= k0_cutoff, 1.0 - (4.0 * ht.kin) + ht.k2, ht.k0)). if statistics == 'all':; # Finally, compute IBD1 (k1) estimate; ht = ht.annotate(k1=1.0 - (ht.k2 + ht.k0)). # Filter table to only have one row for each distinct pair of samples; ht = ht.filter(ht.i <= ht.j); ht = ht.rename({'k0': 'ibd0', 'k1': 'ibd1', 'k2': 'ibd2'}). if min_kinship is not None:; ht = ht.filter(ht.kin >= min_kinship). if statistics != 'all':; fields_to_drop = {'kin': ['ibd0', 'ibd1', 'ibd2'], 'kin2': ['ibd0', 'ibd1'], 'kin20': ['ibd1']}; ht = ht.drop(*fields_to_drop[statistics]). if not include_self_kinship:; ht = ht.filter(ht.i == ht.j, keep=False). col_keys = hl.literal(mt.select_cols().key_cols_by().cols().collect(), dtype=hl.tarray(mt.col_key.dtype)); return ht.key_by(i=col_keys[hl.int32(ht.i)], j=col_keys[hl.int32(ht.j)]).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:2292,Energy Efficiency,efficient,efficient,2292,"float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def pc_relate(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) # doctest: +SKIP. Only compute the kinship statistic. This is more efficient than; computing all statistics. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') # doctest: +SKIP. Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using :meth:`.Table.filter`. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) # doctest: +SKIP. One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:2506,Energy Efficiency,efficient,efficient,2506,",; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) # doctest: +SKIP. Only compute the kinship statistic. This is more efficient than; computing all statistics. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') # doctest: +SKIP. Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using :meth:`.Table.filter`. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) # doctest: +SKIP. One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:9684,Integrability,depend,depending,9684,"netic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that :math:`g_{is}` is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first. The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the `statistics` parameter:. - `i` (``col_key.dtype``) -- First sample. (key field); - `j` (``col_key.dtype``) -- Second sample. (key field); - `kin` (:py:data:`.tfloat64`) -- Kinship estimate, :math:`\widehat{\phi_{ij}}`.; - `ibd2` (:py:data:`.tfloat64`) -- IBD2 estimate, :math:`\widehat{k^{(2)}_{ij}}`.; - `ibd0` (:py:data:`.tfloat64`) -- IBD0 estimate, :math:`\widehat{k^{(0)}_{ij}}`.; - `ibd1` (:py:data:`.tfloat64`) -- IBD1 estimate, :math:`\widehat{k^{(1)}_{ij}}`. Here ``col_key`` refers to the column key of the source matrix table,; and ``col_key.dtype`` is a struct containing the column key fields. There is one row for each pair of distinct samples (columns), where `i`; corresponds to the column of smaller column index. In particular, if the; same column key value exists for :math:`n` columns, then the resulting; table will have :math:`\binom{n-1}{2}` rows with both key fields equal to; that column key value. This may result in unexpected behavior in downst",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:3826,Modifiability,inherit,inherited,3826,"puted principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:8084,Performance,load,loadings,8084,"{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:8111,Performance,load,loadings,8111,"{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:8191,Performance,perform,perform,8191,"phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their gene",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:17391,Testability,assert,assert,17391,"B_plus_BtA(A: BlockMatrix, B: BlockMatrix) -> BlockMatrix:; """"""Compute `(A.T @ B) + (B.T @ A)`, used in estimating IBD0 (k0). Parameters; ----------; A : :class:`.BlockMatrix`; B : :class:`.BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; `(A.T @ B) + (B.T @ A)`; """"""; temp = (A.T @ B).checkpoint(new_temp_file()); return temp + temp.T. def _replace_nan(M: BlockMatrix, value: float) -> BlockMatrix:; """"""Replace NaN entries in a dense :class:`.BlockMatrix` with provided value. Parameters; ----------; M: :class:`.BlockMatrix`; value: :obj:`float`; Value to replace NaN entries with. Returns; -------; :class:`.BlockMatrix`; """"""; return M._map_dense(lambda x: hl.if_else(hl.is_nan(x), value, x)). @typecheck(; call_expr=expr_call,; min_individual_maf=numeric,; k=nullable(int),; scores_expr=nullable(expr_array(expr_float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def _pc_relate_bm(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = ""all"",; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; assert 0.0 <= min_individual_maf <= 1.0, (; f'invalid argument: min_individual_maf={min_individual_maf}. '; f'Must have min_individual_maf on interval [0.0, 1.0].'; ); mt = matrix_table_source('pc_relate_bm/call_expr', call_expr); if k and scores_expr is None:; eigens, scores, _ = _hwe_normalized_blanczos(call_expr, k, compute_loadings=False, q_iterations=10); scores_table = scores.select(__scores=scores.scores).key_by().select('__scores'); compute_S0 = False; elif not k and scores_expr is not None:; analyze('pc_relate_bm/scores_expr', scores_expr, mt._col_indices); eigens = None; scores_table = mt.select_cols(__scores=scores_expr).key_cols_by().select_cols('__scores').cols(); compute_S0 = True; elif k",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:4055,Usability,simpl,simply,4055,"itional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry (as described by principal; component coordinates) and population allele frequency is linear. The estimators for kinship, and identity-by-descent zero, one, and two; follow.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:2036,Availability,failure,failure-tolerant,2036,"iantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instance-attributes; """"""A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. Examples; --------. A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences ::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:3597,Availability,avail,available,3597,"ew_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :class:`int`; The number of Variant Datasets to combine at once.; target_records : :class:`int`; The target number of var",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:3899,Availability,failure,failure,3899,"Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :class:`int`; The number of Variant Datasets to combine at once.; target_records : :class:`int`; The target number of variants per partition.; gvcf_batch_size : :class:`int`; The number of GVCFs to combine into a Variant Dataset at once.; contig_recoding : :class:`dict` mapping :class:`str` to :class:`str` or :obj:`None`; This mapping is applied to GVCF contigs before importing them into Hail. This is u",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:26193,Availability,failure,failure,26193,"alueError(; ""'gvcf_sample_names' and 'gvcf_paths' must have the same length ""; f'{len(gvcf_sample_names)} != {len(gvcf_paths)}'; ). if batch_size is None:; if gvcf_batch_size is None:; gvcf_batch_size = VariantDatasetCombiner._default_gvcf_batch_size; else:; pass; elif gvcf_batch_size is None:; warning(; 'The batch_size parameter is deprecated. '; 'The batch_size parameter will be removed in a future version of Hail. '; 'Please use gvcf_batch_size instead.'; ); gvcf_batch_size = batch_size; else:; raise ValueError(; 'Specify only one of batch_size and gvcf_batch_size. ' f'Received {batch_size} and {gvcf_batch_size}.'; ); del batch_size. def maybe_load_from_saved_path(save_path: str) -> Optional[VariantDatasetCombiner]:; if force:; return None; fs = hl.current_backend().fs; if fs.exists(save_path):; try:; combiner = load_combiner(save_path); warning(f'found existing combiner plan at {save_path}, using it'); # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); combiner._branch_factor = branch_factor; combiner._target_records = target_records; combiner._gvcf_batch_size = gvcf_batch_size; return combiner; except (ValueError, TypeError, OSError, KeyError) as e:; warning(; f'file exists at {save_path}, but it is not a valid combiner plan, overwriting\n'; f' caused by: {e}'; ); return None. # We do the first save_path check now after validating the arguments; if save_path is not None:; saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner. if len(gvcf_paths) > 0:; n_partition_args = (; int(intervals is not None); + int(import_interval_size is not None); + int(use_genome_default_intervals); + int(use_exome_default_intervals); ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': requ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30517,Deployability,update,update,30517,"call_fields:; warning(; ""Mismatch between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encod",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30551,Deployability,update,update,30551," between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); n",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30583,Deployability,update,update,30583,"elds. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30627,Deployability,update,update,30627,"m supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.pa",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30694,Deployability,update,update,30694,"\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = mayb",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30755,Deployability,update,update,30755," ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30807,Deployability,update,update,30807,"t = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generate",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30872,Deployability,update,update,30872,"rce_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VD",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30982,Deployability,update,update,30982,"tig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = [",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31089,Deployability,update,update,31089,"(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,;",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31233,Deployability,update,update,31233," transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetada",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31301,Deployability,update,update,31301,"e_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=Tr",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31419,Deployability,update,update,31419,"pe, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; refe",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31445,Deployability,update,update,31445,"t_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_geno",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31501,Deployability,update,update,31501,"; sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:34258,Deployability,update,updated,34258," x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor,; target_records=target_records,; gvcf_batch_size=gvcf_batch_size,; contig_recoding=contig_recoding,; call_fields=call_fields,; vdses=vdses,; gvcfs=gvcf_paths,; gvcf_import_intervals=intervals,; gvcf_external_header=gvcf_external_header,; gvcf_sample_names=gvcf_sample_names,; gvcf_info_to_keep=gvcf_info_to_keep,; gvcf_reference_entry_fields_to_keep=gvcf_reference_entry_fields_to_keep,; ); combiner._raise_if_output_exists(); return combiner. [docs]def load_combiner(path: str) -> VariantDatasetCombiner:; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; return VariantDatasetCombiner.load(path). class Encoder(json.JSONEncoder):; def default(self, o):; if isinstance(o, VariantDatasetCombiner):; return o.to_dict(); if isinstance(o, HailType):; return str(o); if isinstance(o, tmatrix):; return o.to_dict(); return json.JSONEncoder.default(self, o). class Decoder(json.JSONDecoder):; def __init__(self, **kwargs):; super().__init__(object_hook=Decoder._object_hook, **kwargs). @staticmethod; def _object_hook(obj):; if 'name' not in obj:; return obj; name = obj['name']; if name == VariantDatasetCombiner.__name__:; del obj['name']; obj['vdses'] = [VDSMetadata(*x) for x in obj['vdses']]; obj['dataset_type'] = CombinerOutType(*(tmatrix._from_json(ty) for ty in obj['dataset_type'])); if 'gvcf_type' in obj and obj['gvcf_type']:; obj['gvcf_type'] = tmatrix._from_json(obj['gvcf_type']). rg = hl.get_reference(obj['reference_genome']); obj['reference_genome'] = rg; intervals_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); intervals = intervals_type._convert_from_json(obj['gvcf_import_intervals']); obj['gvcf_import_intervals'] = intervals. return VariantDatasetCombiner(**obj); return obj.  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:3472,Integrability,depend,depends,3472,"10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :cl",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:11651,Integrability,message,messages,11651,"mit // len(self._gvcf_import_intervals); warning(f'gvcf_batch_size of {old_value} would produce too many tasks ' f'using {value} instead'); self._gvcf_batch_size = value. [docs] def __eq__(self, other):; if other.__class__ != VariantDatasetCombiner:; return False; for slot in self.__serialized_slots__:; if getattr(self, slot) != getattr(other, slot):; return False; return True. @property; def finished(self) -> bool:; """"""Have all GVCFs and input Variant Datasets been combined?""""""; return not self._gvcfs and not self._vdses. [docs] def save(self):; """"""Save a :class:`.VariantDatasetCombiner` to its `save_path`.""""""; fs = hl.current_backend().fs; try:; backup_path = self._save_path + '.bak'; if fs.exists(self._save_path):; fs.copy(self._save_path, backup_path); with fs.open(self._save_path, 'w') as out:; json.dump(self, out, indent=2, cls=Encoder); if fs.exists(backup_path):; fs.remove(backup_path); except OSError as e:; # these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail; print(f'Failed saving {self.__class__.__name__} state at {self._save_path}'); print(f'An attempt was made to copy {self._save_path} to {backup_path}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF mer",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:12926,Performance,load,load,12926,"ath}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; interval",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:13104,Performance,load,load,13104,"combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; intervals_typ = hl.tarray(hl.tinterval(hl.tlocus(self._reference_genome))); return {; 'name': self.__class__.__name__,; 'save_path': self._save_path,; 'output_path': self._output_path,; '",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:13238,Performance,load,loaded,13238,"s and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; intervals_typ = hl.tarray(hl.tinterval(hl.tlocus(self._reference_genome))); return {; 'name': self.__class__.__name__,; 'save_path': self._save_path,; 'output_path': self._output_path,; 'temp_path': self._temp_path,; 'reference_genome': str(self._reference_genome),; 'dataset_type': self._dataset_type,; 'gvcf_type': self._gv",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:23667,Performance,load,load,23667,",; gvcf_paths: Optional[List[str]] = None,; vds_paths: Optional[List[str]] = None,; vds_sample_counts: Optional[List[int]] = None,; intervals: Optional[List[Interval]] = None,; import_interval_size: Optional[int] = None,; use_genome_default_intervals: bool = False,; use_exome_default_intervals: bool = False,; gvcf_external_header: Optional[str] = None,; gvcf_sample_names: Optional[List[str]] = None,; gvcf_info_to_keep: Optional[Collection[str]] = None,; gvcf_reference_entry_fields_to_keep: Optional[Collection[str]] = None,; call_fields: Collection[str] = ['PGT'],; branch_factor: int = VariantDatasetCombiner._default_branch_factor,; target_records: int = VariantDatasetCombiner._default_target_records,; gvcf_batch_size: Optional[int] = None,; batch_size: Optional[int] = None,; reference_genome: Union[str, ReferenceGenome] = 'default',; contig_recoding: Optional[Dict[str, str]] = None,; force: bool = False,; ) -> VariantDatasetCombiner:; """"""Create a new :class:`.VariantDatasetCombiner` or load one from `save_path`.""""""; if not (gvcf_paths or vds_paths):; raise ValueError(""at least one of 'gvcf_paths' or 'vds_paths' must be nonempty""); if gvcf_paths is None:; gvcf_paths = []; if len(gvcf_paths) > 0:; if len(set(gvcf_paths)) != len(gvcf_paths):; duplicates = [gvcf for gvcf, count in collections.Counter(gvcf_paths).items() if count > 1]; duplicates = '\n '.join(duplicates); raise ValueError(f'gvcf paths should be unique, the following paths are repeated:{duplicates}'); if gvcf_sample_names is not None and len(set(gvcf_sample_names)) != len(gvcf_sample_names):; duplicates = [gvcf for gvcf, count in collections.Counter(gvcf_sample_names).items() if count > 1]; duplicates = '\n '.join(duplicates); raise ValueError(; ""provided sample names ('gvcf_sample_names') should be unique, ""; f'the following names are repeated:{duplicates}'; ). if vds_paths is None:; vds_paths = []; if vds_sample_counts is not None and len(vds_paths) != len(vds_sample_counts):; raise ValueError(; ""'vds_pa",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:33111,Performance,load,load,33111,"ype=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor,; target_records=target_records,; gvcf_batch_size=gvcf_batch_size,; contig_recoding=contig_recoding,; call_fields=call_fields,; vdses=vdses,; gvcfs=gvcf_paths,; gvcf_import_intervals=intervals,; gvcf_external_header=gvcf_external_header,; gvcf_sample_names=gvcf_sample_names,; gvcf_info_to_keep=gvcf_info_to_keep,; gvcf_reference_entry_fields_to_keep=gvcf_reference_entry_fields_to_keep,; ); combiner._raise_if_output_exists(); return combiner. [docs]def load_combiner(path: str) -> VariantDatasetCombiner:; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; return VariantDatasetCombiner.load(path). class Encoder(json.JSONEncoder):; def default(self, o):; if isinstance(o, VariantDatasetCombiner):; return o.to_dict(); if isinstance(o, HailType):; return str(o); if isinstance(o, tmatrix):; return o.to_dict(); return json.JSONEncoder.default(self, o). class Decoder(json.JSONDecoder):; def __init__(self, **kwargs):; super().__init__(object_hook=Decoder._object_hook, **kwargs). @staticmethod; def _object_hook(obj):; if 'name' not in obj:; return obj; name = obj['name']; if name == VariantDatasetCombiner.__name__:; del obj['name']; obj['vdses'] = [VDSMetadata(*x) for x in obj['vdses']]; obj['dataset_type'] = CombinerOutType(*(tmatrix._from_json(ty) for ty in obj['dataset_type'])); if 'gvcf_type' in obj and obj['gvcf_type']:; obj['gvcf_type'] = tmatrix._from_json(obj['gvcf_type']). rg = hl.get_reference(obj['reference_genome']); obj['reference_genome'] = rg; intervals_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); intervals = intervals_type._convert_from_json(obj['gvcf_import_inter",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:582,Security,hash,hashlib,582,". Hail | ; hail.vds.combiner.variant_dataset_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.combiner.variant_dataset_combiner. Source code for hail.vds.combiner.variant_dataset_combiner; import collections; import hashlib; import json; import os; import sys; import uuid; from itertools import chain; from math import floor, log; from typing import ClassVar, Collection, Dict, List, NamedTuple, Optional, Union. import hail as hl; from hail.expr import HailType, tmatrix; from hail.genetics.reference_genome import ReferenceGenome; from hail.utils import FatalError, Interval; from hail.utils.java import info, warning. from ..variant_dataset import VariantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instanc",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:26034,Security,hash,hash,26034,"alueError(; ""'gvcf_sample_names' and 'gvcf_paths' must have the same length ""; f'{len(gvcf_sample_names)} != {len(gvcf_paths)}'; ). if batch_size is None:; if gvcf_batch_size is None:; gvcf_batch_size = VariantDatasetCombiner._default_gvcf_batch_size; else:; pass; elif gvcf_batch_size is None:; warning(; 'The batch_size parameter is deprecated. '; 'The batch_size parameter will be removed in a future version of Hail. '; 'Please use gvcf_batch_size instead.'; ); gvcf_batch_size = batch_size; else:; raise ValueError(; 'Specify only one of batch_size and gvcf_batch_size. ' f'Received {batch_size} and {gvcf_batch_size}.'; ); del batch_size. def maybe_load_from_saved_path(save_path: str) -> Optional[VariantDatasetCombiner]:; if force:; return None; fs = hl.current_backend().fs; if fs.exists(save_path):; try:; combiner = load_combiner(save_path); warning(f'found existing combiner plan at {save_path}, using it'); # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); combiner._branch_factor = branch_factor; combiner._target_records = target_records; combiner._gvcf_batch_size = gvcf_batch_size; return combiner; except (ValueError, TypeError, OSError, KeyError) as e:; warning(; f'file exists at {save_path}, but it is not a valid combiner plan, overwriting\n'; f' caused by: {e}'; ); return None. # We do the first save_path check now after validating the arguments; if save_path is not None:; saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner. if len(gvcf_paths) > 0:; n_partition_args = (; int(intervals is not None); + int(import_interval_size is not None); + int(use_genome_default_intervals); + int(use_exome_default_intervals); ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': requ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:26616,Security,validat,validating,26616,"ch_size. def maybe_load_from_saved_path(save_path: str) -> Optional[VariantDatasetCombiner]:; if force:; return None; fs = hl.current_backend().fs; if fs.exists(save_path):; try:; combiner = load_combiner(save_path); warning(f'found existing combiner plan at {save_path}, using it'); # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); combiner._branch_factor = branch_factor; combiner._target_records = target_records; combiner._gvcf_batch_size = gvcf_batch_size; return combiner; except (ValueError, TypeError, OSError, KeyError) as e:; warning(; f'file exists at {save_path}, but it is not a valid combiner plan, overwriting\n'; f' caused by: {e}'; ); return None. # We do the first save_path check now after validating the arguments; if save_path is not None:; saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner. if len(gvcf_paths) > 0:; n_partition_args = (; int(intervals is not None); + int(import_interval_size is not None); + int(use_genome_default_intervals); + int(use_exome_default_intervals); ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': require one argument from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals' to choose GVCF partitioning""; ). if n_partition_args > 1:; warning(; ""'new_combiner': multiple colliding arguments found from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals'.""; ""\n The argument found first in the list in this warning will be used, and others ignored.""; ). if intervals is not None:; pass; elif import_interval_size is not None:; intervals = calculate_even_genome_partitioning(reference_genome, import_interval_size); elif use_genome_",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30495,Security,hash,hashlib,30495,"GT', 'GT'); }; if set(call_fields) != vds_call_fields:; warning(; ""Mismatch between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:693,Testability,log,log,693,". Hail | ; hail.vds.combiner.variant_dataset_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.combiner.variant_dataset_combiner. Source code for hail.vds.combiner.variant_dataset_combiner; import collections; import hashlib; import json; import os; import sys; import uuid; from itertools import chain; from math import floor, log; from typing import ClassVar, Collection, Dict, List, NamedTuple, Optional, Union. import hail as hl; from hail.expr import HailType, tmatrix; from hail.genetics.reference_genome import ReferenceGenome; from hail.utils import FatalError, Interval; from hail.utils.java import info, warning. from ..variant_dataset import VariantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instanc",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:9759,Testability,log,log,9759,"e_names is None) != (gvcf_external_header is None):; raise ValueError(""both 'gvcf_sample_names' and 'gvcf_external_header' must be set or unset""); if gvcf_sample_names is not None and len(gvcf_sample_names) != len(gvcfs):; raise ValueError(; ""'gvcf_sample_names' and 'gvcfs' must have the same length "" f'{len(gvcf_sample_names)} != {len(gvcfs)}'; ); if branch_factor < 2:; raise ValueError(f""'branch_factor' must be at least 2, found {branch_factor}""); if gvcf_batch_size < 1:; raise ValueError(f""'gvcf_batch_size' must be at least 1, found {gvcf_batch_size}""). self._save_path = save_path; self._output_path = output_path; self._temp_path = temp_path; self._reference_genome = reference_genome; self._dataset_type = dataset_type; self._gvcf_type = gvcf_type; self._branch_factor = branch_factor; self._target_records = target_records; self._contig_recoding = contig_recoding; self._call_fields = list(call_fields); self._vdses = collections.defaultdict(list); for vds in vdses:; self._vdses[max(1, floor(log(vds.n_samples, self._branch_factor)))].append(vds); self._gvcfs = gvcfs; self._gvcf_sample_names = gvcf_sample_names; self._gvcf_external_header = gvcf_external_header; self._gvcf_import_intervals = gvcf_import_intervals; self._gvcf_info_to_keep = set(gvcf_info_to_keep) if gvcf_info_to_keep is not None else None; self._gvcf_reference_entry_fields_to_keep = (; set(gvcf_reference_entry_fields_to_keep) if gvcf_reference_entry_fields_to_keep is not None else None; ). self._uuid = uuid.uuid4(); self._job_id = 1; self.__intervals_cache = {}; self._gvcf_batch_size = gvcf_batch_size. @property; def gvcf_batch_size(self):; """"""The number of GVCFs to combine into a Variant Dataset at once.""""""; return self._gvcf_batch_size. @gvcf_batch_size.setter; def gvcf_batch_size(self, value: int):; if value * len(self._gvcf_import_intervals) > VariantDatasetCombiner._gvcf_merge_task_limit:; old_value = value; value = VariantDatasetCombiner._gvcf_merge_task_limit // len(self._gvcf_import_intervals);",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:16945,Testability,log,log,16945,"self._vdses[current_bin][self._branch_factor :]. remaining = self._branch_factor - len(files_to_merge); while self._num_vdses > 0 and remaining > 0:; current_bin = min(self._vdses); extra = self._vdses[current_bin][-remaining:]; if len(extra) == len(self._vdses[current_bin]):; del self._vdses[current_bin]; else:; self._vdses[current_bin] = self._vdses[current_bin][:-remaining]; files_to_merge = extra + files_to_merge; remaining = self._branch_factor - len(files_to_merge). new_n_samples = sum(f.n_samples for f in files_to_merge); info(f'VDS Combine (job {self._job_id}): merging {len(files_to_merge)} datasets with {new_n_samples} samples'). temp_path = self._temp_out_path(f'vds-combine_job{self._job_id}'); largest_vds = max(files_to_merge, key=lambda vds: vds.n_samples); vds = hl.vds.read_vds(; largest_vds.path,; _assert_reference_type=self._dataset_type.reference_type,; _assert_variant_type=self._dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ). interval_bin = floor(log(new_n_samples, self._branch_factor)); intervals = self.__intervals_cache.get(interval_bin). if intervals is None:; # we use the reference data since it generally has more rows than the variant data; intervals, _ = calculate_new_intervals(; vds.reference_data, self._target_records, os.path.join(temp_path, 'interval_checkpoint.ht'); ); self.__intervals_cache[interval_bin] = intervals. paths = [f.path for f in files_to_merge]; vdss = self._read_variant_datasets(paths, intervals); combined = combine_variant_datasets(vdss). if self.finished:; self._write_final(combined); return. new_path = os.path.join(temp_path, 'dataset.vds'); combined.write(new_path, overwrite=True, _codec_spec=FAST_CODEC_SPEC); new_bin = floor(log(new_n_samples, self._branch_factor)); # this ensures that we don't somehow stick a vds at the end of; # the same bin, ending up with a weird ordering issue; if new_bin <= original_bin:; new_bin = original_bin + 1; self._vdses[new_bin].append(VDSMetadata(path=new_path, n_sam",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:17668,Testability,log,log,17668,"b{self._job_id}'); largest_vds = max(files_to_merge, key=lambda vds: vds.n_samples); vds = hl.vds.read_vds(; largest_vds.path,; _assert_reference_type=self._dataset_type.reference_type,; _assert_variant_type=self._dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ). interval_bin = floor(log(new_n_samples, self._branch_factor)); intervals = self.__intervals_cache.get(interval_bin). if intervals is None:; # we use the reference data since it generally has more rows than the variant data; intervals, _ = calculate_new_intervals(; vds.reference_data, self._target_records, os.path.join(temp_path, 'interval_checkpoint.ht'); ); self.__intervals_cache[interval_bin] = intervals. paths = [f.path for f in files_to_merge]; vdss = self._read_variant_datasets(paths, intervals); combined = combine_variant_datasets(vdss). if self.finished:; self._write_final(combined); return. new_path = os.path.join(temp_path, 'dataset.vds'); combined.write(new_path, overwrite=True, _codec_spec=FAST_CODEC_SPEC); new_bin = floor(log(new_n_samples, self._branch_factor)); # this ensures that we don't somehow stick a vds at the end of; # the same bin, ending up with a weird ordering issue; if new_bin <= original_bin:; new_bin = original_bin + 1; self._vdses[new_bin].append(VDSMetadata(path=new_path, n_samples=new_n_samples)). def _step_gvcfs(self):; step = self._branch_factor; files_to_merge = self._gvcfs[: self._gvcf_batch_size * step]; self._gvcfs = self._gvcfs[self._gvcf_batch_size * step :]. info(; f'GVCF combine (job {self._job_id}): merging {len(files_to_merge)} GVCFs into '; f'{(len(files_to_merge) + step - 1) // step} datasets'; ). if self._gvcf_external_header is not None:; sample_names = self._gvcf_sample_names[: self._gvcf_batch_size * step]; self._gvcf_sample_names = self._gvcf_sample_names[self._gvcf_batch_size * step :]; else:; sample_names = None; header_file = self._gvcf_external_header or files_to_merge[0]; header_info = hl.eval(hl.get_vcf_header_info(header_file)); mer",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:22015,Testability,log,log,22015,"o_to_keep,; ),; ['locus'],; lambda k, v: k.annotate(data=v),; ),; globals=hl.struct(g=hl.literal(ids).map(lambda s: hl.struct(__cols=[hl.struct(s=s)]))),; ); variant_ht = combine(variant_ht); vds = VariantDataset(; reference_ht._unlocalize_entries('__entries', '__cols', ['s']),; variant_ht._unlocalize_entries('__entries', '__cols', ['s'])._key_rows_by_assert_sorted(; 'locus', 'alleles'; ),; ). merge_vds.append(vds); merge_n_samples.append(len(merging)); if self.finished and len(merge_vds) == 1:; self._write_final(merge_vds[0]); return. temp_path = self._temp_out_path(f'gvcf-combine_job{self._job_id}/dataset_'); pad = len(str(len(merge_vds) - 1)); merge_metadata = [; VDSMetadata(path=temp_path + str(count).rjust(pad, '0') + '.vds', n_samples=n_samples); for count, n_samples in enumerate(merge_n_samples); ]; paths = [md.path for md in merge_metadata]; hl.vds.write_variant_datasets(merge_vds, paths, overwrite=True, codec_spec=FAST_CODEC_SPEC); for md in merge_metadata:; self._vdses[max(1, floor(log(md.n_samples, self._branch_factor)))].append(md). def _temp_out_path(self, extra):; return os.path.join(self._temp_path, 'combiner-intermediates', f'{self._uuid}_{extra}'). def _read_variant_datasets(self, inputs: List[str], intervals: List[Interval]):; reference_type = self._dataset_type.reference_type; variant_type = self._dataset_type.variant_type; return [; hl.vds.read_vds(; path,; intervals=intervals,; _assert_reference_type=reference_type,; _assert_variant_type=variant_type,; _warn_no_ref_block_max_length=False,; ); for path in inputs; ]. [docs]def new_combiner(; *,; output_path: str,; temp_path: str,; save_path: Optional[str] = None,; gvcf_paths: Optional[List[str]] = None,; vds_paths: Optional[List[str]] = None,; vds_sample_counts: Optional[List[int]] = None,; intervals: Optional[List[Interval]] = None,; import_interval_size: Optional[int] = None,; use_genome_default_intervals: bool = False,; use_exome_default_intervals: bool = False,; gvcf_external_header: Optional[s",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:27993,Testability,assert,assert,27993," ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': require one argument from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals' to choose GVCF partitioning""; ). if n_partition_args > 1:; warning(; ""'new_combiner': multiple colliding arguments found from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals'.""; ""\n The argument found first in the list in this warning will be used, and others ignored.""; ). if intervals is not None:; pass; elif import_interval_size is not None:; intervals = calculate_even_genome_partitioning(reference_genome, import_interval_size); elif use_genome_default_intervals:; size = VariantDatasetCombiner.default_genome_interval_size; intervals = calculate_even_genome_partitioning(reference_genome, size); elif use_exome_default_intervals:; size = VariantDatasetCombiner.default_exome_interval_size; intervals = calculate_even_genome_partitioning(reference_genome, size); assert intervals is not None; else:; intervals = []. if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). # we need to compute the type that the combiner will have, this will allow us to read matrix; # tables quickly, especially in an asynchronous environment like query on batch where typing; # a read uses a blocking round trip.; vds = None; gvcf_type = None; if vds_paths:; # sync up gvcf_reference_entry_fields_to_keep and they reference entry types from the VDS; vds = hl.vds.read_vds(vds_paths[0], _warn_no_ref_block_max_length=False); vds_ref_entry = set(; name[1:] if name in ('LGT', 'LPGT') else name for name in vds.reference_data.entry if name != 'END'; ); if gvcf_reference_entry_fields_to_keep is not None and vds_ref_entry != gvcf_reference_entry_fields_to_keep:; warning(; ""Mismatch between 'gvcf_reference_entry_fields' to keep and VDS reference data ""; ""entry types. Overwriting with reference entry fields from suppli",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:12111,Usability,resume,resume,12111,"; return False; return True. @property; def finished(self) -> bool:; """"""Have all GVCFs and input Variant Datasets been combined?""""""; return not self._gvcfs and not self._vdses. [docs] def save(self):; """"""Save a :class:`.VariantDatasetCombiner` to its `save_path`.""""""; fs = hl.current_backend().fs; try:; backup_path = self._save_path + '.bak'; if fs.exists(self._save_path):; fs.copy(self._save_path, backup_path); with fs.open(self._save_path, 'w') as out:; json.dump(self, out, indent=2, cls=Encoder); if fs.exists(backup_path):; fs.remove(backup_path); except OSError as e:; # these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail; print(f'Failed saving {self.__class__.__name__} state at {self._save_path}'); print(f'An attempt was made to copy {self._save_path} to {backup_path}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:26168,Usability,resume,resume,26168,"alueError(; ""'gvcf_sample_names' and 'gvcf_paths' must have the same length ""; f'{len(gvcf_sample_names)} != {len(gvcf_paths)}'; ). if batch_size is None:; if gvcf_batch_size is None:; gvcf_batch_size = VariantDatasetCombiner._default_gvcf_batch_size; else:; pass; elif gvcf_batch_size is None:; warning(; 'The batch_size parameter is deprecated. '; 'The batch_size parameter will be removed in a future version of Hail. '; 'Please use gvcf_batch_size instead.'; ); gvcf_batch_size = batch_size; else:; raise ValueError(; 'Specify only one of batch_size and gvcf_batch_size. ' f'Received {batch_size} and {gvcf_batch_size}.'; ); del batch_size. def maybe_load_from_saved_path(save_path: str) -> Optional[VariantDatasetCombiner]:; if force:; return None; fs = hl.current_backend().fs; if fs.exists(save_path):; try:; combiner = load_combiner(save_path); warning(f'found existing combiner plan at {save_path}, using it'); # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); combiner._branch_factor = branch_factor; combiner._target_records = target_records; combiner._gvcf_batch_size = gvcf_batch_size; return combiner; except (ValueError, TypeError, OSError, KeyError) as e:; warning(; f'file exists at {save_path}, but it is not a valid combiner plan, overwriting\n'; f' caused by: {e}'; ); return None. # We do the first save_path check now after validating the arguments; if save_path is not None:; saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner. if len(gvcf_paths) > 0:; n_partition_args = (; int(intervals is not None); + int(import_interval_size is not None); + int(use_genome_default_intervals); + int(use_exome_default_intervals); ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': requ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:2709,Availability,error,error,2709,"le Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:2972,Availability,error,error,2972,"strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/result",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:5432,Availability,error,error,5432,"bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].exists(path). [docs]def is_file(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].is_file(path). [docs]def is_dir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].is_dir(path). [docs]def stat(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> FileListEntry:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return _fses[requester_pays_config].stat(path). [docs]def ls(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> List[FileListEntry]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.;",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:6123,Availability,error,error,6123,":`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].is_dir(path). [docs]def stat(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> FileListEntry:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return _fses[requester_pays_config].stat(path). [docs]def ls(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> List[FileListEntry]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return _fses[requester_pays_config].ls(path). [docs]def mkdir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Ensure files can be created whose dirname is `path`. Warning; -------. On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:8117,Deployability,update,updated,8117,"ot exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return _fses[requester_pays_config].ls(path). [docs]def mkdir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Ensure files can be created whose dirname is `path`. Warning; -------. On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. """"""; _fses[requester_pays_config].mkdir(path). [docs]def remove(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Removes the file at `path`. If the file does not exist, this function does; nothing. `path` must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].remove(path). [docs]def rmtree(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Recursively remove all files under the given `path`. On a local filesystem,; this removes the directory tree at `path`. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with `path`. As such,; `path` must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].rmtree(path).  Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:3877,Performance,load,load,3877,"ter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; _fses[requester_pays_config].copy(src, dest). [docs]def exists(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].exists(path). [docs]def is_file(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[r",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:1791,Security,access,access,1791,"CSRequesterPaysFSCache(fs_constructor=RouterFS). [docs]def open(; path: str,; mode: str = 'r',; buffer_size: int = 8192,; *,; requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None,; ) -> io.IOBase:; """"""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; ",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:3217,Security,access,access,3217,"lt2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters; ----------; src: :clas",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
https://hail.is/docs/batch/cookbook/clumping.html:1424,Availability,avail,available,1424,"w page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that take",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:5582,Availability,avail,available,5582,"ed=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, v",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:6590,Availability,down,downstream,6590,"ccess images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating fil",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:8654,Availability,avail,available,8654,"s the temporary file; root given to all files in the resource group ({root} when declaring the resource group). Clumping By Chromosome; The second function performs clumping for a given chromosome. The input arguments are the Batch; for which to create a new BashJob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machines memory. PLINKs memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results fi",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:8917,Availability,avail,available,8917,"ob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machines memory. PLINKs memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:9248,Availability,down,downstream,9248,"c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machines memory. PLINKs memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. Control Code; The last thing we want to do is use the functions we wrote above to create new jobs; on a Batch which can be executed with the ServiceBackend.; First, we import the Batch module as hb.; import hailtop.batch as hb. Next, we",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:2034,Deployability,install,installed,2034,"s 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the com",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:4946,Deployability,install,installed,4946,"linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.d",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:5229,Deployability,install,installed,5229,", P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute ",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:1851,Energy Efficiency,consumption,consumption,1851,", and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; ",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:11572,Energy Efficiency,consumption,consumption,11572," the entire Batch and are not outputs of a BashJob.; vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). We use the gwas function defined above to create a new job on the batch to; perform a GWAS that outputs a binary PLINK file and association results:; g = gwas(batch, vcf, phenotypes). We call the clump function once per chromosome and aggregate a list of the; clumping results files passing the outputs from the g job defined above; as inputs to the clump function:; results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). Finally, we use the merge function to concatenate the results into a single file; and then write this output to a permanent location using Batch.write_output().; The inputs to the merge function are the clumped output files from each of the clump; jobs.; m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). The last thing we do is submit the Batch to the service and then close the Backend:; batch.run(open=True, wait=False) # doctest: +SKIP; backend.close(). Synopsis; We provide the code used above in one place for your reference:. run_gwas.py; import argparse. import hail as hl. def run_gwas(vcf_file, phenotypes_file, output_file):; table = hl.import_table(phenotypes_file, impute=True).key_by('Sample'). hl.import_vcf(vcf_file).write('tmp.mt'); mt = hl.read_matrix_table('tmp.mt'). mt = mt.annotate_cols(pheno=table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = (; (mt.GT.is_hom_ref() & (ab <= 0.1)); | (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)); | (mt.GT.is_hom_var() & (ab >= 0.9)); ); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). eigenvalues, pcs",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:15309,Energy Efficiency,consumption,consumption,15309,"s/hail:0.2.37. COPY run_gwas.py /. batch_clumping.py; import hailtop.batch as hb. def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(; ofile={'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam', 'assoc': '{root}.assoc'}; ); g.command(f""""""; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; """"""); return g. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f""""""; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; """"""); return c. def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f""""""; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; """"""); return merger. if __name__ == '__main__':; backend = hb.ServiceBackend(); batch = hb.Batch(backend=backend, name='clumping'). vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). g = gwas(batch, vcf, phenotypes). results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). batch.run(open=True, wait=False); backend.close(). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:4832,Integrability,depend,dependencies,4832,"ant_qc.AF[1] > 0.01). eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). mt = mt.annotate_cols(scores=pcs[mt.s].scores). gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The followin",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:4987,Integrability,depend,depends,4987,"linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.d",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:5040,Integrability,depend,dependencies,5040,".isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:5122,Integrability,depend,dependencies,5122,".isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:1216,Modifiability,flexible,flexible,1216,"Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. Th",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:5243,Modifiability,extend,extend,5243,"as.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functio",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:488,Performance,perform,performing,488,". Clumping GWAS Results  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Introduction; Hail GWAS Script; Docker Image; Batch Script; Functions; Control Code. Synopsis. Random Forest. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:1234,Performance,perform,performing,1234,"Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. Th",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:1323,Performance,scalab,scalable,1323,"Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. Th",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:1623,Performance,scalab,scalable,1623," want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing t",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:1672,Performance,perform,perform,1672,"ent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:1821,Performance,perform,performs,1821,", and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and clump SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; ",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:2566,Performance,perform,performing,2566," while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, well use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, well write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, well build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, well write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument --cores. run_gwas.py; import argparse. import hail as hl. def run_gwas(vcf_file, phenotypes_file, output_file):; table = hl.import_table(phenotypes_file, impute=True).key_by('Sample'). hl.import_vcf(vcf_file).write('tmp.mt'); mt = hl.read_matrix_table('tmp.mt'). mt = mt.annotate_cols(pheno=table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD);",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:7847,Performance,perform,performs,7847,"file={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating files as a group with a common file root, we need to use the BashJob.declare_resource_group(); method. We then pass g.ofile as the output file root to run_gwas.py as that represents the temporary file; root given to all files in the resource group ({root} when declaring the resource group). Clumping By Chromosome; The second function performs clumping for a given chromosome. The input arguments are the Batch; for which to create a new BashJob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory becaus",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:9069,Performance,perform,performance,9069,"e chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machines memory. PLINKs memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. Control Code; The last thing we want to do is use the fun",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:10811,Performance,perform,perform,10811," 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. Control Code; The last thing we want to do is use the functions we wrote above to create new jobs; on a Batch which can be executed with the ServiceBackend.; First, we import the Batch module as hb.; import hailtop.batch as hb. Next, we create a Batch specifying the backend is the ServiceBackend; and give it the name clumping.; backend = hb.ServiceBackend(); batch = hb.Batch(backend=backend, name='clumping'). We create InputResourceFile objects for the VCF file and; phenotypes file using the Batch.read_input() method. These; are the inputs to the entire Batch and are not outputs of a BashJob.; vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). We use the gwas function defined above to create a new job on the batch to; perform a GWAS that outputs a binary PLINK file and association results:; g = gwas(batch, vcf, phenotypes). We call the clump function once per chromosome and aggregate a list of the; clumping results files passing the outputs from the g job defined above; as inputs to the clump function:; results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). Finally, we use the merge function to concatenate the results into a single file; and then write this output to a permanent location using Batch.write_output().; The inputs to the merge function are the clumped output files from each of the clump; jobs.; m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). The last thing we do is submit the Batch to the service and then close the Backend:; batch.run(open=True, wait=False) # doctest: +SKIP; backend.close(). Synopsis; We provide the code used above in one place for your reference:. ru",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:8927,Safety,detect,detection,8927,"ob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machines memory. PLINKs memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:5511,Security,access,access,5511,"ed=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:6081,Security,access,access,6081,"e of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:6527,Security,access,access,6527,"ccess images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your projects private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating fil",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:6676,Testability,test,test,6676,"oogle Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating files as a group with a common file root, we need to use the BashJob.declare_resource_group(); method. We then pass g.ofile as the output file root to ru",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/clumping.html:13644,Testability,test,test,13644,"es). gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. batch_clumping.py; import hailtop.batch as hb. def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(; ofile={'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam', 'assoc': '{root}.assoc'}; ); g.command(f""""""; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; """"""); return g. def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f""""""; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; """"""); return c. def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
https://hail.is/docs/batch/cookbook/random_forest.html:1391,Availability,checkpoint,checkpointing,1391,"rsion Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8061,Availability,resilien,resilient,8061,"to as_tsv is result and we assign the output to tsv_result.; Lastly in the for loop for each window, we append the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output(",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8120,Availability,checkpoint,checkpoint,8120," the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8435,Availability,checkpoint,checkpoint,8435,"sult = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.appen",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8523,Availability,checkpoint,checkpoints,8523," j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running j",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8714,Availability,checkpoint,checkpointed,8714," into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has a",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8778,Availability,checkpoint,checkpointed,8778," permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8895,Availability,checkpoint,checkpoint,8895,"backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:9000,Availability,checkpoint,checkpoint,9000,"backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:9093,Availability,checkpoint,checkpoint,9093,"; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file alread",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:9145,Availability,checkpoint,checkpoint,9145,"run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the resul",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:9181,Availability,checkpoint,checkpoint,9181,"cceeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:9414,Availability,checkpoint,checkpoint,9414,"; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indi",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:9617,Availability,checkpoint,checkpointing,9617,"e list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, c",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:9739,Availability,checkpoint,checkpointing,9739,"InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the rando",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:10088,Availability,checkpoint,checkpoint,10088,"ckpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.a",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:10350,Availability,checkpoint,checkpoint,10350," window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoi",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:10419,Availability,checkpoint,checkpoint,10419,"nt); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:10471,Availability,checkpoint,checkpoint,10471," we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:10507,Availability,checkpoint,checkpoint,10507,"we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:10577,Availability,checkpoint,checkpoint,10577,"s at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for you",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:11000,Availability,checkpoint,checkpoint,11000,".index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:11185,Availability,checkpoint,checkpoint,11185,"e; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_te",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:11299,Availability,checkpoint,checkpoint,11299,"st that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:14555,Availability,checkpoint,checkpoints,14555,"random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:14966,Availability,checkpoint,checkpoint,14966,"[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:15018,Availability,checkpoint,checkpoint,15018,"est; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index =",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:15054,Availability,checkpoint,checkpoint,15054,"RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[d",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:15287,Availability,checkpoint,checkpoint,15287," obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=Fals",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:16672,Availability,checkpoint,checkpoints,16672,"random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:17148,Availability,checkpoint,checkpoint,17148,"l=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:17200,Availability,checkpoint,checkpoint,17200,"l=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:17236,Availability,checkpoint,checkpoint,17236,"l=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:17306,Availability,checkpoint,checkpoint,17306,"l=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:17395,Availability,checkpoint,checkpoint,17395,"l=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:17580,Availability,checkpoint,checkpoint,17580,"l=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:4033,Deployability,install,installed,4033,"r=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.Se",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:4454,Deployability,install,installed,4454,"store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.ServiceBackend(). Second, we create a Batch and specify the default Python image to; use for Python jobs with default_python_image. image is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to.; b = hb.Batch(name='rf',; default_python_image=image). Next, we read the y dataframe locally in order to get the list of windows; to run. The file path containing the dataframe could be stored on the cloud.; Therefore, we use t",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:4590,Deployability,install,installed,4590,". Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.ServiceBackend(). Second, we create a Batch and specify the default Python image to; use for Python jobs with default_python_image. image is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to.; b = hb.Batch(name='rf',; default_python_image=image). Next, we read the y dataframe locally in order to get the list of windows; to run. The file path containing the dataframe could be stored on the cloud.; Therefore, we use the function hfs.open to read the data regardless; of where its located.; with hfs.open(df_y_path) as f:; local_df_y = pd.read",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8030,Deployability,pipeline,pipeline,8030,"to as_tsv is result and we assign the output to tsv_result.; Lastly in the for loop for each window, we append the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output(",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:8217,Deployability,pipeline,pipeline,8217," the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that havent already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results well append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesnt exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:11461,Deployability,pipeline,pipeline,11461,"ndow); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest o",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:2158,Modifiability,variab,variables,2158,"ity as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == w",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:2337,Modifiability,variab,variables,2337,"w of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:6683,Modifiability,variab,variable,6683,"Batch to localize the files as inputs to a Job. Therefore, we; use the method Batch.read_input() to tell Batch to localize these files; when they are referenced by a Job.; df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). We initialize a list to keep track of all of the output files to concatenate; later on.; results = []. We now have all of our inputs ready and can iterate through each window in the; y dataframe. For each window, we create a new PythonJob using the; method Batch.new_python_job(). We then use the method PythonJob.call(); to run the function random_forest. The inputs to random_forest are the Batch inputs; df_x_input and df_y_input as well as the window name. Notice that the first argument to; PythonJob.call() is the reference to the function to call (i.e random_forest and; not random_forest(). The rest of the arguments are the usual positional arguments and; key-word arguments to the function. Lastly, we assign the result of calling the function; to the variable result which is a PythonResult. A PythonResult; can be thought of as a Python object and used in subsequent calls to PythonJob.call().; Since the type of result is a tuple of (str, float, float), we need to convert the Python; tuple to a tab-delimited string that can later be concatenated. We use the as_tsv function; we wrote above to do so. The input to as_tsv is result and we assign the output to tsv_result.; Lastly in the for loop for each window, we append the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outp",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:1224,Performance,perform,perform,1224,"ckpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:588,Safety,predict,predict,588,". Random Forest Model  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Rand",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:1158,Safety,predict,predict,1158,"ks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file conta",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:3414,Safety,predict,predict,3414,"rite a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the cor",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:12479,Safety,predict,predict,12479,"m forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; j = b.new_python_job(); result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:14261,Safety,predict,predict,14261,"ncatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_o",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:16378,Safety,predict,predict,16378,", output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for wi",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:2984,Testability,test,testing,2984,"om Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your compute",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:3388,Testability,test,testing,3388,"and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image()",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:12022,Testability,test,testing,12022,"random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now weve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). resu",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:12453,Testability,test,testing,12453,"y to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; j = b.new_python_job(); result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint.py; from typing import Tuple. import pandas as pd; from sklearn",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:13804,Testability,test,testing,13804,"h(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; j = b.new_python_job(); result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, ind",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:14235,Testability,test,testing,14235,"end(tsv_result.as_str()). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:15921,Testability,test,testing,15921,"ocal_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, ind",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/cookbook/random_forest.html:16352,Testability,test,testing,16352,"enate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, ",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html:983,Modifiability,variab,variable,983,". LocalBackend  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; LocalBackend; LocalBackend. ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; LocalBackend. View page source. LocalBackend. class hailtop.batch.backend.LocalBackend(tmp_dir='/tmp/', gsa_key_file=None, extra_docker_run_flags=None); Bases: Backend[None]; Backend that executes batches on a local computer.; .. rubric:: Examples; >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters:. tmp_dir (str)  Temporary directory to use.; gsa_key_file (Optional[str])  Mount a file with a gsa key to /gsa-key/key.json. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable HAIL_BATCH_GSA_KEY_FILE.; extra_docker_run_flags (Optional[str])  Additional flags to pass to docker run. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS. Methods. _async_run; Execute a batch. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(). Parameters:. batch (Batch)  Batch to execute.; dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files. Return type:; None. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html:1205,Modifiability,variab,variable,1205,". LocalBackend  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; LocalBackend; LocalBackend. ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; LocalBackend. View page source. LocalBackend. class hailtop.batch.backend.LocalBackend(tmp_dir='/tmp/', gsa_key_file=None, extra_docker_run_flags=None); Bases: Backend[None]; Backend that executes batches on a local computer.; .. rubric:: Examples; >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters:. tmp_dir (str)  Temporary directory to use.; gsa_key_file (Optional[str])  Mount a file with a gsa key to /gsa-key/key.json. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable HAIL_BATCH_GSA_KEY_FILE.; extra_docker_run_flags (Optional[str])  Additional flags to pass to docker run. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS. Methods. _async_run; Execute a batch. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(). Parameters:. batch (Batch)  Batch to execute.; dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files. Return type:; None. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html:606,Energy Efficiency,monitor,monitor,606,". RunningBatchType  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; RunningBatchType. Backend; LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; RunningBatchType. View page source. RunningBatchType. class hailtop.batch.backend.RunningBatchType; The type of value returned by Backend._run(). The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; alias of TypeVar(RunningBatchType). Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1209,Availability,echo,echo,1209,"nd; LocalBackend; ServiceBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hails Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1478,Availability,echo,echo,1478,"ckend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hails Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1864,Availability,echo,echo,1864," and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:;",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4470,Availability,avail,available,4470,"a will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word arguments. Parameters:. batch (Batch)  Batch to execute.; dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1330,Deployability,configurat,configuration,1330,"nced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hails Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-acc",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1740,Deployability,configurat,configuration,1740,"ils Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS envir",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2580,Deployability,configurat,configuration,2580,"r gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration i",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2981,Deployability,configurat,configuration,2981,"my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[s",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4297,Deployability,configurat,configuration,4297,"onal[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word argumen",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1330,Modifiability,config,configuration,1330,"nced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hails Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-acc",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1521,Modifiability,config,config,1521," regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hails Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a config",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1582,Modifiability,config,config,1582," regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hails Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a config",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1740,Modifiability,config,configuration,1740,"ils Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named my-billing-account; and stores temporary intermediate files in gs://my-bucket/temporary-files.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS envir",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1907,Modifiability,config,config,1907,"e_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuratio",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1968,Modifiability,config,config,1968,"e_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuratio",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2040,Modifiability,config,config,2040,"e_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuratio",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2580,Modifiability,config,configuration,2580,"r gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration i",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2609,Modifiability,config,config,2609,"r gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration i",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2735,Modifiability,variab,variable,2735,"onfiguration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, config",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2981,Modifiability,config,configuration,2981,"my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[s",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:3010,Modifiability,config,config,3010,"my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; https://my-account.blob.core.windows.net/my-container/tempdir.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[s",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:3736,Modifiability,config,configure,3736,"nment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indi",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:3864,Modifiability,config,configure,3864,"nd(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A specia",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4297,Modifiability,config,configuration,4297,"onal[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word argumen",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4312,Modifiability,variab,variable,4312,"onal[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word argumen",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4372,Modifiability,variab,variables,4372,". Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word arguments. Parameters:. batch (Batch)  Batch to execute.; dry_run (bool)  If True, dont execute code.",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4016,Security,authoriz,authorization,4016,"h/regions us-central1,us-east1. Allow reading or writing to buckets even though they are cold storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str])  Name of billing project to use.; bucket (Optional[str])  This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str])  Temporary data will be stored in this cloud storage folder.; google_project (Optional[str])  This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional)  If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str])  The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]])  Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:5726,Usability,progress bar,progress bar,5726,"r None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]])  A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use cold storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word arguments. Parameters:. batch (Batch)  Batch to execute.; dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files.; wait (bool)  If True, wait for the batch to finish executing before returning.; open (bool)  If True, open the UI page for the batch.; disable_progress_bar (bool)  If True, disable the progress bar.; callback (Optional[str])  If not None, a URL that will receive at most one POST request; after the entire batch completes.; token (Optional[str])  If not None, a string used for idempotency of batch submission. Return type:; Optional[Batch]. static supported_regions(); Get the supported cloud regions; Examples; >>> regions = ServiceBackend.supported_regions(). Returns:; A list of the supported cloud regions. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:977,Availability,echo,echo,977,". Batch  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Batch. Job; BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Batch. View page source. Batch. class hailtop.batch.batch.Batch(name=None, backend=None, attributes=None, requester_pays_project=None, default_image=None, default_memory=None, default_cpu=None, default_storage=None, default_regions=None, default_timeout=None, default_shell=None, default_python_image=None, default_spot=None, project=None, cancel_after_n_failures=None); Bases: object; Object representing the distributed acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints hello:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respective",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:4542,Availability,failure,failures,4542,"t_python_image (Optional[str])  Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool])  If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str])  DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int])  Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. new_bash_job; Initialize a BashJob object with default memory, storage, image, and CPU settings (defined in Batch) upon batch creation. new_job; Alias for Batch.new_bash_job(). new_python_job; Initialize a new PythonJob object with default Python image, memory, storage, and CPU settings (defined in Batch) upon batch creation. read_input; Create a new input resource file object representing a single file. read_input_group; Create a new resource group representing a mapping of identifier to input resource files. run; Execute a batch. select_jobs; Select all jobs in the batch whose name matches pattern. write_output; Write resource file or resource file group to an output destination. static from_batch_id(batch_id, *args, **kwargs); Crea",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:4626,Availability,failure,failures,4626,"the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool])  If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str])  DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int])  Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. new_bash_job; Initialize a BashJob object with default memory, storage, image, and CPU settings (defined in Batch) upon batch creation. new_job; Alias for Batch.new_bash_job(). new_python_job; Initialize a new PythonJob object with default Python image, memory, storage, and CPU settings (defined in Batch) upon batch creation. read_input; Create a new input resource file object representing a single file. read_input_group; Create a new resource group representing a mapping of identifier to input resource files. run; Execute a batch. select_jobs; Select all jobs in the batch whose name matches pattern. write_output; Write resource file or resource file group to an output destination. static from_batch_id(batch_id, *args, **kwargs); Create a Batch from an existing batch id.; Notes; Can only be used with the ServiceBackend.;",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:6205,Availability,echo,echo,6205," group representing a mapping of identifier to input resource files. run; Execute a batch. select_jobs; Select all jobs in the batch whose name matches pattern. write_output; Write resource file or resource file group to an output destination. static from_batch_id(batch_id, *args, **kwargs); Create a Batch from an existing batch id.; Notes; Can only be used with the ServiceBackend.; Examples; Create a batch object from an existing batch id:; >>> b = Batch.from_batch_id(1) . Parameters:; batch_id (int)  ID of an existing Batch. Return type:; Batch. Returns:; A Batch object that can append jobs to an existing batch. new_bash_job(name=None, attributes=None, shell=None); Initialize a BashJob object with default memory, storage,; image, and CPU settings (defined in Batch) upon batch creation.; Examples; Create and execute a batch b with one job j that prints hello world:; >>> b = Batch(); >>> j = b.new_bash_job(name='hello', attributes={'language': 'english'}); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; BashJob. new_job(name=None, attributes=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints hello alice:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Ba",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:10504,Availability,echo,echo,10504,"foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group rg, you can use the attribute notation; rg.identifier or the get item notation rg[identifier].; The file extensions for each file are derived from the identifier. This; is equivalent to {root}.identifier from; BashJob.declare_resource_group(). We are planning on adding; flexibility to incorporate more complicated extensions in the future; such as .vcf.bgz. For now, use JobResourceFile.add_extension(); to add an extension to a resource file. Parameters:; kwargs (Union[str, PathLike])  Key word arguments where the name/key is the identifier and the value; is the file path. Return type:; ResourceGroup. run(dry_run=False, verbose=False, delete_scratch_on_exit=True, **backend_kwargs); Execute a batch.; Examples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11389,Availability,echo,echo,11389,"ples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource)  Resource to be written to a file.; dest (str)  Destination file path. For a single ResourceFile, this will; simply be des",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11586,Availability,echo,echo,11586,"e code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource)  Resource to be written to a file.; dest (str)  Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11791,Availability,echo,echo,11791,"True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource)  Resource to be written to a file.; dest (str)  Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1869,Deployability,configurat,configurations,1869,"llo:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the Local",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:2037,Deployability,configurat,configurations,2037,"is batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None])  Storage setting to use b",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:3750,Deployability,install,installed,3750,"; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None])  Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]])  Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See ServiceBackend for details.; default_timeout (Union[int, float, None])  Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str])  Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool])  If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str])  DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int])  Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. ne",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:3796,Deployability,install,installed,3796,"; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None])  Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]])  Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See ServiceBackend for details.; default_timeout (Union[int, float, None])  Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str])  Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool])  If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str])  DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int])  Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. ne",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:7268,Deployability,install,installed,7268,"rs:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; BashJob. new_job(name=None, attributes=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints hello alice:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. E",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:7529,Deployability,install,installed,7529,"s=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints hello alice:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str)  File path to read. Return type:; InputResourceFile. rea",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8139,Energy Efficiency,charge,charges,8139,"ult_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str)  File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and its index (file extensions matter!):; >>> fasta =",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8643,Energy Efficiency,charge,charges,8643,"eBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str)  File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and its index (file extensions matter!):; >>> fasta = b.read_input_group(**{'fasta': 'data/example.fasta',; ... 'fasta.idx': 'data/example.fasta.idx'}). Create a resource group where the identifiers dont match the file extensions:; >>> rg = b.read_input_group(foo='data/foo.txt',; ... bar='data/bar.txt'). rg.foo and rg.bar will not have the .txt file extension and; instead will be {root}.foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group r",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11979,Energy Efficiency,charge,charges,11979,"True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource)  Resource to be written to a file.; dest (str)  Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1785,Modifiability,variab,variable,1785," acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints hello:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1832,Modifiability,config,config,1832," acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints hello:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1839,Modifiability,variab,variable,1839," acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints hello:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1869,Modifiability,config,configurations,1869,"llo:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the Local",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:2037,Modifiability,config,configurations,2037,"is batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None])  Storage setting to use b",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:3496,Safety,timeout,timeout,3496," any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None])  Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]])  Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See ServiceBackend for details.; default_timeout (Union[int, float, None])  Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str])  Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool])  If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str])  DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Opt",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8116,Safety,avoid,avoid,8116,"ult_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str)  File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and its index (file extensions matter!):; >>> fasta =",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8620,Safety,avoid,avoid,8620,"eBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str])  Name of the job.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str)  File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and its index (file extensions matter!):; >>> fasta = b.read_input_group(**{'fasta': 'data/example.fasta',; ... 'fasta.idx': 'data/example.fasta.idx'}). Create a resource group where the identifiers dont match the file extensions:; >>> rg = b.read_input_group(foo='data/foo.txt',; ... bar='data/bar.txt'). rg.foo and rg.bar will not have the .txt file extension and; instead will be {root}.foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group r",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11956,Safety,avoid,avoid,11956,"True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource)  Resource to be written to a file.; dest (str)  Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:2328,Security,access,accessing,2328,"t already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str])  Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None])  Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]])  Key-value pairs of additional attributes. name is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str])  The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str])  Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None])  Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None])  CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None])  Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]])  Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See S",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:4298,Security,authenticat,authenticating,4298,"ee ServiceBackend for details.; default_timeout (Union[int, float, None])  Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str])  Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool])  If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str])  DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int])  Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. new_bash_job; Initialize a BashJob object with default memory, storage, image, and CPU settings (defined in Batch) upon batch creation. new_job; Alias for Batch.new_bash_job(). new_python_job; Initialize a new PythonJob object with default Python image, memory, storage, and CPU settings (defined in Batch) upon batch creation. read_input; Create a new input resource file object representing a single file. read_input_group; Create a new resource group representing a mapping of identifier to input resource files. run; Execute ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11063,Testability,assert,assert,11063,"eFile.add_extension(); to add an extension to a resource file. Parameters:; kwargs (Union[str, PathLike])  Key word arguments where the name/key is the identifier and the value; is the file path. Return type:; ResourceGroup. run(dry_run=False, verbose=False, delete_scratch_on_exit=True, **backend_kwargs); Execute a batch.; Examples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in w",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:10408,Usability,simpl,simple,10408,"foo and rg.bar will not have the .txt file extension and; instead will be {root}.foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group rg, you can use the attribute notation; rg.identifier or the get item notation rg[identifier].; The file extensions for each file are derived from the identifier. This; is equivalent to {root}.identifier from; BashJob.declare_resource_group(). We are planning on adding; flexibility to incorporate more complicated extensions in the future; such as .vcf.bgz. For now, use JobResourceFile.add_extension(); to add an extension to a resource file. Parameters:; kwargs (Union[str, PathLike])  Key word arguments where the name/key is the identifier and the value; is the file path. Return type:; ResourceGroup. run(dry_run=False, verbose=False, delete_scratch_on_exit=True, **backend_kwargs); Execute a batch.; Examples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool)  If True, dont execute code.; verbose (bool)  If True, print debugging output.; delete_scratch_on_exit (bool)  If True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_outp",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:12380,Usability,simpl,simply,12380,"True, delete temporary directories with intermediate files.; backend_kwargs (Any)  See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str)  Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource)  Resource to be written to a file.; dest (str)  Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:707,Availability,echo,echo,707,". BashJob  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; BashJob. PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BashJob. View page source. BashJob. class hailtop.batch.job.BashJob(batch, token, *, name=None, attributes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the jobs command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'e",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1284,Availability,echo,echo,1284,"Python Version Compatibility Policy; Change Log. Batch. Python API; BashJob. View page source. BashJob. class hailtop.batch.job.BashJob(batch, token, *, name=None, attributes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the jobs command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. Its behavior is to append; commands to run to the set of previously defined commands r",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1453,Availability,echo,echo,1453,"butes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the jobs command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. Its behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1644,Availability,echo,echo,1644,"porary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the jobs command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. Its behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[identifier]. If an object for that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1914,Availability,echo,echo,1914,"y the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the jobs command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. Its behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[identifier]. If an object for that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1957,Availability,echo,echo,1957,"ew_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the jobs command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. Its behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[identifier]. If an object for that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:2000,Availability,echo,echo,2000," Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the jobs command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. Its behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[identifier]. If an object for that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str)  A bash command. Return ty",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:2941,Availability,error,error,2941," {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. Its behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[identifier]. If an object for that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str)  A bash command. Return type:; BashJob. Returns:; Same job object with command appended. declare_resource_group(**mappings); Declare a resource group for a job.; Examples; Declare a resource group:; >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() . Warning; Be careful when specifying the expressions for each file as this is Python; code that is executed with eval!. Parameters:; mappings (Dict[str, Any])  Keywords (in the above example tmp1) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python eval. To use the; keyword as the file name, use {root} (",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:4244,Availability,echo,echo,4244,"r that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str)  A bash command. Return type:; BashJob. Returns:; Same job object with command appended. declare_resource_group(**mappings); Declare a resource group for a job.; Examples; Declare a resource group:; >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() . Warning; Be careful when specifying the expressions for each file as this is Python; code that is executed with eval!. Parameters:; mappings (Dict[str, Any])  Keywords (in the above example tmp1) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python eval. To use the; keyword as the file name, use {root} (in the above example {root}; will be replaced with tmp1). Return type:; BashJob. Returns:; Same job object with resource groups set. image(image); Set the jobs docker image.; Examples; Set the jobs docker image to ubuntu:22.04:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.image('ubuntu:22.04'); ... .command(f'echo ""hello""')); >>> b.run() . Parameters:; image (str)  Docker image to use. Return type:; BashJob. Returns:; Same job object with docker image set. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:3442,Testability,log,log,3442,"he get item syntax of; job[identifier]. If an object for that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str)  A bash command. Return type:; BashJob. Returns:; Same job object with command appended. declare_resource_group(**mappings); Declare a resource group for a job.; Examples; Declare a resource group:; >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() . Warning; Be careful when specifying the expressions for each file as this is Python; code that is executed with eval!. Parameters:; mappings (Dict[str, Any])  Keywords (in the above example tmp1) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python eval. To use the; keyword as the file name, use {root} (in the above example {root}; will be replaced with tmp1). Return type:; BashJob. Returns:; Same job object with resource groups set. image(image); Set the jobs docker image.; Examples; Set the jobs docker image to ubuntu:22.04:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.image('ubuntu:22.04'); ... .command(f'echo ""hello""')); >>> b.run() . Parameters:; image (str)  Docker image to use. Return type:; BashJob. Returns:; Same job object with docker image set. Previous; Next .  Copyright 2024, Hail Team. Built w",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:3456,Testability,log,log,3456,"tax of; job[identifier]. If an object for that identifier doesnt exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str)  A bash command. Return type:; BashJob. Returns:; Same job object with command appended. declare_resource_group(**mappings); Declare a resource group for a job.; Examples; Declare a resource group:; >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() . Warning; Be careful when specifying the expressions for each file as this is Python; code that is executed with eval!. Parameters:; mappings (Dict[str, Any])  Keywords (in the above example tmp1) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python eval. To use the; keyword as the file name, use {root} (in the above example {root}; will be replaced with tmp1). Return type:; BashJob. Returns:; Same job object with resource groups set. image(image); Set the jobs docker image.; Examples; Set the jobs docker image to ubuntu:22.04:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.image('ubuntu:22.04'); ... .command(f'echo ""hello""')); >>> b.run() . Parameters:; image (str)  Docker image to use. Return type:; BashJob. Returns:; Same job object with docker image set. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx usin",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1647,Availability,echo,echo,1647,"hon_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Clou",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2157,Availability,echo,echo,2157,"job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str)  Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:3887,Availability,echo,echo,3887,")); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str)  Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str)  The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool)  If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, moun",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4321,Availability,echo,echo,4321,"orage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ...",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4445,Availability,echo,echo,4445,"sfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:5848,Availability,echo,echo,5848,"(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The path at which the bucket should be mounted to in the Docker; container.; read_only  If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the jobs memory requirements.; Examples; Set the jobs memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; E",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7030,Availability,echo,echo,7030,"fix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7200,Availability,echo,echo,7200,"y 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .co",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7346,Availability,avail,available,7346,"lt value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional su",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7431,Availability,avail,available,7431,"s an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7886,Availability,echo,echo,7886,"n us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:8211,Availability,echo,echo,8211,"Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the Servi",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9563,Availability,echo,echo,9563,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:3720,Energy Efficiency,power,power,3720,"est')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str)  Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str)  The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool)  If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be exp",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6817,Energy Efficiency,charge,charges,6817,"ry('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:823,Integrability,depend,dependencies,823,". Job  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; Job. BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1006,Integrability,depend,dependencies,1006,"  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; Job. BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to alwa",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1962,Integrability,depend,dependencies,1962,"re. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4180,Integrability,depend,dependencies,4180,"(str)  The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool)  If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4367,Integrability,depend,depends,4367,"f. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Pa",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4685,Integrability,depend,dependency,4685," is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The path at which the bucket should be mounted to in the Docker; container.; read_only  If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. mem",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4771,Integrability,depend,depend,4771,"6.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The path at which the bucket should be mounted to in the Docker; container.; read_only  If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the jobs memory requirements.; Examples; Set the jobs memory requ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4833,Integrability,depend,dependencies,4833,"ch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The path at which the bucket should be mounted to in the Docker; container.; read_only  If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the jobs memory requirements.; Examples; Set the jobs memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4855,Modifiability,variab,variable,4855,"u('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None])  Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The path at which the bucket should be mounted to in the Docker; container.; read_only  If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the jobs memory requirements.; Examples; Set the jobs memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2565,Performance,perform,performance,2565," = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str)  Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str)  The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool)  If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where th",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:5140,Performance,perform,performance,5140,"(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The path at which the bucket should be mounted to in the Docker; container.; read_only  If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the jobs memory requirements.; Examples; Set the jobs memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid a",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6844,Performance,latency,latency,6844,"ry('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1263,Safety,timeout,timeout,1263,"ompatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6804,Safety,avoid,avoid,6804,"ry('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9300,Safety,timeout,timeout,9300,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9308,Safety,timeout,timeout,9308,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9535,Safety,timeout,timeout,9535,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9593,Safety,timeout,timeout,9593,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9723,Safety,timeout,timeout,9723,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9789,Safety,timeout,timeout,9789,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1573,Testability,test,test,1573,"tly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performanc",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2091,Testability,test,test,2091,"Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str)  Name of the google storage bucket to mount or the",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2710,Testability,test,test,2710,"utput (bool)  If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str)  Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str)  The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool)  If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores mu",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2888,Testability,test,test,2888,"output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool)  If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str)  Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str)  The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool)  If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the jobs CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the jobs CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'ech",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:5248,Testability,test,test,5248," Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job)  Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket  Name of the google storage bucket to mount.; mount_point  The path at which the bucket should be mounted to in the Docker; container.; read_only  If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the jobs memory requirements.; Examples; Set the jobs memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approxim",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6952,Testability,test,test,6952,"lid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values lowmem, standard,; and highmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job wil",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7133,Testability,test,test,7133,"hmem are also valid arguments. lowmem corresponds to; approximately 1 Gi/core, standard corresponds to approximately; 4 Gi/core, and highmem corresponds to approximately 7 Gi/core.; The default value is standard. Parameters:; memory (Union[str, int, None])  Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (standard). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7813,Testability,test,test,7813,"ss charges as well as improve latency.; Examples; Require the job to run in us-central1:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]])  The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFil",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9498,Testability,test,test,9498,"pot (bool)  If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the jobs storage size.; Examples; Set the jobs disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None])  Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None])  Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:2604,Availability,down,downstream,2604,"lds[1]),; 'add': int(fields[2]),; 'mult': int(fields[3])}; data.append(d); return json.dumps(data). # Get all the multiplication and addition table results. b = Batch(name='add-mult-table'). formatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:4018,Availability,error,error,4018,"s passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable)  A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any])  Positional arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object.; kwargs (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tu",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:3673,Deployability,install,installed,3673,"her bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable)  A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, R",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:3796,Deployability,install,installed,3796,"ly detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable)  A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any])  Positional argum",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:5780,Deployability,install,installed,5780,"ult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any])  Positional arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object.; kwargs (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any])  Key-word arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object. Return type:; PythonResult. Returns:; resource.PythonResult. image(image); Set the jobs docker image.; Notes; image must already exist and have the same version of Python as what is; being used on the computer submitting the Batch. It also must have the; dill Python package installed. You can use the function docker.build_python_image(); to build a new image containing dill and additional Python packages.; Examples; Set the jobs docker image to hailgenetics/python-dill:3.9-slim:; >>> b = Batch(); >>> j = b.new_python_job(); >>> (j.image('hailgenetics/python-dill:3.9-slim'); ... .call(print, 'hello')); >>> b.run() . Parameters:; image (str)  Docker image to use. Return type:; PythonJob. Returns:; Same job object with docker image set. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:2791,Integrability,depend,dependencies,2791,"rmatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that i",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:2783,Safety,detect,detects,2783,"rmatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that i",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:810,Availability,avail,available,810,". BatchPoolExecutor  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:3283,Availability,down,down,3283," backend (Optional[ServiceBackend])  Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str])  The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None])  The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameters value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool)  If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compatible version of map(). Return type:; AsyncGenerator[int, None]. async async_submit(unapplied, *args, **kwargs); Aysncio compatible version of BatchPoolExecutor.submit(). Return type:; BatchPoolFuture. map(fn, *iterables, timeout=None, chunksize=1); Call fn on cloud machines with arguments from iterables.; This function returns a ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2479,Deployability,install,installed,2479,"ior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str])  A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend])  Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str])  The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None])  The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameters value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool)  If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2548,Deployability,install,installed,2548," creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str])  A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend])  Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str])  The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None])  The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameters value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool)  If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2646,Deployability,install,installed,2646,"d. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str])  A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend])  Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str])  The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None])  The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameters value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool)  If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map;",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2732,Energy Efficiency,allocate,allocate,2732,"achine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str])  A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend])  Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str])  The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None])  The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameters value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool)  If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to b",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5722,Energy Efficiency,schedul,schedule,5722,"])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable)  The function to execute.; iterables (Iterable[Any])  The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None])  This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generators; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int)  The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool)  If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the under",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5933,Energy Efficiency,reduce,reduce,5933,"(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable)  The function to execute.; iterables (Iterable[Any])  The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None])  This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generators; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int)  The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool)  If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the default_image argument to BatchPoolExecutor; This function does not return the functions output, it returns a; BatchPoolFuture whose BatchPoolFuture.result() method; can be used to access ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2886,Modifiability,variab,variables,2886,". map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str])  A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend])  Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str])  The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None])  The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameters value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool)  If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compati",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:686,Performance,concurren,concurrent,686,". BatchPoolExecutor  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:730,Performance,concurren,concurrent,730,". BatchPoolExecutor  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:1633,Safety,safe,safely,1633,"xecutes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str])  A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend])  Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str])  The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:3841,Safety,timeout,timeout,3841,"d to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool)  If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compatible version of map(). Return type:; AsyncGenerator[int, None]. async async_submit(unapplied, *args, **kwargs); Aysncio compatible version of BatchPoolExecutor.submit(). Return type:; BatchPoolFuture. map(fn, *iterables, timeout=None, chunksize=1); Call fn on cloud machines with arguments from iterables.; This function returns a generator which will produce each result in the; same order as the iterables, only blocking if the result is not yet; ready. You can convert the generator to a list with list.; Examples; Do nothing, but on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.ran",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:4106,Safety,timeout,timeout,4106," syntax.; cleanup_bucket (bool)  If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str])  DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compatible version of map(). Return type:; AsyncGenerator[int, None]. async async_submit(unapplied, *args, **kwargs); Aysncio compatible version of BatchPoolExecutor.submit(). Return type:; BatchPoolFuture. map(fn, *iterables, timeout=None, chunksize=1); Call fn on cloud machines with arguments from iterables.; This function returns a generator which will produce each result in the; same order as the iterables, only blocking if the result is not yet; ready. You can convert the generator to a list with list.; Examples; Do nothing, but on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5431,Safety,timeout,timeout,5431,"(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable)  The function to execute.; iterables (Iterable[Any])  The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None])  This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generators; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int)  The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool)  If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The functi",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5485,Safety,timeout,timeout,5485,"(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable)  The function to execute.; iterables (Iterable[Any])  The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None])  This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generators; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int)  The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool)  If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The functi",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5671,Safety,timeout,timeout,5671,"white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable)  The function to execute.; iterables (Iterable[Any])  The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None])  This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generators; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int)  The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool)  If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Py",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:6948,Security,access,access,6948,"parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool)  If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the default_image argument to BatchPoolExecutor; This function does not return the functions output, it returns a; BatchPoolFuture whose BatchPoolFuture.result() method; can be used to access the value.; Examples; Do nothing, but on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(lambda x: x, 4); ... future.result(); 4. Call a function with two arguments and one keyword argument, on the; cloud:; >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(lambda x, y, z: x + y + z,; ... ""poly"", ""ethyl"", z=""ene""); ... future.result(); ""polyethylene"". Generate a product of two random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(random_product, 1); ... future.result(); [23.325755364428026]. Parameters:. fn (Callable)  The function to execute.; args (Any)  Arguments for the funciton.; kwargs (Any)  Keyword arguments for the function. Return type:; BatchPoolFuture. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; prov",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1575,Availability,error,error,1575,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:2289,Availability,error,error,2289,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1360,Performance,concurren,concurrent,1360," Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This fu",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1453,Performance,concurren,concurrent,1453,"ool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:2082,Performance,concurren,concurrent,2082,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:2167,Performance,concurren,concurrent,2167,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1243,Safety,timeout,timeout,1243,"iguration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1499,Safety,timeout,timeout,1499,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1567,Safety,timeout,timeout,1567,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1897,Safety,timeout,timeout,1897,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1978,Safety,timeout,timeout,1978,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:2213,Safety,timeout,timeout,2213,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:2281,Safety,timeout,timeout,2281,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None])  Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html:745,Availability,echo,echo,745,". JobResourceFile  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; JobResourceFile. ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; JobResourceFile. View page source. JobResourceFile. class hailtop.batch.resource.JobResourceFile(value, source); Bases: ResourceFile; Class representing an intermediate file from a job.; Examples; j.ofile is a JobResourceFile on the job`j`:; >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; All JobResourceFile are temporary files and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved.; Methods. add_extension; Specify the file extension to use. source. rtype:; Job. add_extension(extension); Specify the file extension to use.; Examples; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; The default file name for a JobResourceFile is the name; of the identifier. Parameters:; extension (str)  File extension to use. Return type:; JobResourceFile. Returns:; JobResourceFile  Same resource file with the extension specified. source(). Return type:; Job. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html
https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html:1153,Availability,echo,echo,1153,". JobResourceFile  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; JobResourceFile. ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; JobResourceFile. View page source. JobResourceFile. class hailtop.batch.resource.JobResourceFile(value, source); Bases: ResourceFile; Class representing an intermediate file from a job.; Examples; j.ofile is a JobResourceFile on the job`j`:; >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; All JobResourceFile are temporary files and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved.; Methods. add_extension; Specify the file extension to use. source. rtype:; Job. add_extension(extension); Specify the file extension to use.; Examples; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; The default file name for a JobResourceFile is the name; of the identifier. Parameters:; extension (str)  File extension to use. Return type:; JobResourceFile. Returns:; JobResourceFile  Same resource file with the extension specified. source(). Return type:; Job. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html
https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html:783,Deployability,install,installed,783,". hailtop.batch.docker.build_python_image  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; build_python_image(). hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.docker.build_python_image. View page source. hailtop.batch.docker.build_python_image. hailtop.batch.docker.build_python_image(fullname, requirements=None, python_version=None, _tmp_dir='/tmp', *, show_docker_output=False); Build a new Python image with dill and the specified pip packages installed.; Notes; This function is used to build Python images for PythonJob.; Examples; >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) . Parameters:. fullname (str)  Full name of where to build the image including any repository prefix and tags; if desired (default tag is latest).; requirements (Optional[List[str]])  List of pip packages to install.; python_version (Optional[str])  String in the format of major_version.minor_version (ex: 3.9). Defaults to; current version of Python that is running.; _tmp_dir (str)  Location to place local temporary files used while building the image.; show_docker_output (bool)  Print the output from Docker when building / pushing the image. Return type:; str. Returns:; Full name where built image is located. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html
https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html:1200,Deployability,install,install,1200,". hailtop.batch.docker.build_python_image  Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; build_python_image(). hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.docker.build_python_image. View page source. hailtop.batch.docker.build_python_image. hailtop.batch.docker.build_python_image(fullname, requirements=None, python_version=None, _tmp_dir='/tmp', *, show_docker_output=False); Build a new Python image with dill and the specified pip packages installed.; Notes; This function is used to build Python images for PythonJob.; Examples; >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) . Parameters:. fullname (str)  Full name of where to build the image including any repository prefix and tags; if desired (default tag is latest).; requirements (Optional[List[str]])  List of pip packages to install.; python_version (Optional[str])  String in the format of major_version.minor_version (ex: 3.9). Defaults to; current version of Python that is running.; _tmp_dir (str)  Location to place local temporary files used while building the image.; show_docker_output (bool)  Print the output from Docker when building / pushing the image. Return type:; str. Returns:; Full name where built image is located. Previous; Next .  Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html
