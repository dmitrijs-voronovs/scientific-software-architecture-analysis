id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/4033#issuecomment-408982088:103,Integrability,message,message,103,Thank @jbloom22 ! Hadn't seen that this had change (was `bool` back in the days). And indeed the error message did not put me on the right track :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4033#issuecomment-408982088
https://github.com/hail-is/hail/pull/4036#issuecomment-408955855:15,Availability,error,error,15,addresses poor error message in https://github.com/hail-is/hail/issues/4033,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4036#issuecomment-408955855
https://github.com/hail-is/hail/pull/4036#issuecomment-408955855:21,Integrability,message,message,21,addresses poor error message in https://github.com/hail-is/hail/issues/4033,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4036#issuecomment-408955855
https://github.com/hail-is/hail/issues/4038#issuecomment-422929159:127,Deployability,deploy,deployment,127,"I investigated this and it will ~half cluster start up time from 4m30s to 2m30s. Should we do this @cseed? We can use the hail deployment to generate a new image each time master changes (that ensures its always younger than 30 days, given our pace of development).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-422929159
https://github.com/hail-is/hail/issues/4038#issuecomment-426225971:0,Deployability,upgrade,upgraded,0,upgraded to `crime against humanity`. https://hail.zulipchat.com/#narrow/stream/127527-team/subject/Dataproc/near/135016050,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-426225971
https://github.com/hail-is/hail/issues/4038#issuecomment-433529930:99,Usability,clear,clear,99,"I'm going to close this if no one objects, I think the service obviates this issue and there is no clear win to be had with images.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-433529930
https://github.com/hail-is/hail/issues/4038#issuecomment-433967898:108,Modifiability,config,configured,108,"IIRC, it took about an hour of effort to do the first time. Repeat invocations are nearly free, once you've configured it all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-433967898
https://github.com/hail-is/hail/pull/4043#issuecomment-409679487:22,Deployability,update,updated,22,@patrick-schultz I've updated this PR so that OrderedRVD.union() uses better logic to avoid the shuffling the RVDs that it's trying to merge. I did this by merging the partitioners so that each resulting partition only gets information from one original partition per input RVD. I've removed the unionDisjoint stuff because this merge logic should automatically do that in the case of disjoint RVDs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487
https://github.com/hail-is/hail/pull/4043#issuecomment-409679487:86,Safety,avoid,avoid,86,@patrick-schultz I've updated this PR so that OrderedRVD.union() uses better logic to avoid the shuffling the RVDs that it's trying to merge. I did this by merging the partitioners so that each resulting partition only gets information from one original partition per input RVD. I've removed the unionDisjoint stuff because this merge logic should automatically do that in the case of disjoint RVDs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487
https://github.com/hail-is/hail/pull/4043#issuecomment-409679487:77,Testability,log,logic,77,@patrick-schultz I've updated this PR so that OrderedRVD.union() uses better logic to avoid the shuffling the RVDs that it's trying to merge. I did this by merging the partitioners so that each resulting partition only gets information from one original partition per input RVD. I've removed the unionDisjoint stuff because this merge logic should automatically do that in the case of disjoint RVDs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487
https://github.com/hail-is/hail/pull/4043#issuecomment-409679487:335,Testability,log,logic,335,@patrick-schultz I've updated this PR so that OrderedRVD.union() uses better logic to avoid the shuffling the RVDs that it's trying to merge. I did this by merging the partitioners so that each resulting partition only gets information from one original partition per input RVD. I've removed the unionDisjoint stuff because this merge logic should automatically do that in the case of disjoint RVDs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487
https://github.com/hail-is/hail/pull/4049#issuecomment-412184897:934,Availability,down,down,934,"> Would it just be index which is the correct path relative to the metadata file?. Yes. So you should take the index path in the metadata interpreted with respect to the base of the index file (directory) rather than hardcoding it. This might seem like overkill but this flexibility has proved useful for the Matrix/Table format and I'd like to copy the idiom here. > I don't use the firstKeyOffset in the internal nodes anywhere. What were you envisioning it would be used for? Otherwise, I think we should delete it. So having indices like this effectively make a Matrix/Table arbitrarily repartitionable. For example, we can double the partitioning for free by splitting one partition into two by splitting at the roughly the midpoint row which I want to get from the root block. That's what I was thinking firstKeyOffset would be used for. This would be for a rough split. For a split accurate to the row-level, we'd have to read down to the leaf node blocks. Maybe this isn't really worth it. I'm on the fence. Delete it if you want. I thought of one more thing we should support in the file: store arbitrary user annotations of the keys. If the annotation is `+struct {}` it would have no overhead. This will give us some future flexibility and I can imagine the following use case: When we go to a sparse VCF, we'll want to store ""checkpoint"" rows to avoid having to search backward arbitrarily far to find ref blocks that might overlap with our variant of interest. The alternative is to make the first row of each partition a checkpoint block, but in that case, we can no longer seek into the middle of the partition if we want to track overlapping ref blocks. So we want to search for the first checkpoint row before our row of interest. Can we add this to the format but not use it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897
https://github.com/hail-is/hail/pull/4049#issuecomment-412184897:1338,Availability,checkpoint,checkpoint,1338,"> Would it just be index which is the correct path relative to the metadata file?. Yes. So you should take the index path in the metadata interpreted with respect to the base of the index file (directory) rather than hardcoding it. This might seem like overkill but this flexibility has proved useful for the Matrix/Table format and I'd like to copy the idiom here. > I don't use the firstKeyOffset in the internal nodes anywhere. What were you envisioning it would be used for? Otherwise, I think we should delete it. So having indices like this effectively make a Matrix/Table arbitrarily repartitionable. For example, we can double the partitioning for free by splitting one partition into two by splitting at the roughly the midpoint row which I want to get from the root block. That's what I was thinking firstKeyOffset would be used for. This would be for a rough split. For a split accurate to the row-level, we'd have to read down to the leaf node blocks. Maybe this isn't really worth it. I'm on the fence. Delete it if you want. I thought of one more thing we should support in the file: store arbitrary user annotations of the keys. If the annotation is `+struct {}` it would have no overhead. This will give us some future flexibility and I can imagine the following use case: When we go to a sparse VCF, we'll want to store ""checkpoint"" rows to avoid having to search backward arbitrarily far to find ref blocks that might overlap with our variant of interest. The alternative is to make the first row of each partition a checkpoint block, but in that case, we can no longer seek into the middle of the partition if we want to track overlapping ref blocks. So we want to search for the first checkpoint row before our row of interest. Can we add this to the format but not use it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897
https://github.com/hail-is/hail/pull/4049#issuecomment-412184897:1535,Availability,checkpoint,checkpoint,1535,"> Would it just be index which is the correct path relative to the metadata file?. Yes. So you should take the index path in the metadata interpreted with respect to the base of the index file (directory) rather than hardcoding it. This might seem like overkill but this flexibility has proved useful for the Matrix/Table format and I'd like to copy the idiom here. > I don't use the firstKeyOffset in the internal nodes anywhere. What were you envisioning it would be used for? Otherwise, I think we should delete it. So having indices like this effectively make a Matrix/Table arbitrarily repartitionable. For example, we can double the partitioning for free by splitting one partition into two by splitting at the roughly the midpoint row which I want to get from the root block. That's what I was thinking firstKeyOffset would be used for. This would be for a rough split. For a split accurate to the row-level, we'd have to read down to the leaf node blocks. Maybe this isn't really worth it. I'm on the fence. Delete it if you want. I thought of one more thing we should support in the file: store arbitrary user annotations of the keys. If the annotation is `+struct {}` it would have no overhead. This will give us some future flexibility and I can imagine the following use case: When we go to a sparse VCF, we'll want to store ""checkpoint"" rows to avoid having to search backward arbitrarily far to find ref blocks that might overlap with our variant of interest. The alternative is to make the first row of each partition a checkpoint block, but in that case, we can no longer seek into the middle of the partition if we want to track overlapping ref blocks. So we want to search for the first checkpoint row before our row of interest. Can we add this to the format but not use it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897
https://github.com/hail-is/hail/pull/4049#issuecomment-412184897:1705,Availability,checkpoint,checkpoint,1705,"> Would it just be index which is the correct path relative to the metadata file?. Yes. So you should take the index path in the metadata interpreted with respect to the base of the index file (directory) rather than hardcoding it. This might seem like overkill but this flexibility has proved useful for the Matrix/Table format and I'd like to copy the idiom here. > I don't use the firstKeyOffset in the internal nodes anywhere. What were you envisioning it would be used for? Otherwise, I think we should delete it. So having indices like this effectively make a Matrix/Table arbitrarily repartitionable. For example, we can double the partitioning for free by splitting one partition into two by splitting at the roughly the midpoint row which I want to get from the root block. That's what I was thinking firstKeyOffset would be used for. This would be for a rough split. For a split accurate to the row-level, we'd have to read down to the leaf node blocks. Maybe this isn't really worth it. I'm on the fence. Delete it if you want. I thought of one more thing we should support in the file: store arbitrary user annotations of the keys. If the annotation is `+struct {}` it would have no overhead. This will give us some future flexibility and I can imagine the following use case: When we go to a sparse VCF, we'll want to store ""checkpoint"" rows to avoid having to search backward arbitrarily far to find ref blocks that might overlap with our variant of interest. The alternative is to make the first row of each partition a checkpoint block, but in that case, we can no longer seek into the middle of the partition if we want to track overlapping ref blocks. So we want to search for the first checkpoint row before our row of interest. Can we add this to the format but not use it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897
https://github.com/hail-is/hail/pull/4049#issuecomment-412184897:1358,Safety,avoid,avoid,1358,"> Would it just be index which is the correct path relative to the metadata file?. Yes. So you should take the index path in the metadata interpreted with respect to the base of the index file (directory) rather than hardcoding it. This might seem like overkill but this flexibility has proved useful for the Matrix/Table format and I'd like to copy the idiom here. > I don't use the firstKeyOffset in the internal nodes anywhere. What were you envisioning it would be used for? Otherwise, I think we should delete it. So having indices like this effectively make a Matrix/Table arbitrarily repartitionable. For example, we can double the partitioning for free by splitting one partition into two by splitting at the roughly the midpoint row which I want to get from the root block. That's what I was thinking firstKeyOffset would be used for. This would be for a rough split. For a split accurate to the row-level, we'd have to read down to the leaf node blocks. Maybe this isn't really worth it. I'm on the fence. Delete it if you want. I thought of one more thing we should support in the file: store arbitrary user annotations of the keys. If the annotation is `+struct {}` it would have no overhead. This will give us some future flexibility and I can imagine the following use case: When we go to a sparse VCF, we'll want to store ""checkpoint"" rows to avoid having to search backward arbitrarily far to find ref blocks that might overlap with our variant of interest. The alternative is to make the first row of each partition a checkpoint block, but in that case, we can no longer seek into the middle of the partition if we want to track overlapping ref blocks. So we want to search for the first checkpoint row before our row of interest. Can we add this to the format but not use it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897
https://github.com/hail-is/hail/issues/4050#issuecomment-409401666:63,Modifiability,variab,variable,63,"I made a branch which I think fixes this (and makes the 'root' variable actually used), but will PR once I have testing going.; https://github.com/jbloom22/hail/tree/vep_csq_global",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4050#issuecomment-409401666
https://github.com/hail-is/hail/issues/4050#issuecomment-409401666:112,Testability,test,testing,112,"I made a branch which I think fixes this (and makes the 'root' variable actually used), but will PR once I have testing going.; https://github.com/jbloom22/hail/tree/vep_csq_global",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4050#issuecomment-409401666
https://github.com/hail-is/hail/pull/4052#issuecomment-409700497:26,Availability,error,error,26,I think you're getting an error because you need to add this to `hl.experimental.__init__.__all__`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409700497
https://github.com/hail-is/hail/pull/4052#issuecomment-409700517:16,Availability,error,error,16,that is a crazy error though,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409700517
https://github.com/hail-is/hail/pull/4052#issuecomment-409731138:109,Usability,clear,clear,109,"Yes, absolutely! I was confused between filename and method name when I first added them. I think things are clear in my head now -- but maybe some more explanations about what is required to add a method to experimental (beyond adding the method code) would be neat!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409731138
https://github.com/hail-is/hail/issues/4055#issuecomment-410039793:8,Availability,error,error,8,updated error:; ```; is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Key should be in partition 1: ([bar]-[foo]]; Invalid key: [quam]; ```; ðŸ¤”,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793
https://github.com/hail-is/hail/issues/4055#issuecomment-410039793:61,Availability,error,error,61,updated error:; ```; is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Key should be in partition 1: ([bar]-[foo]]; Invalid key: [quam]; ```; ðŸ¤”,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793
https://github.com/hail-is/hail/issues/4055#issuecomment-410039793:0,Deployability,update,updated,0,updated error:; ```; is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Key should be in partition 1: ([bar]-[foo]]; Invalid key: [quam]; ```; ðŸ¤”,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793
https://github.com/hail-is/hail/pull/4058#issuecomment-409578166:51,Usability,feedback,feedback,51,"Needs a bit more documentation, but looking to get feedback on the structure of the scala code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4058#issuecomment-409578166
https://github.com/hail-is/hail/issues/4063#issuecomment-409737215:41,Testability,test,tested,41,"Good catch! This is something that's not tested as part of CI, but we should do that (eventually). Will fix this bug, though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4063#issuecomment-409737215
https://github.com/hail-is/hail/pull/4066#issuecomment-410318525:79,Modifiability,config,configuring,79,"Ok, I had to fix some shit with gradle so that it doesn't try to call git when configuring tasks. Should pass tests now. I _hate_ gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4066#issuecomment-410318525
https://github.com/hail-is/hail/pull/4066#issuecomment-410318525:110,Testability,test,tests,110,"Ok, I had to fix some shit with gradle so that it doesn't try to call git when configuring tasks. Should pass tests now. I _hate_ gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4066#issuecomment-410318525
https://github.com/hail-is/hail/pull/4067#issuecomment-409990215:62,Testability,log,log,62,"Alternatively, I could set default `covariates=[1.0]` in lin, log, and skat. I feel like you, @tpoterba and I discussed this but I can't recall if/what we concluded. I'll post explanation of changes on discuss once in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4067#issuecomment-409990215
https://github.com/hail-is/hail/pull/4067#issuecomment-410486374:52,Testability,log,logistic,52,"Looks good to me. > It's going to be painful to get logistic to take no covariates. This isn't immediately obvious to me. Can you say a word or two about why?. I think we thought default `covariates=[1.0]` was misleading because for no covariates with intercept you don't have to add it, but with multiple you do. You'll make a discuss post about the breaking change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4067#issuecomment-410486374
https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:67,Availability,failure,failures,67,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374
https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:128,Energy Efficiency,green,green,128,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374
https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:62,Testability,test,test,62,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374
https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:89,Testability,test,tests,89,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374
https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:142,Testability,test,tests,142,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374
https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:176,Testability,test,tests,176,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374
https://github.com/hail-is/hail/pull/4068#issuecomment-411199876:60,Integrability,synchroniz,synchronization,60,"yeah, master succeeds. . I'm worried this is related to the synchronization in a sneaky way. We haven't built anything to support a multi-user JVM, so there could be things that go wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199876
https://github.com/hail-is/hail/pull/4068#issuecomment-411201627:11,Integrability,synchroniz,synchronizing,11,Let me try synchronizing on a lock object instead of this. It's possible that something else (logging?) may be synchronizing on `this`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411201627
https://github.com/hail-is/hail/pull/4068#issuecomment-411201627:111,Integrability,synchroniz,synchronizing,111,Let me try synchronizing on a lock object instead of this. It's possible that something else (logging?) may be synchronizing on `this`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411201627
https://github.com/hail-is/hail/pull/4068#issuecomment-411201627:94,Testability,log,logging,94,Let me try synchronizing on a lock object instead of this. It's possible that something else (logging?) may be synchronizing on `this`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411201627
https://github.com/hail-is/hail/pull/4068#issuecomment-411239262:6,Testability,test,tests,6,"Cool, tests pass now. Turns out that the issue was not the changes that I made to `context.py`, not the scala side changes. The python side initialization steps are not actually idempotent, so if the context has already been set up, we immediately return instead of running them again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411239262
https://github.com/hail-is/hail/issues/4070#issuecomment-410453178:14,Modifiability,rewrite,rewrite,14,ah! I added a rewrite rule here which uses the old path.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4070#issuecomment-410453178
https://github.com/hail-is/hail/pull/4073#issuecomment-410049172:58,Security,Hash,HashMap,58,Problem with py4j conversion of Python dict to `java.util.HashMap` instead of `scala.collection.immutable.Map`. Tested locally.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4073#issuecomment-410049172
https://github.com/hail-is/hail/pull/4073#issuecomment-410049172:112,Testability,Test,Tested,112,Problem with py4j conversion of Python dict to `java.util.HashMap` instead of `scala.collection.immutable.Map`. Tested locally.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4073#issuecomment-410049172
https://github.com/hail-is/hail/pull/4076#issuecomment-410349941:0,Deployability,update,update,0,"update: `calculateKeyRanges` doesn't work correctly when there's only one element (which we never hit, since `coerce` considers those to be sorted), which was causing the shuffle to drop the only moved element in some of my tests (which is why the tests were running so much faster than the non-shuffling version). . I did fix something in the ReorderedPartitions dependencies that was causing an extra partition to be included in the dependencies during the `union_rows`, and now this branch (sans explode_rows changes) is running at about the same speed as SplitMulti was previously, although there's still generally noticeable bump for the first iteration. so @tpoterba I think this is probably set for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941
https://github.com/hail-is/hail/pull/4076#issuecomment-410349941:364,Integrability,depend,dependencies,364,"update: `calculateKeyRanges` doesn't work correctly when there's only one element (which we never hit, since `coerce` considers those to be sorted), which was causing the shuffle to drop the only moved element in some of my tests (which is why the tests were running so much faster than the non-shuffling version). . I did fix something in the ReorderedPartitions dependencies that was causing an extra partition to be included in the dependencies during the `union_rows`, and now this branch (sans explode_rows changes) is running at about the same speed as SplitMulti was previously, although there's still generally noticeable bump for the first iteration. so @tpoterba I think this is probably set for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941
https://github.com/hail-is/hail/pull/4076#issuecomment-410349941:435,Integrability,depend,dependencies,435,"update: `calculateKeyRanges` doesn't work correctly when there's only one element (which we never hit, since `coerce` considers those to be sorted), which was causing the shuffle to drop the only moved element in some of my tests (which is why the tests were running so much faster than the non-shuffling version). . I did fix something in the ReorderedPartitions dependencies that was causing an extra partition to be included in the dependencies during the `union_rows`, and now this branch (sans explode_rows changes) is running at about the same speed as SplitMulti was previously, although there's still generally noticeable bump for the first iteration. so @tpoterba I think this is probably set for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941
https://github.com/hail-is/hail/pull/4076#issuecomment-410349941:224,Testability,test,tests,224,"update: `calculateKeyRanges` doesn't work correctly when there's only one element (which we never hit, since `coerce` considers those to be sorted), which was causing the shuffle to drop the only moved element in some of my tests (which is why the tests were running so much faster than the non-shuffling version). . I did fix something in the ReorderedPartitions dependencies that was causing an extra partition to be included in the dependencies during the `union_rows`, and now this branch (sans explode_rows changes) is running at about the same speed as SplitMulti was previously, although there's still generally noticeable bump for the first iteration. so @tpoterba I think this is probably set for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941
https://github.com/hail-is/hail/pull/4076#issuecomment-410349941:248,Testability,test,tests,248,"update: `calculateKeyRanges` doesn't work correctly when there's only one element (which we never hit, since `coerce` considers those to be sorted), which was causing the shuffle to drop the only moved element in some of my tests (which is why the tests were running so much faster than the non-shuffling version). . I did fix something in the ReorderedPartitions dependencies that was causing an extra partition to be included in the dependencies during the `union_rows`, and now this branch (sans explode_rows changes) is running at about the same speed as SplitMulti was previously, although there's still generally noticeable bump for the first iteration. so @tpoterba I think this is probably set for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941
https://github.com/hail-is/hail/pull/4076#issuecomment-411258192:33,Testability,log,logic,33,"Did I never PR the VEP splitting logic? I don't see it, so I must not have. I'll wait for this to go in and then do it then",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-411258192
https://github.com/hail-is/hail/issues/4078#issuecomment-410711084:12,Availability,error,error,12,still a bad error message. almost done with a fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084
https://github.com/hail-is/hail/issues/4078#issuecomment-410711084:18,Integrability,message,message,18,still a bad error message. almost done with a fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084
https://github.com/hail-is/hail/issues/4081#issuecomment-410356589:54,Availability,error,error,54,"oh, that's it. You can't do that. This is still a bad error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4081#issuecomment-410356589
https://github.com/hail-is/hail/issues/4081#issuecomment-410359446:22,Availability,error,error,22,maybe we can raise an error on array iteration instead of unsupported operation exception,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4081#issuecomment-410359446
https://github.com/hail-is/hail/pull/4088#issuecomment-410759919:23,Availability,error,error,23,@konradjk I think this error would have been specific enough for you to figure out what the problem was pretty quickly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4088#issuecomment-410759919
https://github.com/hail-is/hail/pull/4089#issuecomment-411863621:95,Usability,learn,learning,95,"I've been thinking about this for a while now, and I think that what's _extremely_ helpful for learning is seeing a lot of short code examples for a lot of different applications. I think what we want is a bunch of examples formatted something like this:. ```; TITLE: what the code does; --------------; TAGS: comma-delimited set of search terms. DESC [optional] one- or two-sentence (max) clarification of what is being done. >>> CODE. --OR--. if there are multiple ways to do something, patterns like:. Method 1, if (condition 1):; >>> CODE 1. Method 2 (if condition 1 is not true):; >>> CODE 2. USES: clickable links to functions used in the code above. Probably not required for basic things like annotate / select. but definitely good to have for ld_prune or trio_matrix or their ilk. UNDERSTANDING [optional] An understanding section with click-to-expand styling. This shouldn't be required for a contribution to the cookbook, but could really help in some cases.; ```. This format has the advantage of keeping everything very visually tight, which will help people who learn through examples just scan through and look for patterns (this is a lot of people, I think). It also lets us embed understanding sections, which I do see the value of.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-411863621
https://github.com/hail-is/hail/pull/4089#issuecomment-411863621:1076,Usability,learn,learn,1076,"I've been thinking about this for a while now, and I think that what's _extremely_ helpful for learning is seeing a lot of short code examples for a lot of different applications. I think what we want is a bunch of examples formatted something like this:. ```; TITLE: what the code does; --------------; TAGS: comma-delimited set of search terms. DESC [optional] one- or two-sentence (max) clarification of what is being done. >>> CODE. --OR--. if there are multiple ways to do something, patterns like:. Method 1, if (condition 1):; >>> CODE 1. Method 2 (if condition 1 is not true):; >>> CODE 2. USES: clickable links to functions used in the code above. Probably not required for basic things like annotate / select. but definitely good to have for ld_prune or trio_matrix or their ilk. UNDERSTANDING [optional] An understanding section with click-to-expand styling. This shouldn't be required for a contribution to the cookbook, but could really help in some cases.; ```. This format has the advantage of keeping everything very visually tight, which will help people who learn through examples just scan through and look for patterns (this is a lot of people, I think). It also lets us embed understanding sections, which I do see the value of.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-411863621
https://github.com/hail-is/hail/pull/4089#issuecomment-411940112:76,Usability,guid,guides,76,"These are good thoughts. I agree that a visually clean format will make the guides easier for people to use. I like the idea of hiding the explanation underneath a clickable thing. I'd be hesitant to completely remove the explanatory text, because I think people will be more likely to use the how-to guides than to go through an entire tutorial, but making it optional is a good middle-ground. I can experiment on one of the files, before I do them all, so we can settle on a good format.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-411940112
https://github.com/hail-is/hail/pull/4089#issuecomment-411940112:301,Usability,guid,guides,301,"These are good thoughts. I agree that a visually clean format will make the guides easier for people to use. I like the idea of hiding the explanation underneath a clickable thing. I'd be hesitant to completely remove the explanatory text, because I think people will be more likely to use the how-to guides than to go through an entire tutorial, but making it optional is a good middle-ground. I can experiment on one of the files, before I do them all, so we can settle on a good format.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-411940112
https://github.com/hail-is/hail/pull/4089#issuecomment-415074240:143,Deployability,toggle,toggleable,143,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240
https://github.com/hail-is/hail/pull/4089#issuecomment-415074240:529,Security,expose,expose,529,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240
https://github.com/hail-is/hail/pull/4089#issuecomment-415074240:14,Usability,simpl,simple,14,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240
https://github.com/hail-is/hail/pull/4089#issuecomment-415077569:168,Integrability,depend,dependencies,168,"This looks *AWESOME*! Organizationally, I think the tags should be english text for common search terms, and the existing tags you have should go into a `see also` or `dependencies` or something section (For clickable links to the method that this implementation uses)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415077569
https://github.com/hail-is/hail/pull/4094#issuecomment-411077664:93,Testability,test,tests,93,"@danking I'm assigning this to you if you want to start looking, but I want to add some more tests before we merge it, and I think it will need to go in after #4076.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4094#issuecomment-411077664
https://github.com/hail-is/hail/pull/4094#issuecomment-412565015:160,Integrability,interface,interface,160,Yes! https://github.com/hail-is/hail/pull/4119/files#diff-b173fb9bd584d50afcfa6724954ef3b5R415 (It will be later work to rip out partition keys from the python interface.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4094#issuecomment-412565015
https://github.com/hail-is/hail/issues/4096#issuecomment-410890171:52,Performance,cache,cached,52,"Yeah, I misjudged, clearly the determinism issue. I cached for my test case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171
https://github.com/hail-is/hail/issues/4096#issuecomment-410890171:66,Testability,test,test,66,"Yeah, I misjudged, clearly the determinism issue. I cached for my test case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171
https://github.com/hail-is/hail/issues/4096#issuecomment-410890171:19,Usability,clear,clearly,19,"Yeah, I misjudged, clearly the determinism issue. I cached for my test case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171
https://github.com/hail-is/hail/pull/4102#issuecomment-411237753:14,Testability,test,tests,14,"I should have tests in python for the functionality I need, so if those are passing it should be OK, but let's talk tomorrow. It turns out caitlin's hack is getting spread around a bit because lots of folks want to do PRS, it would be good to go from hack to the better solution in one PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4102#issuecomment-411237753
https://github.com/hail-is/hail/pull/4104#issuecomment-411397458:149,Testability,test,testing,149,"Joining something with itself shouldn't be doing any repartitioning. If the problem does have something to do with repartitioning, it might be worth testing it on #4094.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411397458
https://github.com/hail-is/hail/pull/4104#issuecomment-411794524:24,Modifiability,rewrite,rewrite,24,If you can make this IR rewrite pass determinism with/without optimization I will be totally confident in this change; https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/ir/Simplify.scala#L294,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411794524
https://github.com/hail-is/hail/pull/4104#issuecomment-411794524:62,Performance,optimiz,optimization,62,If you can make this IR rewrite pass determinism with/without optimization I will be totally confident in this change; https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/ir/Simplify.scala#L294,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411794524
https://github.com/hail-is/hail/pull/4104#issuecomment-411794524:194,Usability,Simpl,Simplify,194,If you can make this IR rewrite pass determinism with/without optimization I will be totally confident in this change; https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/ir/Simplify.scala#L294,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411794524
https://github.com/hail-is/hail/pull/4104#issuecomment-411814585:108,Modifiability,rewrite,rewrite,108,"ah wait - in the example we discussed, there needs to be randomness *after* the order by to fully test this rewrite rule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411814585
https://github.com/hail-is/hail/pull/4104#issuecomment-411814585:98,Testability,test,test,98,"ah wait - in the example we discussed, there needs to be randomness *after* the order by to fully test this rewrite rule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411814585
https://github.com/hail-is/hail/pull/4104#issuecomment-411878967:280,Performance,optimiz,optimization,280,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
https://github.com/hail-is/hail/pull/4104#issuecomment-411878967:545,Performance,perform,performed,545,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
https://github.com/hail-is/hail/pull/4104#issuecomment-411878967:198,Testability,test,tests,198,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
https://github.com/hail-is/hail/pull/4104#issuecomment-411878967:251,Testability,test,tests,251,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
https://github.com/hail-is/hail/pull/4104#issuecomment-411878967:457,Usability,simpl,simplify,457,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
https://github.com/hail-is/hail/issues/4110#issuecomment-411901041:17,Availability,error,error,17,"this is a python error, with analyze probably",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4110#issuecomment-411901041
https://github.com/hail-is/hail/pull/4113#issuecomment-413067829:5,Deployability,update,update,5,Nice update!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4113#issuecomment-413067829
https://github.com/hail-is/hail/pull/4118#issuecomment-411796377:22,Availability,error,error,22,I think this is a bad error message but should still be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377
https://github.com/hail-is/hail/pull/4118#issuecomment-411796377:59,Availability,error,error,59,I think this is a bad error message but should still be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377
https://github.com/hail-is/hail/pull/4118#issuecomment-411796377:28,Integrability,message,message,28,I think this is a bad error message but should still be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377
https://github.com/hail-is/hail/pull/4121#issuecomment-416343633:5,Testability,test,test,5,nice test : code ratio,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4121#issuecomment-416343633
https://github.com/hail-is/hail/pull/4123#issuecomment-412981153:91,Integrability,protocol,protocol,91,"I'm ready to approve, but I'm not sure why the build is failing, and I don't know what the protocol is now for approving PRs failing tests. Seems like it increases the chances of merging a dumb mistake.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4123#issuecomment-412981153
https://github.com/hail-is/hail/pull/4123#issuecomment-412981153:133,Testability,test,tests,133,"I'm ready to approve, but I'm not sure why the build is failing, and I don't know what the protocol is now for approving PRs failing tests. Seems like it increases the chances of merging a dumb mistake.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4123#issuecomment-412981153
https://github.com/hail-is/hail/pull/4123#issuecomment-412981330:8,Testability,test,tests,8,failing tests will still prevent merging,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4123#issuecomment-412981330
https://github.com/hail-is/hail/pull/4123#issuecomment-412981834:37,Testability,test,tests,37,"Sure, I mean that somebody could get tests passing and immediately push the commit, and the CI merge, before they notice the dumb mistake. Mostly a danger if the tests are failing for non-trivial reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4123#issuecomment-412981834
https://github.com/hail-is/hail/pull/4123#issuecomment-412981834:162,Testability,test,tests,162,"Sure, I mean that somebody could get tests passing and immediately push the commit, and the CI merge, before they notice the dumb mistake. Mostly a danger if the tests are failing for non-trivial reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4123#issuecomment-412981834
https://github.com/hail-is/hail/pull/4125#issuecomment-412540177:40,Testability,test,tests,40,@tpoterba this appears to break several tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4125#issuecomment-412540177
https://github.com/hail-is/hail/issues/4127#issuecomment-412018951:14,Testability,log,log,14,need the hail.log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4127#issuecomment-412018951
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:161,Availability,failure,failure,161,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:220,Availability,failure,failure,220,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12320,Availability,Error,Error,12320,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4250,Energy Efficiency,schedul,scheduler,4250,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4322,Energy Efficiency,schedul,scheduler,4322,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4687,Energy Efficiency,schedul,scheduler,4687,ollection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4727,Energy Efficiency,schedul,scheduler,4727,at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4826,Energy Efficiency,schedul,scheduler,4826,t scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4924,Energy Efficiency,schedul,scheduler,4924,onfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5178,Energy Efficiency,schedul,scheduler,5178,nfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5259,Energy Efficiency,schedul,scheduler,5259,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5365,Energy Efficiency,schedul,scheduler,5365,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5515,Energy Efficiency,schedul,scheduler,5515,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5604,Energy Efficiency,schedul,scheduler,5604,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5702,Energy Efficiency,schedul,scheduler,5702,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5798,Energy Efficiency,schedul,scheduler,5798,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.hail.utils.richUtils.RichContextRDD.writePartitions(RichContextRDD.scala:48); 	at is.hail.io.RichC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5963,Energy Efficiency,schedul,scheduler,5963,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.hail.utils.richUtils.RichContextRDD.writePartitions(RichContextRDD.scala:48); 	at is.hail.io.RichContextRDDRegionValue$.writeRows$extension(RowStore.scala:1096); 	at is.hail.rvd.RVD$class.write(RVD.scala:467); 	at is.hail.rvd.OrderedRVD.write(OrderedRVD.scala:32),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:11890,Energy Efficiency,schedul,scheduler,11890,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:11962,Energy Efficiency,schedul,scheduler,11962,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4447,Performance,concurren,concurrent,4447,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4532,Performance,concurren,concurrent,4532, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12087,Performance,concurren,concurrent,12087,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12172,Performance,concurren,concurrent,12172,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:140,Safety,abort,aborted,140,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4858,Safety,abort,abortStage,4858,tractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:4956,Safety,abort,abortStage,4956,un$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5201,Safety,abort,abortStage,5201,xt.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:38,Testability,Assert,Assertion,38,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:49,Testability,Log,Log,49,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:331,Testability,Assert,AssertionError,331,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:347,Testability,assert,assertion,347,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:383,Testability,assert,assert,383,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:7971,Testability,Assert,AssertionError,7971,leValue.write(TableValue.scala:85); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:589); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:48); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:23); 	at is.hail.table.Table.write(Table.scala:607); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:7987,Testability,assert,assertion,7987,leValue.write(TableValue.scala:85); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:589); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:48); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:23); 	at is.hail.table.Table.write(Table.scala:607); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:8023,Testability,assert,assert,8023,at is.hail.expr.ir.Interpret$.apply(Interpret.scala:589); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:48); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:23); 	at is.hail.table.Table.write(Table.scala:607); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12335,Testability,Assert,AssertionError,12335,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12351,Testability,assert,assertion,12351,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719
https://github.com/hail-is/hail/pull/4128#issuecomment-412775955:107,Availability,Error,Error,107,"Can confirm that this ""fixes"" it, since current master results in:; ```; Hail version: devel-f4fc571b4570; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: +Struct{}; expect: Struct{}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412775955
https://github.com/hail-is/hail/pull/4128#issuecomment-412775955:122,Testability,Assert,AssertionError,122,"Can confirm that this ""fixes"" it, since current master results in:; ```; Hail version: devel-f4fc571b4570; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: +Struct{}; expect: Struct{}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412775955
https://github.com/hail-is/hail/pull/4128#issuecomment-412775955:138,Testability,assert,assertion,138,"Can confirm that this ""fixes"" it, since current master results in:; ```; Hail version: devel-f4fc571b4570; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: +Struct{}; expect: Struct{}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412775955
https://github.com/hail-is/hail/pull/4128#issuecomment-413500936:198,Performance,optimiz,optimized,198,The other option is to drop the literals added to the globals in references to globals inside the IRs. This will generally look like (GetField (DropFields globals (a b c ...) x) which will just get optimized to (GetField globals x).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-413500936
https://github.com/hail-is/hail/pull/4128#issuecomment-413510162:79,Deployability,pipeline,pipelines,79,"Bowing out of this one as it is too spicy for my taste. Happy to test it on my pipelines once its passing tests and ready, but someone else should probably do the code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-413510162
https://github.com/hail-is/hail/pull/4128#issuecomment-413510162:65,Testability,test,test,65,"Bowing out of this one as it is too spicy for my taste. Happy to test it on my pipelines once its passing tests and ready, but someone else should probably do the code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-413510162
https://github.com/hail-is/hail/pull/4128#issuecomment-413510162:106,Testability,test,tests,106,"Bowing out of this one as it is too spicy for my taste. Happy to test it on my pipelines once its passing tests and ready, but someone else should probably do the code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-413510162
https://github.com/hail-is/hail/pull/4131#issuecomment-412149140:106,Testability,test,test,106,"@tpoterba When you get a chance, could you please look over the PruneDeadFields code I added and also the test suite for it? I wasn't sure what to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4131#issuecomment-412149140
https://github.com/hail-is/hail/pull/4139#issuecomment-412612911:140,Deployability,update,update,140,"Changing the `dev-environment.yaml` requires you to build a new CI image. You can do this with `make push-hail-ci-build-image`. This should update the file `hail-ci-build-image`. If you check that change into this branch, the CI system will use that image, with the new conda environment, instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4139#issuecomment-412612911
https://github.com/hail-is/hail/issues/4147#issuecomment-415026464:42,Testability,test,test,42,"Do you have an alternative suggestion? We test export/import is the identity, which is a nice property.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4147#issuecomment-415026464
https://github.com/hail-is/hail/issues/4147#issuecomment-415033609:400,Modifiability,config,configurable,400,"We're really being compatible with R here, no? [Pandas defaults to the empty string](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) (which seems *definitively* worse). It's a compelling argument nonetheless. I would prefer an export mode that's ""R compatible"" which basically means flattening out any structs and writing all missing values as `NA` (or some user configurable string). Then, I'd prefer the default is the JSON-y representation, which is to say, `null` for missing values whether they're alone in a column or inside a struct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4147#issuecomment-415033609
https://github.com/hail-is/hail/pull/4150#issuecomment-413124680:65,Testability,test,testing,65,If you made one with `csq=True` it would be pretty easy (in your testing script) to take a line of the output and assert that it's the same length (after splitting both on `|`) as string in the header,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4150#issuecomment-413124680
https://github.com/hail-is/hail/pull/4150#issuecomment-413124680:114,Testability,assert,assert,114,If you made one with `csq=True` it would be pretty easy (in your testing script) to take a line of the output and assert that it's the same length (after splitting both on `|`) as string in the header,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4150#issuecomment-413124680
https://github.com/hail-is/hail/pull/4150#issuecomment-415224730:124,Testability,test,test,124,"If you're happy, I'll assign this for scala review and after it's in, the next step is figuring out how to set up the CI to test vep with both options for reproducing these files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4150#issuecomment-415224730
https://github.com/hail-is/hail/pull/4150#issuecomment-415225079:138,Testability,test,tests,138,"@jigold you won the lottery, but DO NOT approve until Konrad has responded with regard to whether he's happy with the result of my manual tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4150#issuecomment-415225079
https://github.com/hail-is/hail/pull/4151#issuecomment-415795977:35,Testability,test,tested,35,@danking shouldn't this be getting tested / merged?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4151#issuecomment-415795977
https://github.com/hail-is/hail/issues/4153#issuecomment-413194946:175,Security,access,accessed,175,"> Second, StructExpression.init creates expressions for each field in the constructor. That seems excessive, we should construct the field expressions on demand when they are accessed. We lose dot-completion, then. I think the current design is correct (constructing 3k things shouldn't be a problem, anyhow)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4153#issuecomment-413194946
https://github.com/hail-is/hail/pull/4154#issuecomment-414640137:21,Integrability,interface,interface,21,I very much like the interface. :+1:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4154#issuecomment-414640137
https://github.com/hail-is/hail/pull/4154#issuecomment-416591508:66,Integrability,interface,interface,66,@konradjk This isn't a breaking change. We're keeping the current interface as an alias to `aggregate_entries().result()`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4154#issuecomment-416591508
https://github.com/hail-is/hail/issues/4160#issuecomment-413959206:48,Availability,avail,available,48,"FYI I've made 1000 genomes phase 3 MTs publicly available here: ; ```; gs://hail-datasets/hail-data/1000_genomes_phase3_{autosomes,chrX,chrY,chrMT}.GRCh37.mt; gs://hail-datasets/hail-data/1000_genomes_phase3_{autosomes,chrX,chrY,chrMT}.GRCh38.liftover.mt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4160#issuecomment-413959206
https://github.com/hail-is/hail/issues/4160#issuecomment-413977183:132,Deployability,release,release,132,"FYI, Liam made matrix tables from the files direct from EBI, which have this header line:; ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/; https://console.cloud.google.com/storage/browser/hail-datasets/raw-data/?project=broad-ctsa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4160#issuecomment-413977183
https://github.com/hail-is/hail/pull/4165#issuecomment-413875178:68,Availability,error,erroring,68,@patrick-schultz I'm closing this for now because there's something erroring in the python tests that I don't understand. I'll re-open once I've fixed that and am more certain that I actually understand what's happening.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4165#issuecomment-413875178
https://github.com/hail-is/hail/pull/4165#issuecomment-413875178:91,Testability,test,tests,91,@patrick-schultz I'm closing this for now because there's something erroring in the python tests that I don't understand. I'll re-open once I've fixed that and am more certain that I actually understand what's happening.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4165#issuecomment-413875178
https://github.com/hail-is/hail/pull/4166#issuecomment-415835792:637,Usability,clear,clear,637,"So my reading now of `seed` is that it uniquely identifies a sequence of numbers. I read the docs now, and I'm not sure I agree with these sentences:; > The values are seeded when the function is called, so calling a random Hail function and then using it several times in the same expression will yield the same result each time.; > ; > Evaluating the same expression will yield the same value every time, but multiple calls of the same function will have different results. I think the trouble is with the meaning of ""called"", ""using"", and ""evaluating"". I see now that you mean calling the python function by ""called"", but that wasn't clear to me on first reading. ""Using"" I think just means appearing in the source code, which feels right. The last sentence, I think, is not true, given the below:. ```python; In [31]: z = hl.rand_unif(0, 1, seed=0); ...: ; ...: t = hl.utils.range_table(2); ...: t = t.annotate(; ...: x = hl.literal([1,2,3]).map(lambda i: hl.rand_unif(0,1,seed=0)),; ...: y = hl.literal([1,2,3]).map(lambda i: hl.rand_unif(0,1,seed=0)),; ...: z = hl.literal([1,2,3]).map(lambda i: z)); ...: ; ...: t.show(); ```. I think I really prefer the `[z,z]` means two draws. If you want one draw, you gotta use a let or `annotate_globals` or something.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415835792
https://github.com/hail-is/hail/pull/4166#issuecomment-415848091:63,Integrability,interface,interface,63,"@danking I've made all of the changes except the one about the interface for `af_dist`. (I agree with the point you're making about it being confusing that both functions are seeded separately and need to be kept the same for the same dataset to be produced, but making it take a function might just add a lot of visual noise. Honestly, I'm really tempted to take all of the `seed` stuff out of `balding_nichols_model` and put a note in to the effect of ""for reproducible results, use `hl.set_global_seed()` just before creating a dataset."" How would you feel about that?). re: how seeding functions happens more generally---I don't think anything I've said in the docs is actually incorrect (I would consider array transformations a context, kind of like the axes of a table or matrix table, so the table should end up with `x = y = z` but different values for the elements in the array, and different values between rows), but I'll work on making the language clearer so that they actually say what I mean. w.r.t. `[z, z]` being two draws---I have sometimes waffled on this but in general I think I tend to disagree, in part because I have struggled to come up with a consistent definition of what that would mean. Happy to talk more about this, but I don't know if it's super relevant to balding nichols since all of the distributions are once-per-element draws.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415848091
https://github.com/hail-is/hail/pull/4166#issuecomment-415848091:962,Usability,clear,clearer,962,"@danking I've made all of the changes except the one about the interface for `af_dist`. (I agree with the point you're making about it being confusing that both functions are seeded separately and need to be kept the same for the same dataset to be produced, but making it take a function might just add a lot of visual noise. Honestly, I'm really tempted to take all of the `seed` stuff out of `balding_nichols_model` and put a note in to the effect of ""for reproducible results, use `hl.set_global_seed()` just before creating a dataset."" How would you feel about that?). re: how seeding functions happens more generally---I don't think anything I've said in the docs is actually incorrect (I would consider array transformations a context, kind of like the axes of a table or matrix table, so the table should end up with `x = y = z` but different values for the elements in the array, and different values between rows), but I'll work on making the language clearer so that they actually say what I mean. w.r.t. `[z, z]` being two draws---I have sometimes waffled on this but in general I think I tend to disagree, in part because I have struggled to come up with a consistent definition of what that would mean. Happy to talk more about this, but I don't know if it's super relevant to balding nichols since all of the distributions are once-per-element draws.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415848091
https://github.com/hail-is/hail/pull/4166#issuecomment-415881986:29,Testability,test,tests,29,@catoverdrive looks like the tests still expect a seed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415881986
https://github.com/hail-is/hail/pull/4177#issuecomment-414490470:40,Modifiability,refactor,refactored,40,ahhhhh yes it was. aggregateEntries was refactored to aggregateEntriesAST but the new aggregateEntries (IR) didn't get the new node.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4177#issuecomment-414490470
https://github.com/hail-is/hail/pull/4178#issuecomment-414487778:0,Performance,perform,performance,0,performance tests should catch stuff like this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4178#issuecomment-414487778
https://github.com/hail-is/hail/pull/4178#issuecomment-414487778:12,Testability,test,tests,12,performance tests should catch stuff like this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4178#issuecomment-414487778
https://github.com/hail-is/hail/pull/4185#issuecomment-414755012:71,Deployability,update,update,71,"With @danking 's recent work on our custom CI service, we also need to update the docker image anytime the environment changes. From the repo head:. ```; make prime-the-engines; make push-hail-ci-build-image; ```. This will update the `hail-ci-build-image` file (a pointer to google container registry)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4185#issuecomment-414755012
https://github.com/hail-is/hail/pull/4185#issuecomment-414755012:224,Deployability,update,update,224,"With @danking 's recent work on our custom CI service, we also need to update the docker image anytime the environment changes. From the repo head:. ```; make prime-the-engines; make push-hail-ci-build-image; ```. This will update the `hail-ci-build-image` file (a pointer to google container registry)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4185#issuecomment-414755012
https://github.com/hail-is/hail/pull/4189#issuecomment-414839713:5,Testability,test,test,5,I'll test on VEP once you resolve the conflicts,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4189#issuecomment-414839713
https://github.com/hail-is/hail/pull/4189#issuecomment-415088122:82,Availability,error,error,82,"Don't know what's going on with `test_linreg` failing. It passes locally, and the error ""java.io.FileNotFoundException: /tmp/blockmgr-0a5af284-ccc3-4893-bfcc-bf840e7b1973/1e/broadcast_3344 (No space left on device)"" looks like a CI issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4189#issuecomment-415088122
https://github.com/hail-is/hail/pull/4189#issuecomment-415173413:156,Testability,TEST,TEST,156,```; Checking 'hl.vep' replicates on 'gs://hail-common/vep/vep/vep_examplars/vep_35d9e30.mt'; 2018-08-22 20:43:44 Hail: INFO: vep: annotated 1196 variants; TEST PASSED; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4189#issuecomment-415173413
https://github.com/hail-is/hail/pull/4191#issuecomment-418202591:0,Testability,Test,Tests,0,"Tests added, ready for review, happy to explain in person too if helpful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4191#issuecomment-418202591
https://github.com/hail-is/hail/issues/4192#issuecomment-415137754:579,Availability,robust,robust,579,"This is an overflow bug that's existed since June 25: https://github.com/hail-is/hail/pull/3833. The implementation was vulnerable to `(a + b) * (c + d) * (b + d) * (a + c)` exceeding max int.; ```; val ad = a * d; val bc = (b * c).toDouble; val det = ad - bc; val chiSquare = (det * det * (a + b + c + d)) / ((a + b) * (c + d) * (b + d) * (a + c)); ```; If so, chiSquare and hence the p-value would be wrong.; The odds ratio was also susceptible to overflow, though less so: if `a * d` or `b * c` exceeded max int. The new implementation converts the ints to floats and is more robust if we switch to allowing float inputs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4192#issuecomment-415137754
https://github.com/hail-is/hail/issues/4192#issuecomment-415167977:63,Testability,test,test-contingency-table-test-present-since-june-,63,http://discuss.hail.is/t/fixed-overflow-bug-in-0-2-chi-squared-test-contingency-table-test-present-since-june-25-2018/619,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4192#issuecomment-415167977
https://github.com/hail-is/hail/issues/4193#issuecomment-422375767:44,Deployability,release,release,44,I kinda feel like we can't ship this in 0.2 release in its current state of brokenness,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4193#issuecomment-422375767
https://github.com/hail-is/hail/pull/4197#issuecomment-416761645:18,Testability,log,logic,18,"I added some more logic since the criteria (at the IR level) of ""this works as an interval join"" is different if you're joining to a Table vs joining to a MatrixTable. I also feel like the interval logic MatrixAnnotateRowsTable IR node should get pulled out into its own separate node (and be the same as a table interval join?), but I will follow up with that later and not here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4197#issuecomment-416761645
https://github.com/hail-is/hail/pull/4197#issuecomment-416761645:198,Testability,log,logic,198,"I added some more logic since the criteria (at the IR level) of ""this works as an interval join"" is different if you're joining to a Table vs joining to a MatrixTable. I also feel like the interval logic MatrixAnnotateRowsTable IR node should get pulled out into its own separate node (and be the same as a table interval join?), but I will follow up with that later and not here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4197#issuecomment-416761645
https://github.com/hail-is/hail/issues/4202#issuecomment-416762038:79,Availability,down,down,79,"for some reason I guess I tagged this issue wrong from the PR?. We traced this down to a Spark problem with the iterator that comes out of the shuffle, and so #4228 should fix the memory leak issue for this specific case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4202#issuecomment-416762038
https://github.com/hail-is/hail/pull/4211#issuecomment-416093339:89,Deployability,upgrade,upgrade,89,"OK, will change to exec/wait and fix the merge conflict (there was a conflict due to the upgrade to; libsimdpp-2.1, I tried to fix that but may need to do more, or there may be a new issue). My preference is to leave the build-command execution on the C++ side because it's (arguably); easier to read/understand the combined Scala + C++ functionality if the Scala NativeModule is a ; trivial wrapper and all the substance is on one side, in this case in C++. Since that's also acceptable to; you, I'll keep it that way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339
https://github.com/hail-is/hail/pull/4211#issuecomment-416093339:392,Integrability,wrap,wrapper,392,"OK, will change to exec/wait and fix the merge conflict (there was a conflict due to the upgrade to; libsimdpp-2.1, I tried to fix that but may need to do more, or there may be a new issue). My preference is to leave the build-command execution on the C++ side because it's (arguably); easier to read/understand the combined Scala + C++ functionality if the Scala NativeModule is a ; trivial wrapper and all the substance is on one side, in this case in C++. Since that's also acceptable to; you, I'll keep it that way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339
https://github.com/hail-is/hail/pull/4211#issuecomment-417031457:740,Performance,load,loading,740,"The latest changes adopt the ""happy medium"" of having at all times either 0 or 1 NativeModule; objects corresponding to each lib, and a worker may get a shared ptr to what was constructed as; a master NativeModule. . In either case, the constructor is responsible for checking that lib already exists, or populating; it if it didn't exist. The big_mutex is held during constructors, in a way which a) protects the; module_table, and b) makes the transition from ""no lib file"" to ""complete and immutable lib file""; appear atomic, whether that occurs by invoking the makefile, or by writing binary data. Consequently, the makefile is simplified to just build $(MODULE).so without worrying about; atomicity, and the perl rename goes away. The loading is now done eagerly in the worker constructor, but still lazily for master-constructed; NativeModule's, since the most common lifecycle for a master is to construct it, do getKey, getBinary,; then throw it away without needing to load it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-417031457
https://github.com/hail-is/hail/pull/4211#issuecomment-417031457:978,Performance,load,load,978,"The latest changes adopt the ""happy medium"" of having at all times either 0 or 1 NativeModule; objects corresponding to each lib, and a worker may get a shared ptr to what was constructed as; a master NativeModule. . In either case, the constructor is responsible for checking that lib already exists, or populating; it if it didn't exist. The big_mutex is held during constructors, in a way which a) protects the; module_table, and b) makes the transition from ""no lib file"" to ""complete and immutable lib file""; appear atomic, whether that occurs by invoking the makefile, or by writing binary data. Consequently, the makefile is simplified to just build $(MODULE).so without worrying about; atomicity, and the perl rename goes away. The loading is now done eagerly in the worker constructor, but still lazily for master-constructed; NativeModule's, since the most common lifecycle for a master is to construct it, do getKey, getBinary,; then throw it away without needing to load it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-417031457
https://github.com/hail-is/hail/pull/4211#issuecomment-417031457:632,Usability,simpl,simplified,632,"The latest changes adopt the ""happy medium"" of having at all times either 0 or 1 NativeModule; objects corresponding to each lib, and a worker may get a shared ptr to what was constructed as; a master NativeModule. . In either case, the constructor is responsible for checking that lib already exists, or populating; it if it didn't exist. The big_mutex is held during constructors, in a way which a) protects the; module_table, and b) makes the transition from ""no lib file"" to ""complete and immutable lib file""; appear atomic, whether that occurs by invoking the makefile, or by writing binary data. Consequently, the makefile is simplified to just build $(MODULE).so without worrying about; atomicity, and the perl rename goes away. The loading is now done eagerly in the worker constructor, but still lazily for master-constructed; NativeModule's, since the most common lifecycle for a master is to construct it, do getKey, getBinary,; then throw it away without needing to load it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-417031457
https://github.com/hail-is/hail/issues/4215#issuecomment-423534766:123,Security,access,access,123,"I can't `git pull` at the moment, but as of `c2508f35dc41` this is still an issue. Small example that you guys should have access to:. ```; mutation_ht = hl.import_table('gs://gnomad-resources/constraint/source/fordist_1KG_mutation_rate_table.txt',; delimiter=' ', impute=True); mutation_ht = mutation_ht.transmute(context=mutation_ht['from'], ref=mutation_ht['from'][1],; alt=mutation_ht.to[1]).key_by('context', 'ref', 'alt'); mu = mutation_ht.aggregate(hl.agg.group_by(; hl.struct(context=mutation_ht.context, ref=mutation_ht.ref, alt=mutation_ht.alt),; hl.agg.collect(mutation_ht.mu_snp))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4215#issuecomment-423534766
https://github.com/hail-is/hail/issues/4216#issuecomment-416051659:31,Availability,error,error,31,Actually looks like just a bad error message. One of the Tables had 0 rows.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4216#issuecomment-416051659
https://github.com/hail-is/hail/issues/4216#issuecomment-416051659:37,Integrability,message,message,37,Actually looks like just a bad error message. One of the Tables had 0 rows.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4216#issuecomment-416051659
https://github.com/hail-is/hail/issues/4216#issuecomment-416346870:21,Testability,assert,assertion,21,it was failing on an assertion of num_partitions and i think a table with 0 rows has 0 partitions,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4216#issuecomment-416346870
https://github.com/hail-is/hail/pull/4218#issuecomment-416754740:18,Safety,sanity check,sanity check,18,"I will do a quick sanity check on this before dismissing the review and requesting anew. Right now no tests cover this, but @jbloom22's framework could if we really wanted (annotate a multi-allelic, split it using this, and assert that each allele has at least one annotation)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4218#issuecomment-416754740
https://github.com/hail-is/hail/pull/4218#issuecomment-416754740:102,Testability,test,tests,102,"I will do a quick sanity check on this before dismissing the review and requesting anew. Right now no tests cover this, but @jbloom22's framework could if we really wanted (annotate a multi-allelic, split it using this, and assert that each allele has at least one annotation)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4218#issuecomment-416754740
https://github.com/hail-is/hail/pull/4218#issuecomment-416754740:224,Testability,assert,assert,224,"I will do a quick sanity check on this before dismissing the review and requesting anew. Right now no tests cover this, but @jbloom22's framework could if we really wanted (annotate a multi-allelic, split it using this, and assert that each allele has at least one annotation)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4218#issuecomment-416754740
https://github.com/hail-is/hail/pull/4220#issuecomment-416617372:22,Deployability,update,update,22,"I realize I forgot to update the latest-hash lines, I'll fix that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372
https://github.com/hail-is/hail/pull/4220#issuecomment-416617372:40,Security,hash,hash,40,"I realize I forgot to update the latest-hash lines, I'll fix that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372
https://github.com/hail-is/hail/pull/4225#issuecomment-417308800:79,Deployability,deploy,deployment,79,Shuffling reviewers since I'd like to get this merged soonish so I can turn on deployment for 0.1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4225#issuecomment-417308800
https://github.com/hail-is/hail/pull/4229#issuecomment-416789786:433,Performance,concurren,concurrency,433,"By looking at the full state on a dataset consist of 5000 copies of the same row, I've narrowed the search to this line giving the wrong values in the first row of `xdx` (not including upper left element) on a non-deterministic subset of rows despite `px` and `dpa` being correct:. ```; xdx(r0, r1) := px.t * dpa; ```. This is the only place `px` is used, and indeed, copying `px` is sufficient to fix the bug. I'd say it's a breeze concurrency bug, except that not only does it go away whenever no single computer processes more than one partition (i.e. big partition or single-core workers), it also doesn't occur on my laptop or google VM. I'd prefer a solution that avoids copying `px` per thread, since it's samples by covariates, rather than covariates by covariates (xdx) or covariates by 1 (xdy). But while I'm still working on what the heck is going on, I see no reason not to merge this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4229#issuecomment-416789786
https://github.com/hail-is/hail/pull/4229#issuecomment-416789786:670,Safety,avoid,avoids,670,"By looking at the full state on a dataset consist of 5000 copies of the same row, I've narrowed the search to this line giving the wrong values in the first row of `xdx` (not including upper left element) on a non-deterministic subset of rows despite `px` and `dpa` being correct:. ```; xdx(r0, r1) := px.t * dpa; ```. This is the only place `px` is used, and indeed, copying `px` is sufficient to fix the bug. I'd say it's a breeze concurrency bug, except that not only does it go away whenever no single computer processes more than one partition (i.e. big partition or single-core workers), it also doesn't occur on my laptop or google VM. I'd prefer a solution that avoids copying `px` per thread, since it's samples by covariates, rather than covariates by covariates (xdx) or covariates by 1 (xdy). But while I'm still working on what the heck is going on, I see no reason not to merge this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4229#issuecomment-416789786
https://github.com/hail-is/hail/pull/4229#issuecomment-416813716:105,Deployability,Update,Updated,105,"In fact, changing `px.t * dpa` to `(dpa.t * px).t` also resolves the bug, without the need to copy `px`. Updated accordingly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4229#issuecomment-416813716
https://github.com/hail-is/hail/pull/4235#issuecomment-417332020:39,Deployability,deploy,deployed,39,I was looking for a PR to be the first deployed ;),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4235#issuecomment-417332020
https://github.com/hail-is/hail/issues/4238#issuecomment-417320400:23,Performance,load,loading,23,"looks like this was on loading an int, which is obviously a little foolish. i don't think it's a bug then, but would be nice to be able to parse this if possible",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4238#issuecomment-417320400
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:52,Availability,error,error,52,"cc: @cseed @rcownie @catoverdrive . Amanda saw this error with these changes:. ```; amwang: fyi, this is the error the executor is seeing on a cluster with that jar:. ERROR: dlopen(""/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so""): /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:109,Availability,error,error,109,"cc: @cseed @rcownie @catoverdrive . Amanda saw this error with these changes:. ```; amwang: fyi, this is the error the executor is seeing on a cluster with that jar:. ERROR: dlopen(""/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so""): /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:167,Availability,ERROR,ERROR,167,"cc: @cseed @rcownie @catoverdrive . Amanda saw this error with these changes:. ```; amwang: fyi, this is the error the executor is seeing on a cluster with that jar:. ERROR: dlopen(""/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so""): /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3867,Availability,ERROR,ERROR,3867,"cala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:4529,Availability,Error,Error,4529,"cala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:4613,Availability,Error,Error,4613,"cala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3054,Energy Efficiency,schedul,scheduler,3054,"ge$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3125,Energy Efficiency,schedul,scheduler,3125,"RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNet",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:4114,Integrability,protocol,protocol,4114,"cala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:4586,Integrability,protocol,protocol,4586,"cala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:1514,Performance,load,load,1514,98654_0001_01_000002/tmp/libhail8271834084559267793.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:1622,Performance,load,loadLibrary,1622,Error: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(Context,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:1724,Performance,load,load,1724,r_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3248,Performance,concurren,concurrent,3248,"textRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3332,Performance,concurren,concurrent,3332,"DD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/jav",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186
https://github.com/hail-is/hail/pull/4239#issuecomment-417434486:8,Testability,test,tests,8,We need tests that actually run on dataproc.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417434486
https://github.com/hail-is/hail/pull/4239#issuecomment-417436405:70,Deployability,upgrade,upgrade,70,Fixing with a change to dataproc image version requires a cloud tools upgrade form all our users. Perhaps we should start a new latest-hash file and push a cloud tools update that looks there and uses `1.2-deb9`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417436405
https://github.com/hail-is/hail/pull/4239#issuecomment-417436405:168,Deployability,update,update,168,Fixing with a change to dataproc image version requires a cloud tools upgrade form all our users. Perhaps we should start a new latest-hash file and push a cloud tools update that looks there and uses `1.2-deb9`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417436405
https://github.com/hail-is/hail/pull/4239#issuecomment-417436405:135,Security,hash,hash,135,Fixing with a change to dataproc image version requires a cloud tools upgrade form all our users. Perhaps we should start a new latest-hash file and push a cloud tools update that looks there and uses `1.2-deb9`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417436405
https://github.com/hail-is/hail/pull/4239#issuecomment-417437115:85,Security,hash,hash,85,we should also add logic to cloudtools that checks if it's looking at an out-of-date hash,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417437115
https://github.com/hail-is/hail/pull/4239#issuecomment-417437115:19,Testability,log,logic,19,we should also add logic to cloudtools that checks if it's looking at an out-of-date hash,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417437115
https://github.com/hail-is/hail/pull/4240#issuecomment-417521786:296,Testability,test,testing,296,"I haven't looked into it, but it looks like the libhail for cpp5 got built differently than the other prebuilt .so in a way that's incompatible with dataproc. If we rebuild it on a dataproc head node, for example, it should probably be good to go. Before we do that, I would like to see dataproc testing get set up ... which of course @danking is already on: https://github.com/hail-is/hail/pull/4241. Nice!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-417521786
https://github.com/hail-is/hail/pull/4240#issuecomment-419237692:36,Testability,test,tests,36,@danking looks like this is passing tests on the newest version of cloudtools.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419237692
https://github.com/hail-is/hail/pull/4240#issuecomment-419488010:5,Testability,test,testing,5,"It's testing with the correct image, though?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419488010
https://github.com/hail-is/hail/pull/4240#issuecomment-419495529:4,Testability,log,log,4,The log shows cloud tools 1.1.16 was using 1.2-deb9 for the cluster image and I can confirm cloud tools 1.2.0 does as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419495529
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:86,Availability,avail,available,86,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:380,Availability,Down,Downloading,380,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:108,Deployability,install,installing,108,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:334,Deployability,install,install,334,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:535,Deployability,Install,Installing,535,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:591,Deployability,install,installed,591,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:9,Testability,Test,Tests,9,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:711,Usability,simpl,simple,711,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
https://github.com/hail-is/hail/pull/4240#issuecomment-419854054:86,Availability,avail,available,86,The problem is that `gsutil` only works with python2. So you'll need a python2 binary available as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419854054
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:1625,Availability,error,error,1625,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:105,Deployability,deploy,deployment,105,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:153,Deployability,deploy,deploy,153,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:177,Deployability,deploy,deploy,177,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:865,Deployability,release,release,865,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:1193,Deployability,deploy,deployed,1193,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:1655,Deployability,install,install,1655,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:74,Integrability,interface,interfaces,74,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:1631,Integrability,message,message,1631,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:504,Performance,queue,queue,504,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:317,Usability,guid,guide,317,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:1383,Usability,simpl,simple,1383,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:1639,Usability,guid,guiding,1639,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
https://github.com/hail-is/hail/pull/4241#issuecomment-417653146:35,Availability,robust,robust,35,"A checklist of things to make this robust:. - [x] https://github.com/Nealelab/cloudtools/issues/72; - [x] we need more permissions:; ```; ++ cluster start ci-test-4d8a9b262c3687f33359d92afdae693c819dfb09-e9e8a40bb4f0c2337e5088c26186a4da4948bed2 --version devel --spark 2.2.0 --jar build/libs/hail-all-spark.jar --zip build/distributions/hail-python.zip; ERROR: (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.; ```; - [x] be certain clusters don't stick around. I am not too concerned about the latter. We should look carefully, but it appears that, by default, processes on pods [get 30s notice via TERM](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) before they're killed. All `cluster` needs to do is to send google a termination request. Although the command takes forever to exit after `cluster stop`, this is because it waits for the cluster to shut down before returning. I regularly issue `cluster stop` and then force-kill the `cluster` command instead of waiting for the cluster to shutdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146
https://github.com/hail-is/hail/pull/4241#issuecomment-417653146:354,Availability,ERROR,ERROR,354,"A checklist of things to make this robust:. - [x] https://github.com/Nealelab/cloudtools/issues/72; - [x] we need more permissions:; ```; ++ cluster start ci-test-4d8a9b262c3687f33359d92afdae693c819dfb09-e9e8a40bb4f0c2337e5088c26186a4da4948bed2 --version devel --spark 2.2.0 --jar build/libs/hail-all-spark.jar --zip build/distributions/hail-python.zip; ERROR: (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.; ```; - [x] be certain clusters don't stick around. I am not too concerned about the latter. We should look carefully, but it appears that, by default, processes on pods [get 30s notice via TERM](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) before they're killed. All `cluster` needs to do is to send google a termination request. Although the command takes forever to exit after `cluster stop`, this is because it waits for the cluster to shut down before returning. I regularly issue `cluster stop` and then force-kill the `cluster` command instead of waiting for the cluster to shutdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146
https://github.com/hail-is/hail/pull/4241#issuecomment-417653146:939,Availability,down,down,939,"A checklist of things to make this robust:. - [x] https://github.com/Nealelab/cloudtools/issues/72; - [x] we need more permissions:; ```; ++ cluster start ci-test-4d8a9b262c3687f33359d92afdae693c819dfb09-e9e8a40bb4f0c2337e5088c26186a4da4948bed2 --version devel --spark 2.2.0 --jar build/libs/hail-all-spark.jar --zip build/distributions/hail-python.zip; ERROR: (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.; ```; - [x] be certain clusters don't stick around. I am not too concerned about the latter. We should look carefully, but it appears that, by default, processes on pods [get 30s notice via TERM](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) before they're killed. All `cluster` needs to do is to send google a termination request. Although the command takes forever to exit after `cluster stop`, this is because it waits for the cluster to shut down before returning. I regularly issue `cluster stop` and then force-kill the `cluster` command instead of waiting for the cluster to shutdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146
https://github.com/hail-is/hail/pull/4241#issuecomment-417653146:439,Security,authenticat,authentication,439,"A checklist of things to make this robust:. - [x] https://github.com/Nealelab/cloudtools/issues/72; - [x] we need more permissions:; ```; ++ cluster start ci-test-4d8a9b262c3687f33359d92afdae693c819dfb09-e9e8a40bb4f0c2337e5088c26186a4da4948bed2 --version devel --spark 2.2.0 --jar build/libs/hail-all-spark.jar --zip build/distributions/hail-python.zip; ERROR: (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.; ```; - [x] be certain clusters don't stick around. I am not too concerned about the latter. We should look carefully, but it appears that, by default, processes on pods [get 30s notice via TERM](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) before they're killed. All `cluster` needs to do is to send google a termination request. Although the command takes forever to exit after `cluster stop`, this is because it waits for the cluster to shut down before returning. I regularly issue `cluster stop` and then force-kill the `cluster` command instead of waiting for the cluster to shutdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146
https://github.com/hail-is/hail/pull/4241#issuecomment-417653146:158,Testability,test,test-,158,"A checklist of things to make this robust:. - [x] https://github.com/Nealelab/cloudtools/issues/72; - [x] we need more permissions:; ```; ++ cluster start ci-test-4d8a9b262c3687f33359d92afdae693c819dfb09-e9e8a40bb4f0c2337e5088c26186a4da4948bed2 --version devel --spark 2.2.0 --jar build/libs/hail-all-spark.jar --zip build/distributions/hail-python.zip; ERROR: (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.; ```; - [x] be certain clusters don't stick around. I am not too concerned about the latter. We should look carefully, but it appears that, by default, processes on pods [get 30s notice via TERM](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) before they're killed. All `cluster` needs to do is to send google a termination request. Although the command takes forever to exit after `cluster stop`, this is because it waits for the cluster to shut down before returning. I regularly issue `cluster stop` and then force-kill the `cluster` command instead of waiting for the cluster to shutdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146
https://github.com/hail-is/hail/pull/4241#issuecomment-417700063:50,Deployability,deploy,deployed,50,it will pass once the next cloud tools version is deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417700063
https://github.com/hail-is/hail/pull/4241#issuecomment-417791819:21,Testability,test,test,21,@cseed how about `ci-test-$(cat /dev/urandom | base64 | head -c 40)`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417791819
https://github.com/hail-is/hail/pull/4241#issuecomment-417796578:95,Availability,echo,echo,95,"base64 can contain some symbols, can the cluster names handle it? I normally use `tr`:. ```; $ echo `LC_CTYPE=C tr -dc 'a-z0-9' < /dev/urandom | head -c 8`; k1awyx7o; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417796578
https://github.com/hail-is/hail/pull/4241#issuecomment-417797285:32,Testability,test,test-,32,"And indeed:. > Cluster name 'ci-test-7ncBRTSJAgu1t8kTtltIXse5A1RwFFih0cIBma6T' must match pattern `(?:[a-z](?:[-a-z0-9]{0,49}[a-z0-9])?)`. And indeed, no upper case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417797285
https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:404,Availability,Down,Downloading,404,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:70,Deployability,upgrade,upgrade,70,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:358,Deployability,install,install,358,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:559,Deployability,Install,Installing,559,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:615,Deployability,install,installed,615,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:52,Usability,simpl,simply,52,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
https://github.com/hail-is/hail/pull/4241#issuecomment-418776942:100,Deployability,install,install,100,"I've seen this before, the PyPI databases are out of sync. You can see the latest with list but not install",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776942
https://github.com/hail-is/hail/pull/4241#issuecomment-418778114:29,Deployability,update,updated,29,hmm it seems to have finally updated,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418778114
https://github.com/hail-is/hail/pull/4242#issuecomment-417758456:36,Availability,error,errors,36,do we have tests that verify we get errors if we try to rekey with annotate_rows?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417758456
https://github.com/hail-is/hail/pull/4242#issuecomment-417758456:11,Testability,test,tests,11,do we have tests that verify we get errors if we try to rekey with annotate_rows?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417758456
https://github.com/hail-is/hail/pull/4242#issuecomment-417758586:72,Safety,safe,safety,72,I guess I mean really at the level of the IR. there are probably python safety belts on annotate_rows,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417758586
https://github.com/hail-is/hail/pull/4242#issuecomment-417780228:25,Availability,down,downstream,25,@patrick-schultz this is downstream of a `.entries()`. The latest errors indicate its a shuffle read error. Seems like shuffling our tiny 1kg downsample blows out spark's memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417780228
https://github.com/hail-is/hail/pull/4242#issuecomment-417780228:66,Availability,error,errors,66,@patrick-schultz this is downstream of a `.entries()`. The latest errors indicate its a shuffle read error. Seems like shuffling our tiny 1kg downsample blows out spark's memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417780228
https://github.com/hail-is/hail/pull/4242#issuecomment-417780228:101,Availability,error,error,101,@patrick-schultz this is downstream of a `.entries()`. The latest errors indicate its a shuffle read error. Seems like shuffling our tiny 1kg downsample blows out spark's memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417780228
https://github.com/hail-is/hail/pull/4242#issuecomment-417780228:142,Availability,down,downsample,142,@patrick-schultz this is downstream of a `.entries()`. The latest errors indicate its a shuffle read error. Seems like shuffling our tiny 1kg downsample blows out spark's memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417780228
https://github.com/hail-is/hail/pull/4242#issuecomment-417780296:31,Testability,test,test,31,I'm inclined to not change the test and fix the entries issue. this seems like a good smoke test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417780296
https://github.com/hail-is/hail/pull/4242#issuecomment-417780296:92,Testability,test,test,92,I'm inclined to not change the test and fix the entries issue. this seems like a good smoke test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417780296
https://github.com/hail-is/hail/pull/4243#issuecomment-417655757:30,Availability,failure,failure,30,I don't really understand the failure. Seems like the stack trace is missing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4243#issuecomment-417655757
https://github.com/hail-is/hail/pull/4247#issuecomment-423306621:24,Security,access,access,24,"@tpoterba I think write access is necessary to dismiss reviews. I rebased this branch, hopefully should be good to go now. Will probably be making some changes in the near future, but I think it should be fine in experimental for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4247#issuecomment-423306621
https://github.com/hail-is/hail/issues/4259#issuecomment-424833285:84,Usability,clear,clear,84,Is there something in particular you had in mind? The example that exists now seems clear to me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4259#issuecomment-424833285
https://github.com/hail-is/hail/pull/4262#issuecomment-418770000:14,Deployability,update,update,14,"we could also update to something from within the last 2 years, too",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4262#issuecomment-418770000
https://github.com/hail-is/hail/pull/4262#issuecomment-419147345:20,Testability,test,tests,20,"If it passes the CI tests, I'm cool with it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4262#issuecomment-419147345
https://github.com/hail-is/hail/issues/4263#issuecomment-418797402:0,Deployability,Pipeline,Pipeline,0,"Pipeline is a little involved but mostly annotate_rows(some_aggregators), followed by a group_cols_by().aggregate() -> annotate_rows(take())",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4263#issuecomment-418797402
https://github.com/hail-is/hail/pull/4265#issuecomment-419128948:15,Testability,test,test,15,added a struct test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4265#issuecomment-419128948
https://github.com/hail-is/hail/issues/4270#issuecomment-419929482:34,Safety,predict,predictable,34,"That's fair, although I would add predictable behavior to `interval_table[ht.key]` => Either first or last overlapping record of `interval_table`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4270#issuecomment-419929482
https://github.com/hail-is/hail/issues/4270#issuecomment-481240768:92,Energy Efficiency,efficient,efficiently,92,"Here's a little algorithm I think is a core piece of implementing producting interval joins efficiently. It takes an iterator of intervals, sorted lexicographically (by left endpoint, then right endpoint), and produces an iterator of `(Interval, Array[Interval])` pairs. The intervals `i` of the pairs `(i, a)` are guaranteed to be disjoint (and still sorted), and `a` will contain all intervals from the original iterator containing `i`. The algorithm maintains a min-heap of intervals, with the weight of an interval given by the right endpoint, i.e. the interval with least right endpoint is at the top of the heap. The following pseudocode shows how to produce each `(Interval, Array[Interval])` pair, given the input iterator `it`.; ```; i = it.next(); while (heap.top disjoint i); heap.pop(); heap.push(i); while (it.head.left == i.left); heap.push(it.next()); iOut := Interval(i.left, min(heap.top.right, it.head.left)); emit(iOut, heap.toArray); ```. It's easy to see that at the time of the last line, `heap` contains all and only those intervals which contain `iOut`, and that `iOut` is the intersection of the intervals in `heap`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4270#issuecomment-481240768
https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:77,Availability,error,error,77,"@jigold sorry about that CI frigged up, but things look good now, there's an error:; ```; =================================== FAILURES ===================================; ___________________ [doctest] hail.methods.impex.import_bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': exp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590
https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:126,Availability,FAILURE,FAILURES,126,"@jigold sorry about that CI frigged up, but things look good now, there's an error:; ```; =================================== FAILURES ===================================; ___________________ [doctest] hail.methods.impex.import_bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': exp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590
https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:1745,Integrability,wrap,wrapper,1745,"bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590
https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:567,Performance,Load,Load,567,"@jigold sorry about that CI frigged up, but things look good now, there's an error:; ```; =================================== FAILURES ===================================; ___________________ [doctest] hail.methods.impex.import_bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': exp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590
https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:1521,Testability,test,test,1521,"bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590
https://github.com/hail-is/hail/pull/4293#issuecomment-419658619:71,Deployability,pipeline,pipeline,71,"Nice!!! This appears to have skipped all the extraneous stages, and my pipeline goes straight to the write stage!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4293#issuecomment-419658619
https://github.com/hail-is/hail/pull/4301#issuecomment-420022480:15,Deployability,update,update,15,"FYI, I need to update developer getting started.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4301#issuecomment-420022480
https://github.com/hail-is/hail/pull/4303#issuecomment-420030281:38,Testability,log,log,38,"@gtiao here's how you can use this to log:. ```; ht.show(10, handler=lambda x: logging.info(x)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4303#issuecomment-420030281
https://github.com/hail-is/hail/pull/4303#issuecomment-420030281:79,Testability,log,logging,79,"@gtiao here's how you can use this to log:. ```; ht.show(10, handler=lambda x: logging.info(x)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4303#issuecomment-420030281
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:27,Availability,error,error,27,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:293,Availability,Error,Error,293,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:323,Performance,optimiz,optimization,323,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:146,Testability,test,test,146,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:207,Testability,test,test,207,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:379,Testability,test,test,379,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:467,Testability,test,test,467,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:649,Testability,test,test,649,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:747,Testability,test,test,747,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:904,Testability,test,test,904,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986
https://github.com/hail-is/hail/issues/4311#issuecomment-420252895:64,Testability,log,logic,64,the PR that concatenated global fields in table join didn't put logic in PruneDeadFields for that,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420252895
https://github.com/hail-is/hail/issues/4311#issuecomment-420261417:17,Availability,error,error,17,"another possible error - I think prune dead fields might be broken if you have a same-name row field on the left and right, and only use the `_1` version from the right table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420261417
https://github.com/hail-is/hail/issues/4311#issuecomment-420263190:79,Testability,assert,asserts,79,"Yeah, that should be fine. I'd propose redesigning the scala so that TableJoin asserts there are no field overlaps (globals or row fields), and to emit renames in Python. Then the suffix stuff is pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420263190
https://github.com/hail-is/hail/issues/4311#issuecomment-420263211:79,Testability,assert,asserts,79,"Yeah, that should be fine. I'd propose redesigning the scala so that TableJoin asserts there are no field overlaps (globals or row fields), and to emit renames in Python. Then the suffix stuff is pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420263211
https://github.com/hail-is/hail/issues/4314#issuecomment-420343076:29,Deployability,pipeline,pipeline,29,Just hit it again on another pipeline that has some `.key_by`s - this is now a showstopper â˜¹ï¸,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420343076
https://github.com/hail-is/hail/issues/4314#issuecomment-420405267:1158,Integrability,wrap,wrapper,1158,"); File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/constraint.py"", line 36, in main; recalculate_all_possible_summary=True, remove_common_downsampled=False, remove_common_ordinary=True); File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 263, in calculate_mu_by_downsampling; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 41, in get_old_mu_data; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 772, in transmute; File ""<decorator-gen-648>"", line 2, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/typecheck/check.py"", line 546, in wrapper; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 438, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 447, in _select_scala; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.convert.Wrappers$JListWrapper.length(Wrappers.scala:86); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.AbstractSeq.size(Seq.scala:41); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:285); 	at scala.collection.AbstractTraversable.toArray(Traversable.scala:104); 	at is.hail.utils.richUtils.RichIterable.toFastIndexedSeq(RichIterable.scala:83); 	at is.hail.table.Table.select(Table.scala:436",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267
https://github.com/hail-is/hail/issues/4314#issuecomment-420405267:1746,Integrability,Wrap,Wrappers,1746," ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 41, in get_old_mu_data; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 772, in transmute; File ""<decorator-gen-648>"", line 2, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/typecheck/check.py"", line 546, in wrapper; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 438, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 447, in _select_scala; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.convert.Wrappers$JListWrapper.length(Wrappers.scala:86); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.AbstractSeq.size(Seq.scala:41); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:285); 	at scala.collection.AbstractTraversable.toArray(Traversable.scala:104); 	at is.hail.utils.richUtils.RichIterable.toFastIndexedSeq(RichIterable.scala:83); 	at is.hail.table.Table.select(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267
https://github.com/hail-is/hail/issues/4314#issuecomment-420405267:1775,Integrability,Wrap,Wrappers,1775,"40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 41, in get_old_mu_data; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 772, in transmute; File ""<decorator-gen-648>"", line 2, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/typecheck/check.py"", line 546, in wrapper; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 438, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 447, in _select_scala; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.convert.Wrappers$JListWrapper.length(Wrappers.scala:86); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.AbstractSeq.size(Seq.scala:41); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:285); 	at scala.collection.AbstractTraversable.toArray(Traversable.scala:104); 	at is.hail.utils.richUtils.RichIterable.toFastIndexedSeq(RichIterable.scala:83); 	at is.hail.table.Table.select(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267
https://github.com/hail-is/hail/issues/4314#issuecomment-420405426:12,Usability,simpl,simple,12,"that one is simple:; ```; def get_old_mu_data() -> hl.Table:; old_mu_data = hl.import_table('gs://gnomad-resources/constraint/source/fordist_1KG_mutation_rate_table.txt',; delimiter=' ', impute=True); return old_mu_data.transmute(context=old_mu_data['from'], ref=old_mu_data['from'][1],; alt=old_mu_data.to[1]).key_by('context', 'ref', 'alt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405426
https://github.com/hail-is/hail/pull/4320#issuecomment-420689376:148,Safety,timeout,timeouts,148,"definitely has an infinite loop, job was running for 13 hours. I manually killed it. Issue https://github.com/hail-is/ci/issues/91 created to track timeouts on jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4320#issuecomment-420689376
https://github.com/hail-is/hail/pull/4320#issuecomment-420750223:238,Availability,error,error,238,"Some of the array tests (min, max, etc.) are failing because they rely on the `If` to protect from calling `ArrayRef` on a zero-length array. :-| I'm not sure if you can mitigate this by just not folding along nodes that might reasonably error, like `ArrayRef`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4320#issuecomment-420750223
https://github.com/hail-is/hail/pull/4320#issuecomment-420750223:18,Testability,test,tests,18,"Some of the array tests (min, max, etc.) are failing because they rely on the `If` to protect from calling `ArrayRef` on a zero-length array. :-| I'm not sure if you can mitigate this by just not folding along nodes that might reasonably error, like `ArrayRef`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4320#issuecomment-420750223
https://github.com/hail-is/hail/pull/4326#issuecomment-422401985:16,Testability,test,tests,16,Changed list of tests in hail/build.gradle testCppCodegen,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4326#issuecomment-422401985
https://github.com/hail-is/hail/pull/4326#issuecomment-422401985:43,Testability,test,testCppCodegen,43,Changed list of tests in hail/build.gradle testCppCodegen,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4326#issuecomment-422401985
https://github.com/hail-is/hail/pull/4328#issuecomment-422128214:178,Deployability,deploy,deployed,178,"@cseed took a different approach, generating a python file which I import in hail initialization. This seemed like the only way to make things work in all the ways Python can be deployed (zip, egg, directory)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4328#issuecomment-422128214
https://github.com/hail-is/hail/pull/4332#issuecomment-421074006:99,Deployability,release,released,99,for future PRs can you make a branch on a forked repo and PR in from there? It's nice to have only released branches on the main repo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4332#issuecomment-421074006
https://github.com/hail-is/hail/issues/4338#issuecomment-421403518:36,Integrability,rout,routine,36,"If you do build a sites-only export routine, now's the time to request it work on Tables in addition to (or instead of, doesn't matter too much) MatrixTables",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4338#issuecomment-421403518
https://github.com/hail-is/hail/issues/4340#issuecomment-421627054:26,Deployability,deploy,deploy,26,"er, maybe a bit longer to deploy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4340#issuecomment-421627054
https://github.com/hail-is/hail/pull/4347#issuecomment-422132532:75,Deployability,update,update,75,Looks like this failed due to the wrong init script. Probably just need to update cloudtools on the CI server?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422132532
https://github.com/hail-is/hail/pull/4347#issuecomment-422132872:65,Deployability,update,update,65,"Yeah, Dan is fixing the underlying CI issue regarding cloudtools update, hopefully it then works",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422132872
https://github.com/hail-is/hail/pull/4347#issuecomment-422134593:382,Availability,avail,available,382,"@konradjk pssh, what is this ""server"" you speak of. The kubernetes pod indeed needs a newer version of cloud tools. We always grab the latest when running the hail PR jobs. Unfortunately the deploy for cloud tools on python3 was broken. That's being fixed by https://github.com/Nealelab/cloudtools/pull/101. `pip`, unhelpfully, tells you the latest version is X.Y even if X.Y isn't available for your version of python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422134593
https://github.com/hail-is/hail/pull/4347#issuecomment-422134593:191,Deployability,deploy,deploy,191,"@konradjk pssh, what is this ""server"" you speak of. The kubernetes pod indeed needs a newer version of cloud tools. We always grab the latest when running the hail PR jobs. Unfortunately the deploy for cloud tools on python3 was broken. That's being fixed by https://github.com/Nealelab/cloudtools/pull/101. `pip`, unhelpfully, tells you the latest version is X.Y even if X.Y isn't available for your version of python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422134593
https://github.com/hail-is/hail/pull/4347#issuecomment-422170789:90,Availability,error,error,90,"Awesome. God, how long was this in coming. No for this PR, but I observe this seems a bit error prone:. > --vep; > actual = hl.vep(expected.select_rows(), 'gs://hail-common/vep/vep/vep85-loftee-gcloud.json', csq=csq). We should probably think a bit more about the vep/cloudtools interface after this. I'm thinking `cloudtools --vep --vep-version=85 --vep-assembly=GRCh37 --loffee-version=beta ...` and then just `hl.vep(foo, csq=csq)` where the properties file defaults to something set up by cloudtools. @konradjk?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422170789
https://github.com/hail-is/hail/pull/4347#issuecomment-422170789:279,Integrability,interface,interface,279,"Awesome. God, how long was this in coming. No for this PR, but I observe this seems a bit error prone:. > --vep; > actual = hl.vep(expected.select_rows(), 'gs://hail-common/vep/vep/vep85-loftee-gcloud.json', csq=csq). We should probably think a bit more about the vep/cloudtools interface after this. I'm thinking `cloudtools --vep --vep-version=85 --vep-assembly=GRCh37 --loffee-version=beta ...` and then just `hl.vep(foo, csq=csq)` where the properties file defaults to something set up by cloudtools. @konradjk?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422170789
https://github.com/hail-is/hail/pull/4361#issuecomment-423248241:40,Testability,assert,assertion,40,"looks like your change is breaking this assertion on VEP line 84:; ` assert(ht.key.contains(FastIndexedSeq(""locus"", ""alleles"")))`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4361#issuecomment-423248241
https://github.com/hail-is/hail/pull/4361#issuecomment-423248241:69,Testability,assert,assert,69,"looks like your change is breaking this assertion on VEP line 84:; ` assert(ht.key.contains(FastIndexedSeq(""locus"", ""alleles"")))`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4361#issuecomment-423248241
https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:317,Availability,avail,available,317,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734
https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:502,Modifiability,layers,layers,502,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734
https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:629,Modifiability,layers,layers,629,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734
https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:199,Performance,load,load,199,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734
https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:370,Performance,cache,cache,370,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734
https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:580,Performance,cache,cache-from,580,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734
https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:693,Performance,cache,cache-from,693,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734
https://github.com/hail-is/hail/pull/4367#issuecomment-423307651:27,Modifiability,rewrite,rewrites,27,"Okay, I have a branch that rewrites `TableOrderBy` to use the same mechanism we use for key ordering. It's based on #4354, since there I factored out the pieces that I'm reusing in `TableOrderBy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4367#issuecomment-423307651
https://github.com/hail-is/hail/pull/4380#issuecomment-423268427:37,Availability,failure,failure,37,oops. thanks for preempting the test failure!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4380#issuecomment-423268427
https://github.com/hail-is/hail/pull/4380#issuecomment-423268427:32,Testability,test,test,32,oops. thanks for preempting the test failure!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4380#issuecomment-423268427
https://github.com/hail-is/hail/pull/4383#issuecomment-423410409:16,Usability,clear,clear,16,"ugh. just to be clear, this is just removing the `:`s right? (to force converting to tables most of the time). but you are planning on keeping `[]` generally (maybe as shorthand if you suggest generally using `index_*`)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4383#issuecomment-423410409
https://github.com/hail-is/hail/issues/4394#issuecomment-423657075:8,Availability,fault,fault,8,oops my fault.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4394#issuecomment-423657075
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:543,Deployability,update,update,543,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:359,Integrability,wrap,wrapper,359,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:47,Performance,queue,queue,47,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:11,Testability,benchmark,benchmark,11,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:136,Testability,benchmark,benchmark,136,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:207,Testability,benchmark,benchmark,207,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:630,Testability,Test,Test,630,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:690,Testability,benchmark,benchmark,690,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101
https://github.com/hail-is/hail/pull/4396#issuecomment-434065807:26,Testability,benchmark,benchmark,26,Bump on this. Request for benchmark was 20 days ago.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-434065807
https://github.com/hail-is/hail/pull/4396#issuecomment-435198817:39,Deployability,update,updates,39,Alright. There's been some significant updates since this was last touched. I'm closing this and will be opening a new series of PRs with the new end product.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-435198817
https://github.com/hail-is/hail/pull/4400#issuecomment-423773748:7,Deployability,update,updated,7,Also I updated the pr-builder base image to miniconda3 (needs-redeploy.py requires Python 3) and made the version explicit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4400#issuecomment-423773748
https://github.com/hail-is/hail/pull/4404#issuecomment-423779185:101,Deployability,deploy,deploys,101,"Oops, I messed up the last PR, I forget the ""3"" in ""miniconda3"" in the build image Docker. I presume deploys are broken now, but I haven't checked. Rebuilding a new docker image now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4404#issuecomment-423779185
https://github.com/hail-is/hail/pull/4408#issuecomment-424187841:39,Deployability,deploy,deploy,39,"Pushed an additional commit:; - try to deploy all projects from the toplevel file; - at toplevel, only deploy if project has changed,; - added new build convention `<project>/get-deployed-sha.sh` which should output the current deployed sha for the project (if there is one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841
https://github.com/hail-is/hail/pull/4408#issuecomment-424187841:103,Deployability,deploy,deploy,103,"Pushed an additional commit:; - try to deploy all projects from the toplevel file; - at toplevel, only deploy if project has changed,; - added new build convention `<project>/get-deployed-sha.sh` which should output the current deployed sha for the project (if there is one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841
https://github.com/hail-is/hail/pull/4408#issuecomment-424187841:179,Deployability,deploy,deployed-sha,179,"Pushed an additional commit:; - try to deploy all projects from the toplevel file; - at toplevel, only deploy if project has changed,; - added new build convention `<project>/get-deployed-sha.sh` which should output the current deployed sha for the project (if there is one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841
https://github.com/hail-is/hail/pull/4408#issuecomment-424187841:228,Deployability,deploy,deployed,228,"Pushed an additional commit:; - try to deploy all projects from the toplevel file; - at toplevel, only deploy if project has changed,; - added new build convention `<project>/get-deployed-sha.sh` which should output the current deployed sha for the project (if there is one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841
https://github.com/hail-is/hail/pull/4408#issuecomment-424188007:28,Deployability,deploy,deployment,28,"FYI @danking for setting up deployment in ci, last comment in particular.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424188007
https://github.com/hail-is/hail/pull/4408#issuecomment-424304088:11,Deployability,deploy,deployment,11,Also added deployment for scorecard. I'm done making changes now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424304088
https://github.com/hail-is/hail/pull/4414#issuecomment-424099229:156,Deployability,deploy,deploy,156,"I pushed another commit. Toplevel hail-ci-build.sh handles all known projects (and skips them if they aren't there yet or don't have a build script). Also, deploy batch only when changed. I will also make this generic by adding a get-deployed-sha.sh to retrieve the current deployed sha to compare against when considering redeploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4414#issuecomment-424099229
https://github.com/hail-is/hail/pull/4414#issuecomment-424099229:234,Deployability,deploy,deployed-sha,234,"I pushed another commit. Toplevel hail-ci-build.sh handles all known projects (and skips them if they aren't there yet or don't have a build script). Also, deploy batch only when changed. I will also make this generic by adding a get-deployed-sha.sh to retrieve the current deployed sha to compare against when considering redeploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4414#issuecomment-424099229
https://github.com/hail-is/hail/pull/4414#issuecomment-424099229:274,Deployability,deploy,deployed,274,"I pushed another commit. Toplevel hail-ci-build.sh handles all known projects (and skips them if they aren't there yet or don't have a build script). Also, deploy batch only when changed. I will also make this generic by adding a get-deployed-sha.sh to retrieve the current deployed sha to compare against when considering redeploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4414#issuecomment-424099229
https://github.com/hail-is/hail/issues/4418#issuecomment-454131636:29,Deployability,pipeline,pipelines,29,"I haven't run very many Hail pipelines since ASHG, so there hasn't been much opportunity to see this bug. Sorry I can't help more!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4418#issuecomment-454131636
https://github.com/hail-is/hail/pull/4421#issuecomment-424189518:78,Testability,log,logs,78,"@tpoterba, these are great. We should create a service that accepts and write logs so we can write a command line `hl.send_log_to_hail_team()`. (Maybe shorter.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4421#issuecomment-424189518
https://github.com/hail-is/hail/pull/4422#issuecomment-424477261:202,Deployability,install,installed,202,"Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1; on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). The dataproc nodes have g++ and make already installed, so this PR should suffice to; make them work with C++ codegen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424477261
https://github.com/hail-is/hail/pull/4422#issuecomment-424477261:48,Testability,test,tests,48,"Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1; on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). The dataproc nodes have g++ and make already installed, so this PR should suffice to; make them work with C++ codegen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424477261
https://github.com/hail-is/hail/pull/4422#issuecomment-424744733:493,Availability,avail,available,493,"1. It's feasible to build and ship the compiler + libraries for a limited number of known platforms; (at Physics Speed I did this for Ubuntu-16.04 and one particular version of CentOS). It gets nuts if; you have many different OS'es each of which needs its own compiler build (and it then becomes; another build-system/packaging issue to get all those compilers built correctly for each OS).; Possibly a good thing to do in the long run. Probably not something I could do in the limited time; available. 2. If you build your own compiler + library, then you risk becoming incompatible with other ; libraries on the target system which were built against that system's ""standard"" compiler; and library and header files. e.g. BLAS. [Though this only applies to libraries compiled from C++,; not libraries in C, which might conatin the damage]. So it seemed like the least disruptive path in the short term was to excise the few uses of; std::string and std::stringstream, so that we can build a libhail.so which should work across; a wide variety of Linux systems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733
https://github.com/hail-is/hail/pull/4422#issuecomment-424744733:558,Safety,risk,risk,558,"1. It's feasible to build and ship the compiler + libraries for a limited number of known platforms; (at Physics Speed I did this for Ubuntu-16.04 and one particular version of CentOS). It gets nuts if; you have many different OS'es each of which needs its own compiler build (and it then becomes; another build-system/packaging issue to get all those compilers built correctly for each OS).; Possibly a good thing to do in the long run. Probably not something I could do in the limited time; available. 2. If you build your own compiler + library, then you risk becoming incompatible with other ; libraries on the target system which were built against that system's ""standard"" compiler; and library and header files. e.g. BLAS. [Though this only applies to libraries compiled from C++,; not libraries in C, which might conatin the damage]. So it seemed like the least disruptive path in the short term was to excise the few uses of; std::string and std::stringstream, so that we can build a libhail.so which should work across; a wide variety of Linux systems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:837,Availability,avail,available,837,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:604,Integrability,depend,dependencies,604,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:518,Safety,risk,risk,518,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:1046,Security,access,access,1046,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:161,Testability,Test,Testing,161,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:1146,Testability,test,tests,1146,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:1262,Testability,test,test,1262,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:1357,Testability,test,tests,1357,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448
https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:874,Integrability,interoperab,interoperable,874,">We only have to support two platforms: recent Linux and OSX. OSX is not a problem, all recent versions are based on libc++ (rather than libstdc++) which has; had a more stable ABI. The problem is precisely that ""recent Linux"" encompasses both; systems based on g++-4.8.x/4.9.x with only old-ABI std::string's (e.g. debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941
https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:1438,Integrability,interface,interfaceswhich,1438,"ing's (e.g. debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn't cause trouble.; ; In short, it's a can of worms. Avoiding std::string in libhail.so keeps the can closed for now.; And I believe dataproc will move to using debian9 images as the default in November, so at; some point the need to support old-ABI systems (debian8) will diminish and possibly go away c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941
https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:1743,Integrability,interoperab,interoperability,1743,". debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn't cause trouble.; ; In short, it's a can of worms. Avoiding std::string in libhail.so keeps the can closed for now.; And I believe dataproc will move to using debian9 images as the default in November, so at; some point the need to support old-ABI systems (debian8) will diminish and possibly go away completely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941
https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:2053,Safety,Avoid,Avoiding,2053,". debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn't cause trouble.; ; In short, it's a can of worms. Avoiding std::string in libhail.so keeps the can closed for now.; And I believe dataproc will move to using debian9 images as the default in November, so at; some point the need to support old-ABI systems (debian8) will diminish and possibly go away completely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941
https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:468,Security,access,access,468,">We only have to support two platforms: recent Linux and OSX. OSX is not a problem, all recent versions are based on libc++ (rather than libstdc++) which has; had a more stable ABI. The problem is precisely that ""recent Linux"" encompasses both; systems based on g++-4.8.x/4.9.x with only old-ABI std::string's (e.g. debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941
https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:538,Security,access,access,538,">We only have to support two platforms: recent Linux and OSX. OSX is not a problem, all recent versions are based on libc++ (rather than libstdc++) which has; had a more stable ABI. The problem is precisely that ""recent Linux"" encompasses both; systems based on g++-4.8.x/4.9.x with only old-ABI std::string's (e.g. debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941
https://github.com/hail-is/hail/pull/4422#issuecomment-424794222:426,Safety,Avoid,Avoiding,426,"> And there's a slight question about whether your libstdc++ will work against the other systems libc.so. Is there something special about libstdc++ here? Our code will certainly call libc directly, too. As far as I know there no proposal on the table handles incompatible libc's. I think we'd have to make multiple builds. Luckily, as far as I know, recent distributions have compatible libc so this shouldn't be an issue. > Avoiding std::string in libhail.so keeps the can closed for now. Can you explain why std::string is special here vs the rest of the standard library? What assumption are you working under?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424794222
https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:978,Deployability,release,release,978," is special here vs the rest of the standard library? What assumption >are you working under?. Yes. The C++11 final standard introduced some new constraints on the definition of std::string; and std::list (essentially outlawing a copy-on-write implementation of std::string, and requiring; that std::list::size() must be O(1)). Consequently libstdc++ had to be redesigned with incompatible implementations of ; std::string and std::list to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My und",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612
https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:2353,Deployability,release,release,2353,"to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My understanding is that it is perfectly feasible to mix and match different g++/clang++compiler; versions and different libstdc++ versions. It just doesn't happen very much because both the; g++ version and the libstdc++ are chosen at the same time, early in building a Linux distribution,; and then are frozen throughout the release's lifetime to ensure interoperability of binaries. So debian8's version of g++ and libstdc++ doesn't change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612
https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:835,Integrability,interface,interfaces,835,">Can you explain why std::string is special here vs the rest of the standard library? What assumption >are you working under?. Yes. The C++11 final standard introduced some new constraints on the definition of std::string; and std::list (essentially outlawing a copy-on-write implementation of std::string, and requiring; that std::list::size() must be O(1)). Consequently libstdc++ had to be redesigned with incompatible implementations of ; std::string and std::list to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612
https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:2382,Integrability,interoperab,interoperability,2382,"to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My understanding is that it is perfectly feasible to mix and match different g++/clang++compiler; versions and different libstdc++ versions. It just doesn't happen very much because both the; g++ version and the libstdc++ are chosen at the same time, early in building a Linux distribution,; and then are frozen throughout the release's lifetime to ensure interoperability of binaries. So debian8's version of g++ and libstdc++ doesn't change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612
https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:1941,Modifiability,rewrite,rewrite-from-scratch,1941,"to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My understanding is that it is perfectly feasible to mix and match different g++/clang++compiler; versions and different libstdc++ versions. It just doesn't happen very much because both the; g++ version and the libstdc++ are chosen at the same time, early in building a Linux distribution,; and then are frozen throughout the release's lifetime to ensure interoperability of binaries. So debian8's version of g++ and libstdc++ doesn't change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612
https://github.com/hail-is/hail/pull/4422#issuecomment-424835504:373,Deployability,deploy,deployed,373,">Did you test submitting jobs to the cluster itself? This can be quite a different environment than the >tests. Not as such. I ssh'ed in to the master node (after explicitly running a gcloud command to get a cluster with the 1.2 image rather than the default 1.2-deb9). I wasn't confident about ; how to get my own hail.jar to run in the cluster environment instead of the deployed version.; But since NativeModule.cpp is the biggest user of string's, I confirmed that we could run tests; *and* see codegen'ed files showing up in /tmp/hail_*/*.cpp, indicating that C++ decoders were; being generated and were working correctly. >Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we >submit cluster_sanity_check.py with and without C++ codegen enabled?. I don't have a specific plan, just imagined that it wouldn't be difficult to arrange, and I don't; have any particular opinion about the best way to control it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424835504
https://github.com/hail-is/hail/pull/4422#issuecomment-424835504:9,Testability,test,test,9,">Did you test submitting jobs to the cluster itself? This can be quite a different environment than the >tests. Not as such. I ssh'ed in to the master node (after explicitly running a gcloud command to get a cluster with the 1.2 image rather than the default 1.2-deb9). I wasn't confident about ; how to get my own hail.jar to run in the cluster environment instead of the deployed version.; But since NativeModule.cpp is the biggest user of string's, I confirmed that we could run tests; *and* see codegen'ed files showing up in /tmp/hail_*/*.cpp, indicating that C++ decoders were; being generated and were working correctly. >Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we >submit cluster_sanity_check.py with and without C++ codegen enabled?. I don't have a specific plan, just imagined that it wouldn't be difficult to arrange, and I don't; have any particular opinion about the best way to control it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424835504
https://github.com/hail-is/hail/pull/4422#issuecomment-424835504:105,Testability,test,tests,105,">Did you test submitting jobs to the cluster itself? This can be quite a different environment than the >tests. Not as such. I ssh'ed in to the master node (after explicitly running a gcloud command to get a cluster with the 1.2 image rather than the default 1.2-deb9). I wasn't confident about ; how to get my own hail.jar to run in the cluster environment instead of the deployed version.; But since NativeModule.cpp is the biggest user of string's, I confirmed that we could run tests; *and* see codegen'ed files showing up in /tmp/hail_*/*.cpp, indicating that C++ decoders were; being generated and were working correctly. >Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we >submit cluster_sanity_check.py with and without C++ codegen enabled?. I don't have a specific plan, just imagined that it wouldn't be difficult to arrange, and I don't; have any particular opinion about the best way to control it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424835504
https://github.com/hail-is/hail/pull/4422#issuecomment-424835504:482,Testability,test,tests,482,">Did you test submitting jobs to the cluster itself? This can be quite a different environment than the >tests. Not as such. I ssh'ed in to the master node (after explicitly running a gcloud command to get a cluster with the 1.2 image rather than the default 1.2-deb9). I wasn't confident about ; how to get my own hail.jar to run in the cluster environment instead of the deployed version.; But since NativeModule.cpp is the biggest user of string's, I confirmed that we could run tests; *and* see codegen'ed files showing up in /tmp/hail_*/*.cpp, indicating that C++ decoders were; being generated and were working correctly. >Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we >submit cluster_sanity_check.py with and without C++ codegen enabled?. I don't have a specific plan, just imagined that it wouldn't be difficult to arrange, and I don't; have any particular opinion about the best way to control it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424835504
https://github.com/hail-is/hail/pull/4427#issuecomment-424465040:0,Availability,ping,ping,0,ping @cseed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4427#issuecomment-424465040
https://github.com/hail-is/hail/pull/4429#issuecomment-424416553:170,Deployability,deploy,deploy,170,"Erm. This won't work unless I add the nginx package from apt to the pr-builder image. I guess that's better than nothing? But really I want to use the docker image we'll deploy. Once we're building docker images ourselves, this will be much easier.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4429#issuecomment-424416553
https://github.com/hail-is/hail/issues/4437#issuecomment-424558979:56,Deployability,deploy,deploy,56,"In particular, it should link to the index.html for the deploy,e.g. `gs://hail-ci-0-1/deploy/f69da9402030173e68b33608f38d94c45a1d7928/index.html`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-424558979
https://github.com/hail-is/hail/issues/4437#issuecomment-424558979:86,Deployability,deploy,deploy,86,"In particular, it should link to the index.html for the deploy,e.g. `gs://hail-ci-0-1/deploy/f69da9402030173e68b33608f38d94c45a1d7928/index.html`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-424558979
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:78,Deployability,deploy,deploy,78,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:112,Deployability,deploy,deploy,112,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:55,Testability,test,tests,55,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:61,Testability,log,logs,61,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:85,Testability,log,logs,85,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:119,Testability,log,logs,119,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:138,Testability,test,test,138,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:143,Testability,log,logs,143,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286
https://github.com/hail-is/hail/issues/4437#issuecomment-438077079:37,Testability,log,log,37,"ah, I see. Do we currently keep that log file in GS?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438077079
https://github.com/hail-is/hail/issues/4437#issuecomment-438146859:53,Deployability,deploy,deploy,53,"Yes, for example:. ```; $ gsutil ls gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/index.html; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/job.log; ```. where the hash is from master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859
https://github.com/hail-is/hail/issues/4437#issuecomment-438146859:120,Deployability,deploy,deploy,120,"Yes, for example:. ```; $ gsutil ls gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/index.html; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/job.log; ```. where the hash is from master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859
https://github.com/hail-is/hail/issues/4437#issuecomment-438146859:197,Deployability,deploy,deploy,197,"Yes, for example:. ```; $ gsutil ls gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/index.html; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/job.log; ```. where the hash is from master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859
https://github.com/hail-is/hail/issues/4437#issuecomment-438146859:269,Security,hash,hash,269,"Yes, for example:. ```; $ gsutil ls gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/index.html; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/job.log; ```. where the hash is from master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859
https://github.com/hail-is/hail/issues/4437#issuecomment-438146859:249,Testability,log,log,249,"Yes, for example:. ```; $ gsutil ls gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/index.html; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/job.log; ```. where the hash is from master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859
https://github.com/hail-is/hail/issues/4439#issuecomment-424486040:57,Testability,test,test,57,> We should run R's PCRelate on a few randomly generated test datasets and save them into hail's repo and use them for testing. omg yes please,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4439#issuecomment-424486040
https://github.com/hail-is/hail/issues/4439#issuecomment-424486040:119,Testability,test,testing,119,> We should run R's PCRelate on a few randomly generated test datasets and save them into hail's repo and use them for testing. omg yes please,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4439#issuecomment-424486040
https://github.com/hail-is/hail/pull/4442#issuecomment-424518782:29,Deployability,deploy,deploy,29,"wait what, how did this auto-deploy?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4442#issuecomment-424518782
https://github.com/hail-is/hail/pull/4442#issuecomment-424543053:49,Deployability,deploy,deployment,49,"Sorry, the title is wrong, this fixes hail batch deployment",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4442#issuecomment-424543053
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:901,Availability,Error,Error,901,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:47,Deployability,deploy,deploy,47,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:316,Deployability,configurat,configuration,316,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:335,Deployability,update,updated,335,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:291,Modifiability,config,configure-docker,291,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:316,Modifiability,config,configuration,316,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:563,Performance,cache,cachefrom,563,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:73,Testability,log,log,73,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708
https://github.com/hail-is/hail/pull/4446#issuecomment-424727330:5,Deployability,deploy,deploy,5,I'll deploy to cluster by hand when this is merged,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424727330
https://github.com/hail-is/hail/pull/4446#issuecomment-424728688:26,Deployability,deploy,deploy,26,You want me to set up the deploy logic to match batch and the other services?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424728688
https://github.com/hail-is/hail/pull/4446#issuecomment-424728688:33,Testability,log,logic,33,You want me to set up the deploy logic to match batch and the other services?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424728688
https://github.com/hail-is/hail/pull/4446#issuecomment-424729339:52,Deployability,deploy,deployable,52,"Sure! Now that we have docker working, CI should be deployable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424729339
https://github.com/hail-is/hail/pull/4448#issuecomment-424758306:17,Availability,failure,failure,17,introduced dummy failure to test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4448#issuecomment-424758306
https://github.com/hail-is/hail/pull/4448#issuecomment-424758306:28,Testability,test,test,28,introduced dummy failure to test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4448#issuecomment-424758306
https://github.com/hail-is/hail/pull/4449#issuecomment-424771189:45,Testability,test,testing,45,"bump @tpoterba, the churn is interrupting my testing of CI-in-CI :/.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4449#issuecomment-424771189
https://github.com/hail-is/hail/pull/4449#issuecomment-424797229:53,Deployability,deploy,deploy,53,I'm gonna force merge this since it only changes the deploy script and I'm not sure the PR build job will finish before the deploy job re-deploys.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4449#issuecomment-424797229
https://github.com/hail-is/hail/pull/4449#issuecomment-424797229:124,Deployability,deploy,deploy,124,I'm gonna force merge this since it only changes the deploy script and I'm not sure the PR build job will finish before the deploy job re-deploys.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4449#issuecomment-424797229
https://github.com/hail-is/hail/pull/4449#issuecomment-424797229:138,Deployability,deploy,deploys,138,I'm gonna force merge this since it only changes the deploy script and I'm not sure the PR build job will finish before the deploy job re-deploys.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4449#issuecomment-424797229
https://github.com/hail-is/hail/pull/4458#issuecomment-425258445:116,Testability,test,test,116,"one more:; ```; File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs); File ""<doctest genetics.rst[21]>"", line 1; female_pheno = hl.case(); ^; SyntaxError: multiple statements found while compiling a single statement; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4458#issuecomment-425258445
https://github.com/hail-is/hail/pull/4459#issuecomment-424869066:196,Testability,log,log,196,@tpoterba this only changed the site so there were no artifacts https://storage.googleapis.com/hail-ci-0-1/ci/3a49ac588e2ffd20f6cee2fbc795b3b1fb668f3a/81c08e2d6376d6ebdf2f9e5c990137cd3ec6fdb3/job-log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4459#issuecomment-424869066
https://github.com/hail-is/hail/pull/4465#issuecomment-425114995:53,Availability,error,error,53,Returning null from the JSON parsing was throwing an error a few lines below when we try to parse a variant. This will throw the real error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4465#issuecomment-425114995
https://github.com/hail-is/hail/pull/4465#issuecomment-425114995:134,Availability,error,error,134,Returning null from the JSON parsing was throwing an error a few lines below when we try to parse a variant. This will throw the real error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4465#issuecomment-425114995
https://github.com/hail-is/hail/pull/4465#issuecomment-425601550:121,Integrability,wrap,wrap,121,"I did that originally and then switched to the state in the last commit, because of a preference for try/catch blocks to wrap the minimal amount of code for clarity",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4465#issuecomment-425601550
https://github.com/hail-is/hail/pull/4474#issuecomment-425942111:16,Testability,test,test,16,"Hmm. One of the test PR builds appears to have taken 6 minutes to run. These [don't really do anything](https://github.com/hail-is/hail/blob/master/ci/test-repo/hail-ci-build.sh), so it's likely the k8s nodes were overloaded and it couldn't start the job for a while. `kubectl describe pod job-29-z5mpw` (based on the job number in the logs, I'm pretty sure this is the pod in question), doesn't show any useful information.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4474#issuecomment-425942111
https://github.com/hail-is/hail/pull/4474#issuecomment-425942111:151,Testability,test,test-repo,151,"Hmm. One of the test PR builds appears to have taken 6 minutes to run. These [don't really do anything](https://github.com/hail-is/hail/blob/master/ci/test-repo/hail-ci-build.sh), so it's likely the k8s nodes were overloaded and it couldn't start the job for a while. `kubectl describe pod job-29-z5mpw` (based on the job number in the logs, I'm pretty sure this is the pod in question), doesn't show any useful information.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4474#issuecomment-425942111
https://github.com/hail-is/hail/pull/4474#issuecomment-425942111:336,Testability,log,logs,336,"Hmm. One of the test PR builds appears to have taken 6 minutes to run. These [don't really do anything](https://github.com/hail-is/hail/blob/master/ci/test-repo/hail-ci-build.sh), so it's likely the k8s nodes were overloaded and it couldn't start the job for a while. `kubectl describe pod job-29-z5mpw` (based on the job number in the logs, I'm pretty sure this is the pod in question), doesn't show any useful information.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4474#issuecomment-425942111
https://github.com/hail-is/hail/issues/4479#issuecomment-425769239:84,Safety,predict,predicting,84,"Agreed, this is good for useability. note itâ€™s also a special case of simple linreg predicting y from x (we return the square r_sq. The sign is that of beta).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4479#issuecomment-425769239
https://github.com/hail-is/hail/issues/4479#issuecomment-425769239:70,Usability,simpl,simple,70,"Agreed, this is good for useability. note itâ€™s also a special case of simple linreg predicting y from x (we return the square r_sq. The sign is that of beta).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4479#issuecomment-425769239
https://github.com/hail-is/hail/issues/4483#issuecomment-428374032:51,Deployability,pipeline,pipelines,51,bumping this because it's probably killing certain pipelines,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4483#issuecomment-428374032
https://github.com/hail-is/hail/issues/4483#issuecomment-442165921:11,Performance,optimiz,optimizing,11,We are now optimizing after the lift.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4483#issuecomment-442165921
https://github.com/hail-is/hail/pull/4484#issuecomment-426097277:280,Performance,bottleneck,bottlenecked,280,"That test with `10000` fields takes about two minutes, so I didn't want to add that to the test suite. Although two minutes seems insanely slow, it's better than the quadratic scaling provided by the old implementation of `Infer(InsertFields(...))`. Moreover, these tests are now bottlenecked by the py4j boundary rather than constructing immutable maps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4484#issuecomment-426097277
https://github.com/hail-is/hail/pull/4484#issuecomment-426097277:5,Testability,test,test,5,"That test with `10000` fields takes about two minutes, so I didn't want to add that to the test suite. Although two minutes seems insanely slow, it's better than the quadratic scaling provided by the old implementation of `Infer(InsertFields(...))`. Moreover, these tests are now bottlenecked by the py4j boundary rather than constructing immutable maps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4484#issuecomment-426097277
https://github.com/hail-is/hail/pull/4484#issuecomment-426097277:91,Testability,test,test,91,"That test with `10000` fields takes about two minutes, so I didn't want to add that to the test suite. Although two minutes seems insanely slow, it's better than the quadratic scaling provided by the old implementation of `Infer(InsertFields(...))`. Moreover, these tests are now bottlenecked by the py4j boundary rather than constructing immutable maps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4484#issuecomment-426097277
https://github.com/hail-is/hail/pull/4484#issuecomment-426097277:266,Testability,test,tests,266,"That test with `10000` fields takes about two minutes, so I didn't want to add that to the test suite. Although two minutes seems insanely slow, it's better than the quadratic scaling provided by the old implementation of `Infer(InsertFields(...))`. Moreover, these tests are now bottlenecked by the py4j boundary rather than constructing immutable maps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4484#issuecomment-426097277
https://github.com/hail-is/hail/issues/4485#issuecomment-429003951:56,Performance,perform,performed,56,"On this note, could the keys that the join was actually performed on be printed separately? Would make a nice sanity check",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4485#issuecomment-429003951
https://github.com/hail-is/hail/issues/4485#issuecomment-429003951:110,Safety,sanity check,sanity check,110,"On this note, could the keys that the join was actually performed on be printed separately? Would make a nice sanity check",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4485#issuecomment-429003951
https://github.com/hail-is/hail/pull/4487#issuecomment-426431157:52,Testability,test,tests,52,"Sorry, looks like a bug slipped in after I last ran tests locally. I'll look at it in the morning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4487#issuecomment-426431157
https://github.com/hail-is/hail/issues/4492#issuecomment-428398036:37,Integrability,interface,interface,37,See: http://dev.hail.is/t/aggregator-interface/120,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4492#issuecomment-428398036
https://github.com/hail-is/hail/pull/4494#issuecomment-427159094:10,Usability,clear,clearer,10,"It's much clearer, awesome",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4494#issuecomment-427159094
https://github.com/hail-is/hail/pull/4498#issuecomment-430451063:27,Deployability,deploy,deploy,27,"Ok, this should pass. I'll deploy 0.2.1 after master is merged. I'll get self-deployment working soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4498#issuecomment-430451063
https://github.com/hail-is/hail/pull/4498#issuecomment-430451063:78,Deployability,deploy,deployment,78,"Ok, this should pass. I'll deploy 0.2.1 after master is merged. I'll get self-deployment working soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4498#issuecomment-430451063
https://github.com/hail-is/hail/pull/4499#issuecomment-428398568:186,Usability,simpl,simpler,186,"@catoverdrive I see now there is an unwrap that should have had this same effect, so I don't quite understand why this was necessary (although it seemed to be). Anyway, this still seems simpler/more correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4499#issuecomment-428398568
https://github.com/hail-is/hail/pull/4501#issuecomment-429380091:22,Testability,test,tested,22,@tpoterba this is now tested / ready (I'll add a doc example and a bit more documentation later). Let's discuss reconciliation,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4501#issuecomment-429380091
https://github.com/hail-is/hail/pull/4502#issuecomment-427220196:21,Availability,error,error,21,"also, NB: I got this error after trying the script one last time; ```; ERROR: (gcloud.iam.service-accounts.keys.create) RESOURCE_EXHAUSTED: Maximum number of keys on account reached.; - '@type': type.googleapis.com/google.rpc.RetryInfo; retryDelay: 86401s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4502#issuecomment-427220196
https://github.com/hail-is/hail/pull/4502#issuecomment-427220196:71,Availability,ERROR,ERROR,71,"also, NB: I got this error after trying the script one last time; ```; ERROR: (gcloud.iam.service-accounts.keys.create) RESOURCE_EXHAUSTED: Maximum number of keys on account reached.; - '@type': type.googleapis.com/google.rpc.RetryInfo; retryDelay: 86401s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4502#issuecomment-427220196
https://github.com/hail-is/hail/issues/4506#issuecomment-427574034:14,Deployability,pipeline,pipeline,14,Can you add a pipeline/dataset that demonstrates this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427574034
https://github.com/hail-is/hail/issues/4506#issuecomment-427575965:199,Performance,queue,queue,199,"plink removed 160 of 7516. Hail removed 424. Also, Hail spent 2s removing 419, and 9 minutes removing the other 5. ```; 2018-10-06 09:42:36 Hail: INFO: ld_prune: running local pruning stage with max queue size of 2166480 variants; 2018-10-06 09:42:37 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:42:38 Hail: INFO: wrote 7097 items in 100 partitions; 2018-10-06 09:42:40 Hail: INFO: wrote 7097 items in 100 partitions to file:/tmp/hail.jM6D3jREhNqh/Jx7rAbqyTP; 2018-10-06 09:42:40 Hail: INFO: ld_prune: local pruning stage retained 7097 variants; 2018-10-06 09:42:41 Hail: INFO: Wrote all 2 blocks of 7097 x 284 matrix with block size 4096.; 2018-10-06 09:42:59 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:46:52 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:46:52 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:48:58 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:48:58 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:46 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:51:46 Hail: INFO: wrote 5 items in 3 partitions; 2018-10-06 09:51:46 Hail: INFO: ld_prune: correlation graph of locally-pruned variants has 5 edges,; finding maximal independent set...; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: wrote 7092 items in 100 partitions; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427575965
https://github.com/hail-is/hail/issues/4506#issuecomment-427593611:149,Deployability,update,update,149,"**EDIT: If you're finding this thread in 2021 or later, do not attempt to use a tiny block size. LD prune works best with the default block size.**. update: using tiny block size:; ```; print(""extract pruned set of variants""); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000, block_size=75); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```; takes 16.1s, about 40x faster. This is still wayyyyy too slow -- the total input data is 14M!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427593611
https://github.com/hail-is/hail/issues/4506#issuecomment-451536640:21,Testability,test,test,21,"Here's an executable test script. ```; import timeit; import hail as hl. setup = '''; import hail as hl; '''; hl.utils.get_1kg('data/'). print(""variant QC""); vqc = timeit.timeit('''; mt = hl.read_matrix_table('data/1kg.mt'); mt = hl.variant_qc(mt); mt.write(""variant_qc.mt"", overwrite=True); ''', number=1, setup=setup); print(""filter variants for QC""); filter = timeit.timeit('''; mt = hl.read_matrix_table('variant_qc.mt'); non_autosomal = [hl.parse_locus_interval(x); for x in (hl.get_reference(""GRCh37"").mt_contigs +; hl.get_reference(""GRCh37"").x_contigs +; hl.get_reference(""GRCh37"").y_contigs)]; mt = hl.filter_intervals(mt, non_autosomal, keep=False); mt = mt.filter_rows(hl.is_snp(mt.alleles[0], mt.alleles[1])); mt = mt.filter_rows(~ hl.is_mnp(mt.alleles[0], mt.alleles[1])); mt = mt.filter_rows(~ hl.is_indel(mt.alleles[0], mt.alleles[1])); mt = mt.filter_rows(~ hl.is_complex(mt.alleles[0], mt.alleles[1])); mt = mt.filter_rows(mt.variant_qc.AF[1] >= 0.01); mt = mt.filter_rows(mt.variant_qc.AF[1] <= 0.99); mt = mt.filter_rows(mt.variant_qc.call_rate >= 0.98); mt.write(""mt.filtered"", overwrite=True); ''', number=1, setup=setup); print(""repartition""); repart = timeit.timeit('''; mt = hl.read_matrix_table('mt.filtered'); mt = mt.repartition(100); mt.write(""repartitioned.mt"", overwrite=True); ''', number=1, setup=setup); print(""extract pruned set of variants""); ldprune = timeit.timeit('''; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht""); ''', number=1, setup=setup); print(""write filtered matrix table""); writefilt = timeit.timeit('''; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.read_table('pruned_tbl.ht'); mt = mt.filter_rows(hl.is_defined(pruned_tbl[mt.row_key])); mt.write('pruned.mt', overwrite=True); ''', number=1, setup=setup). print(f'''; vqc {vqc}; filter {filter}; repart {repart}; ldprune {ldprune}; writefilt {writefilt}; '''",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-451536640
https://github.com/hail-is/hail/issues/4508#issuecomment-427574724:55,Testability,test,test,55,probably can unkey in python to fix this. also need to test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4508#issuecomment-427574724
https://github.com/hail-is/hail/issues/4508#issuecomment-428976439:289,Availability,robust,robust,289,"@tpoterba Do you have a specific example where this fails? I think the columns are already unkeyed before export with this line:. ```; dataset = dataset._select_all(col_exprs=fam_exprs,; col_key=[],; row_exprs=bim_exprs,; entry_exprs=entry_exprs); ```. I tried making the Python test more robust where I permute the columns first so not in alphabetical order before exporting, but couldn't replicate the error. The same is true for `export_gen`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4508#issuecomment-428976439
https://github.com/hail-is/hail/issues/4508#issuecomment-428976439:404,Availability,error,error,404,"@tpoterba Do you have a specific example where this fails? I think the columns are already unkeyed before export with this line:. ```; dataset = dataset._select_all(col_exprs=fam_exprs,; col_key=[],; row_exprs=bim_exprs,; entry_exprs=entry_exprs); ```. I tried making the Python test more robust where I permute the columns first so not in alphabetical order before exporting, but couldn't replicate the error. The same is true for `export_gen`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4508#issuecomment-428976439
https://github.com/hail-is/hail/issues/4508#issuecomment-428976439:279,Testability,test,test,279,"@tpoterba Do you have a specific example where this fails? I think the columns are already unkeyed before export with this line:. ```; dataset = dataset._select_all(col_exprs=fam_exprs,; col_key=[],; row_exprs=bim_exprs,; entry_exprs=entry_exprs); ```. I tried making the Python test more robust where I permute the columns first so not in alphabetical order before exporting, but couldn't replicate the error. The same is true for `export_gen`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4508#issuecomment-428976439
https://github.com/hail-is/hail/pull/4509#issuecomment-428052468:33,Deployability,integrat,integration,33,"Love it! Though email, not slack integration? ðŸ˜›",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468
https://github.com/hail-is/hail/pull/4509#issuecomment-428052468:33,Integrability,integrat,integration,33,"Love it! Though email, not slack integration? ðŸ˜›",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468
https://github.com/hail-is/hail/pull/4509#issuecomment-428393236:62,Deployability,deploy,deployed-sha,62,I think I addressed all the comments. I also noticed some get-deployed-sha.sh scripts were broken (name of deployment was wrong) and I fixed them.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428393236
https://github.com/hail-is/hail/pull/4509#issuecomment-428393236:107,Deployability,deploy,deployment,107,I think I addressed all the comments. I also noticed some get-deployed-sha.sh scripts were broken (name of deployment was wrong) and I fixed them.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428393236
https://github.com/hail-is/hail/pull/4509#issuecomment-428393425:51,Deployability,deploy,deployed-sha,51,Also added upload to projects and added upload get-deployed-sha and deploy scripts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428393425
https://github.com/hail-is/hail/pull/4509#issuecomment-428393425:68,Deployability,deploy,deploy,68,Also added upload to projects and added upload get-deployed-sha and deploy scripts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428393425
https://github.com/hail-is/hail/pull/4509#issuecomment-429176963:9,Availability,failure,failure,9,New test failure mode:. ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/broad-ctsa/regions/global/operations/07164d0e-6c27-35d9-8132-9960b0db6d43] failed: Internal server error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963
https://github.com/hail-is/hail/pull/4509#issuecomment-429176963:24,Availability,ERROR,ERROR,24,New test failure mode:. ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/broad-ctsa/regions/global/operations/07164d0e-6c27-35d9-8132-9960b0db6d43] failed: Internal server error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963
https://github.com/hail-is/hail/pull/4509#issuecomment-429176963:189,Availability,error,error,189,New test failure mode:. ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/broad-ctsa/regions/global/operations/07164d0e-6c27-35d9-8132-9960b0db6d43] failed: Internal server error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963
https://github.com/hail-is/hail/pull/4509#issuecomment-429176963:4,Testability,test,test,4,New test failure mode:. ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/broad-ctsa/regions/global/operations/07164d0e-6c27-35d9-8132-9960b0db6d43] failed: Internal server error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:8,Availability,failure,failure,8,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:23,Security,Access,AccessDeniedException,23,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:132,Security,access,access,132,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:445,Security,authoriz,authorizing,445,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:150,Testability,test,test,150,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:172,Testability,test,test-ci,172,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:245,Testability,test,test,245,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:364,Testability,assert,assert,364,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:531,Testability,test,test,531,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:396,Deployability,deploy,deploy,396,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:530,Security,authenticat,authentication,530,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:26,Testability,test,testing,26,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:62,Testability,test,tested,62,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:296,Testability,test,test,296,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:380,Testability,test,tests,380,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:420,Testability,test,test,420,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:568,Testability,test,tests,568,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:617,Testability,test,testing,617,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:635,Testability,log,login,635,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:733,Testability,test,testing-of-,733,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:852,Testability,test,testing-with-rest-assured-,852,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:936,Testability,test,test,936,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:980,Testability,test,tests,980,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:1019,Testability,test,testing,1019,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595
https://github.com/hail-is/hail/issues/4513#issuecomment-428377258:247,Availability,error,error,247,"> I've been working on an R interface to Hail through the sparklyr package. this also sounds awesome. To the issue -- this usually means something really bad happened on an executor, like a segfault. Spark usually is reluctant to provide the real error message, but in this case I think it's our fault. . Did you already do our job of bisecting to that commit for us? I'm not sure where exactly the generated C++ is being used (just the decoder, right, team?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258
https://github.com/hail-is/hail/issues/4513#issuecomment-428377258:296,Availability,fault,fault,296,"> I've been working on an R interface to Hail through the sparklyr package. this also sounds awesome. To the issue -- this usually means something really bad happened on an executor, like a segfault. Spark usually is reluctant to provide the real error message, but in this case I think it's our fault. . Did you already do our job of bisecting to that commit for us? I'm not sure where exactly the generated C++ is being used (just the decoder, right, team?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258
https://github.com/hail-is/hail/issues/4513#issuecomment-428377258:28,Integrability,interface,interface,28,"> I've been working on an R interface to Hail through the sparklyr package. this also sounds awesome. To the issue -- this usually means something really bad happened on an executor, like a segfault. Spark usually is reluctant to provide the real error message, but in this case I think it's our fault. . Did you already do our job of bisecting to that commit for us? I'm not sure where exactly the generated C++ is being used (just the decoder, right, team?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258
https://github.com/hail-is/hail/issues/4513#issuecomment-428377258:253,Integrability,message,message,253,"> I've been working on an R interface to Hail through the sparklyr package. this also sounds awesome. To the issue -- this usually means something really bad happened on an executor, like a segfault. Spark usually is reluctant to provide the real error message, but in this case I think it's our fault. . Did you already do our job of bisecting to that commit for us? I'm not sure where exactly the generated C++ is being used (just the decoder, right, team?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258
https://github.com/hail-is/hail/issues/4513#issuecomment-428377634:157,Energy Efficiency,allocate,allocated,157,"The decoder should only be using generated C++ if the environment flag `ENABLE_CPP_CODEGEN` is set (off by default). The `Region` class is now backed by C++-allocated memory regions, though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377634
https://github.com/hail-is/hail/issues/4513#issuecomment-429132148:118,Availability,down,down,118,"Yea, binary incompatibility is as good a guess as any. I'm sure it's this commit though. What's the best way to break down the commit into likely culprits that I could test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429132148
https://github.com/hail-is/hail/issues/4513#issuecomment-429132148:168,Testability,test,test,168,"Yea, binary incompatibility is as good a guess as any. I'm sure it's this commit though. What's the best way to break down the commit into likely culprits that I could test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429132148
https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:43,Deployability,release,release,43,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190
https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:9,Integrability,interface,interface,9,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190
https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:522,Modifiability,config,config,522,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190
https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:830,Modifiability,config,config,830,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190
https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:837,Modifiability,config,config,837,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190
https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:1037,Testability,log,log,1037,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190
https://github.com/hail-is/hail/issues/4513#issuecomment-430451868:463,Availability,down,down,463,"Just skimmed the discussion. >> I've been working on an R interface to Hail through the sparklyr package; >; > this also sounds awesome. woah, hell yes. I'll look tomorrow. Our build situation is a bit messed up right now. I'll try to isolate your issue and fix it. Moreover, I should be fixing the build situation for good soon. Can you share a full executor log for an executor that fails? That should have some information about why the spark context got shut down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868
https://github.com/hail-is/hail/issues/4513#issuecomment-430451868:58,Integrability,interface,interface,58,"Just skimmed the discussion. >> I've been working on an R interface to Hail through the sparklyr package; >; > this also sounds awesome. woah, hell yes. I'll look tomorrow. Our build situation is a bit messed up right now. I'll try to isolate your issue and fix it. Moreover, I should be fixing the build situation for good soon. Can you share a full executor log for an executor that fails? That should have some information about why the spark context got shut down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868
https://github.com/hail-is/hail/issues/4513#issuecomment-430451868:360,Testability,log,log,360,"Just skimmed the discussion. >> I've been working on an R interface to Hail through the sparklyr package; >; > this also sounds awesome. woah, hell yes. I'll look tomorrow. Our build situation is a bit messed up right now. I'll try to isolate your issue and fix it. Moreover, I should be fixing the build situation for good soon. Can you share a full executor log for an executor that fails? That should have some information about why the spark context got shut down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868
https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:152,Availability,failure,failure,152,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977
https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:1281,Availability,failure,failure,1281,"sspath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9] tools_3.5.1 digest_0.6.18 jsonlite_1.5 tibble_1.4.2 ; [13] nlme_3.1-137 lattice_0.20-35 pkgconfig_2.0.2 rlang_0.2.2 ; [17] shiny_1.1.0 DBI_1.0.0 rstudioapi_0.8 bindrcpp_0.2.2 ; [21] withr_2.1.2 dplyr_0.7.7 httr_1.3.1 sparklyr_0.9.2 ; [25] rappdirs_0.3.1 h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977
https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:439,Modifiability,config,config,439,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977
https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:747,Modifiability,config,config,747,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977
https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:754,Modifiability,config,config,754,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977
https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:1838,Performance,load,loaded,1838,"g=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9] tools_3.5.1 digest_0.6.18 jsonlite_1.5 tibble_1.4.2 ; [13] nlme_3.1-137 lattice_0.20-35 pkgconfig_2.0.2 rlang_0.2.2 ; [17] shiny_1.1.0 DBI_1.0.0 rstudioapi_0.8 bindrcpp_0.2.2 ; [21] withr_2.1.2 dplyr_0.7.7 httr_1.3.1 sparklyr_0.9.2 ; [25] rappdirs_0.3.1 htmlwidgets_1.3 rprojroot_1.3-2 grid_3.5.1 ; [29] tidyselect_0.2.5 glue_1.3.0 forge_0.1.0 R6_2.3.0 ; [33] purrr_0.2.5 tidyr_0.8.1 magrittr_1.5 backports_1.1.2 ; [37] promises_1.0.1 htmltools_0.3.6 assertthat_0.2.0 mime_0.6 ; [41] xtable_1.8-3 httpuv_1.4.5 openssl_1.0.2 lazyeval_0.2.1 ; [45] broom_0.5.0 crayon_1.3.4 ; [1] 32; ```. How are you telling sparklyr where the Spark jars are? I used `install_spark`. It looks like the latest sparklyr is 0.9.2. What does `sessionInfo()` print for you?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977
https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:954,Testability,log,log,954,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977
https://github.com/hail-is/hail/issues/4514#issuecomment-428408138:108,Energy Efficiency,green,green,108,I need to see the index page (with the build log) to try to fix this. Everything on that page is rightfully green,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-428408138
https://github.com/hail-is/hail/issues/4514#issuecomment-428408138:45,Testability,log,log,45,I need to see the index page (with the build log) to try to fix this. Everything on that page is rightfully green,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-428408138
https://github.com/hail-is/hail/issues/4514#issuecomment-428410840:71,Testability,log,log,71,The PR failed: https://github.com/hail-is/hail/pull/4509 and the build log shows failed exit code 2: https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-428410840
https://github.com/hail-is/hail/issues/4514#issuecomment-428410840:233,Testability,log,log,233,The PR failed: https://github.com/hail-is/hail/pull/4509 and the build log shows failed exit code 2: https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-428410840
https://github.com/hail-is/hail/issues/4514#issuecomment-428563325:188,Testability,test,testing,188,I guess the design of the whole artifacts system doesn't really account for anything but hail/hail. We need to fix up the index page to account for other projects and add another level of testing indirection,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-428563325
https://github.com/hail-is/hail/issues/4514#issuecomment-437121438:70,Integrability,depend,dependency,70,I'm unassigning myself after our Zulip discussion on the intended job dependency graph in batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-437121438
https://github.com/hail-is/hail/issues/4514#issuecomment-451512691:137,Testability,log,log,137,"I'm closing because this ticket merely notes that the artifact page is only representative of hail. This is expected behavior. The build log is the source of truth, the artifact page is only helpful for the `hail` subproject. In the future, the CI can define a DAG of jobs for the different subproject. The associated UI will enable users to quickly see what failed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-451512691
https://github.com/hail-is/hail/issues/4516#issuecomment-428425318:17,Availability,down,down,17,"FWIW, cutting it down to 2446 aggregators work (sat in code generation for about a minute but then ran in 18 seconds)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4516#issuecomment-428425318
https://github.com/hail-is/hail/issues/4516#issuecomment-613456757:11,Testability,benchmark,benchmark,11,Is there a benchmark test for this?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4516#issuecomment-613456757
https://github.com/hail-is/hail/issues/4516#issuecomment-613456757:21,Testability,test,test,21,Is there a benchmark test for this?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4516#issuecomment-613456757
https://github.com/hail-is/hail/issues/4517#issuecomment-428986578:28,Availability,failure,failure,28,This is in response to this failure: https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/39a94649482a2512a7a514e6084c5b84f48b8205/index.html. Which might be mis-diagnosed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428986578
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:782,Availability,error,error,782,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:845,Availability,error,error,845,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:168,Deployability,deploy,deploy,168,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:509,Deployability,deploy,deploy,509,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:738,Deployability,deploy,deploy,738,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:344,Modifiability,config,config,344,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:407,Modifiability,config,config,407,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:93,Testability,test,test-repo,93,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:123,Testability,test,test-repo,123,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:150,Testability,test,test-repo,150,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:375,Testability,test,tests,375,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:437,Testability,test,tests,437,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:1047,Testability,log,logs,1047,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662
https://github.com/hail-is/hail/issues/4517#issuecomment-428992101:52,Performance,race condition,race condition,52,It looks like the repo create succeeded. Is there a race condition and it isn't ready yet?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428992101
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5087,Availability,down,downloads,5087,"test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5676,Deployability,release,releases,5676,"9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5772,Deployability,deploy,deployments,5772," --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:161,Testability,test,test-locally,161,"So the sequence of events is:; - create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.gi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:225,Testability,test,test-locally,225,"So the sequence of events is:; - create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.gi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:270,Testability,test,test-cluster,270,"So the sequence of events is:; - create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.gi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:549,Testability,test,test-,549,"So the sequence of events is:; - create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.gi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:787,Testability,test,test,787,"So the sequence of events is:; - create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.gi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:795,Testability,test,test-,795,"So the sequence of events is:; - create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.gi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:971,Testability,test,test-,971,"create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/recei",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1010,Testability,test,test,1010,"create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/recei",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1018,Testability,test,test-,1018,"create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/recei",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1067,Testability,log,login,1067,"create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/recei",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1084,Testability,test,test,1084,"create repo on GH (via REST call); - initialize local repository (no GH comms); - add remote (no GH comms); - push to GH. (see [test-locally.sh](https://github.com/hail-is/hail/blob/master/ci/test-locally.sh#L31-L53), which is called by test-cluster.sh, after exposing its pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/recei",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1294,Testability,test,test,1294,"ts pod via nginx). We're failing on that last step. It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1342,Testability,test,test,1342,"It appears that the repository creation succeeded, the response to that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1405,Testability,test,test,1405,"o that REST call was a successful looking JSON object (which indicates a create repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1478,Testability,test,test,1478,"te repository with the expected name, `REPO_NAME=ci-test-p4a9fxo7`) [1]. It is unclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1560,Testability,test,test,1560,"nclear if repository deletion succeeded [2], I should find a way to make curl print the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1637,Testability,test,test,1637," the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1727,Testability,test,test,1727," deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1808,Testability,test,test,1808,"AIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/numbe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1872,Testability,test,test,1872,"; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1938,Testability,test,test,1938,"""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2024,Testability,test,test,2024,"""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2138,Testability,test,test,2138,"ttps://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2146,Testability,test,test-,2146,"ttps://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2246,Testability,test,test,2246,"://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2254,Testability,test,test-,2254,"://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2322,Testability,test,test,2322,"-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2330,Testability,test,test-,2330,"-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2403,Testability,test,test,2403,"ing_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2411,Testability,test,test-,2411,"ing_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2501,Testability,test,test,2501,"pi.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2509,Testability,test,test-,2509,"pi.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2606,Testability,test,test,2606,"ers/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2614,Testability,test,test-,2614,"ers/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2688,Testability,test,test,2688,"m/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2696,Testability,test,test-,2696,"m/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2777,Testability,test,test,2777,"ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-tes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2785,Testability,test,test-,2785,"ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-tes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2877,Testability,test,test,2877," ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-te",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2885,Testability,test,test-,2885," ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-te",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2964,Testability,test,test,2964,"ps://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2972,Testability,test,test-,2972,"ps://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3060,Testability,test,test,3060,": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3068,Testability,test,test-,3068,": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3153,Testability,test,test,3153,"ion"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-te",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3161,Testability,test,test-,3161,"ion"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-te",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3234,Testability,test,test,3234,"4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3242,Testability,test,test-,3242,"4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3329,Testability,test,test,3329,"keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3337,Testability,test,test-,3337,"keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3423,Testability,test,test,3423,"aborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3431,Testability,test,test-,3431,"aborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3514,Testability,test,test,3514,"llaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3522,Testability,test,test-,3522,"llaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3609,Testability,test,test,3609,"; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3617,Testability,test,test-,3617,"; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3704,Testability,test,test,3704,"ents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3712,Testability,test,test-,3712,"ents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3795,Testability,test,test,3795,"}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3803,Testability,test,test-,3803,"}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3889,Testability,test,test,3889,"nees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3897,Testability,test,test-,3897,"nees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3984,Testability,test,test,3984,"nches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3992,Testability,test,test-,3992,"nches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4079,Testability,test,test,4079,"""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""is",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4087,Testability,test,test-,4087,"""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""is",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4170,Testability,test,test,4170,"""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pul",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4178,Testability,test,test-,4178,"""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pul",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4266,Testability,test,test,4266,"ttps://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""mileston",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4274,Testability,test,test-,4274,"ttps://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""mileston",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4363,Testability,test,test,4363,"//api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notific",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4371,Testability,test,test-,4371,"//api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notific",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4465,Testability,test,test,4465,".com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,parti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4473,Testability,test,test-,4473,".com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,parti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4569,Testability,test,test,4569,"repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4577,Testability,test,test-,4577,"repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4664,Testability,test,test,4664,"//api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4672,Testability,test,test-,4672,"//api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4765,Testability,test,test,4765,"ithub.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4773,Testability,test,test-,4773,"ithub.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4962,Testability,test,test,4962,"test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4970,Testability,test,test-,4970,"test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5065,Testability,test,test,5065,"test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5073,Testability,test,test-,5073,"test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5152,Testability,test,test,5152,"i-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5160,Testability,test,test-,5160,"i-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5244,Testability,test,test,5244,"/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_iss",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5252,Testability,test,test-,5252,"/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_iss",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5340,Testability,test,test,5340,"-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5348,Testability,test,test-,5348,"-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5444,Testability,test,test,5444,"9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5452,Testability,test,test-,5452,"9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5561,Testability,test,test,5561,"fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5569,Testability,test,test-,5569,"fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5654,Testability,test,test,5654,"9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5662,Testability,test,test-,5662,"9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5750,Testability,test,test,5750," --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5758,Testability,test,test-,5758," --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5940,Testability,test,test,5940,"om/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5948,Testability,test,test-,5948,"om/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6004,Testability,test,test,6004," ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6012,Testability,test,test-,6012," ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6074,Testability,test,test,6074,"4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6082,Testability,test,test-,6082,"4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6142,Testability,test,test,6142,"ll,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6150,Testability,test,test-,6150,"ll,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6725,Testability,log,login,6725,"ll,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6742,Testability,test,test,6742,"ll,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6952,Testability,test,test,6952,".git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7000,Testability,test,test,7000,"est-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7063,Testability,test,test,7063,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7136,Testability,test,test,7136,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7218,Testability,test,test,7218,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7295,Testability,test,test,7295,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7385,Testability,test,test,7385,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7466,Testability,test,test,7466,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7530,Testability,test,test,7530,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7596,Testability,test,test,7596,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:7682,Testability,test,test,7682,"p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858
https://github.com/hail-is/hail/issues/4517#issuecomment-429025153:158,Availability,down,down,158,"It is possible there is a race condition, though I have not witnessed this before. In fact, it seems rather reasonable that GitHub had some intermittent slow down that delayed repository creation or ability to find said repository temporarily.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153
https://github.com/hail-is/hail/issues/4517#issuecomment-429025153:26,Performance,race condition,race condition,26,"It is possible there is a race condition, though I have not witnessed this before. In fact, it seems rather reasonable that GitHub had some intermittent slow down that delayed repository creation or ability to find said repository temporarily.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153
https://github.com/hail-is/hail/issues/4517#issuecomment-429026174:42,Testability,log,log,42,"Also, I think you've referenced the wrong log, the one from your original reporting email is: https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log. The log you referenced appears to be failing because another process (presumably `batch`) has already bound to port 5000. Perhaps `batch`'s `hail-ci-build.sh` is not properly cleaning up processes when it exits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026174
https://github.com/hail-is/hail/issues/4517#issuecomment-429026174:226,Testability,log,log,226,"Also, I think you've referenced the wrong log, the one from your original reporting email is: https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log. The log you referenced appears to be failing because another process (presumably `batch`) has already bound to port 5000. Perhaps `batch`'s `hail-ci-build.sh` is not properly cleaning up processes when it exits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026174
https://github.com/hail-is/hail/issues/4517#issuecomment-429026174:235,Testability,log,log,235,"Also, I think you've referenced the wrong log, the one from your original reporting email is: https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log. The log you referenced appears to be failing because another process (presumably `batch`) has already bound to port 5000. Perhaps `batch`'s `hail-ci-build.sh` is not properly cleaning up processes when it exits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026174
https://github.com/hail-is/hail/issues/4517#issuecomment-429026782:94,Availability,failure,failures,94,The repository in question was created at `2018-10-10T00:32:59Z`. GitHub indicates [no system failures](https://status.github.com/messages) on October the tenth or the ninth.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782
https://github.com/hail-is/hail/issues/4517#issuecomment-429026782:130,Integrability,message,messages,130,The repository in question was created at `2018-10-10T00:32:59Z`. GitHub indicates [no system failures](https://status.github.com/messages) on October the tenth or the ninth.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782
https://github.com/hail-is/hail/issues/4517#issuecomment-429133941:45,Availability,failure,failure,45,"So there appear to be to distinct issues:. - failure to push the initial data to a repository due to a 404 https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log. - 404s due to someone else already bound to port 5000 (maybe `batch`?) https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/39a94649482a2512a7a514e6084c5b84f48b8205/index.html; ```; Traceback (most recent call last):; File ""ci/ci.py"", line 372, in <module>; app.run(host='0.0.0.0', threaded=False); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/flask/app.py"", line 943, in run; run_simple(host, port, self, **options); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 814, in run_simple; inner(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 774, in inner; fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 666, in make_server; passthrough_errors, ssl_context, fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 577, in __init__; self.address_family), handler); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 449, in __init__; self.server_bind(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/http/server.py"", line 137, in server_bind; socketserver.TCPServer.server_bind(self); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 463, in server_bind; self.socket.bind(self.server_address); OSError: [Errno 98] Address already in use; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429133941
https://github.com/hail-is/hail/issues/4517#issuecomment-429133941:239,Testability,log,log,239,"So there appear to be to distinct issues:. - failure to push the initial data to a repository due to a 404 https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log. - 404s due to someone else already bound to port 5000 (maybe `batch`?) https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/39a94649482a2512a7a514e6084c5b84f48b8205/index.html; ```; Traceback (most recent call last):; File ""ci/ci.py"", line 372, in <module>; app.run(host='0.0.0.0', threaded=False); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/flask/app.py"", line 943, in run; run_simple(host, port, self, **options); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 814, in run_simple; inner(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 774, in inner; fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 666, in make_server; passthrough_errors, ssl_context, fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 577, in __init__; self.address_family), handler); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 449, in __init__; self.server_bind(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/http/server.py"", line 137, in server_bind; socketserver.TCPServer.server_bind(self); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 463, in server_bind; self.socket.bind(self.server_address); OSError: [Errno 98] Address already in use; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429133941
https://github.com/hail-is/hail/issues/4517#issuecomment-429134998:44,Performance,race condition,race condition,44,I'm fairly certain this was an intermittent race condition in GitHub that we have no control over. Closing. Re-open if we see it again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429134998
https://github.com/hail-is/hail/issues/4517#issuecomment-429135514:61,Testability,test,testing,61,"Created https://github.com/hail-is/hail/issues/4533 to track testing for account liveliness. There's no good solution to bullet two, I just email GitHub support and ask them to unblock. I guess this page is now the documentation for that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429135514
https://github.com/hail-is/hail/issues/4522#issuecomment-429133433:63,Testability,test,test,63,@cseed I was planning to dig into this to find a small IR-only test that fails (removing the need for a dataset) but I'm feeling a bit oversubscribed right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429133433
https://github.com/hail-is/hail/issues/4522#issuecomment-429179210:233,Availability,down,down,233,Is there currently a debugging version of Region (one that checks bounds and throws an exception)? Dan and I have both uses that profitably in the past to isolate bugs like this quickly. That might be the first step to tracking this down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429179210
https://github.com/hail-is/hail/issues/4522#issuecomment-429349677:190,Availability,failure,failures,190,"Ryan Koesterer is reporting a related issue: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/subject/export.20table.20failure. The former causes inexplicable container failures, the latter succeeds.; ```; mt = mt.annotate_cols(phenoFemale =; hl.cond(hl.literal({""female"",""Female"",""f"",""F"",""2""}).contains(mt.pheno[args.sex_col]), True, False)); ```; ```; mt = mt.annotate_cols(; phenoFemale = hl.cond(~ hl.is_missing(mt.pheno[args.sex_col]),; (mt.pheno[args.sex_col] == 'female') |; (mt.pheno[args.sex_col] == 'Female') |; (mt.pheno[args.sex_col] == 'f') |; (mt.pheno[args.sex_col] == 'F') |; (mt.pheno[args.sex_col] == '2'), False)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429349677
https://github.com/hail-is/hail/issues/4522#issuecomment-429350451:18,Integrability,message,message,18,Let's remember to message him when we fix this ticket.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429350451
https://github.com/hail-is/hail/issues/4522#issuecomment-430000941:15,Testability,test,test,15,"I made a small test example that fails. Still in Python -- couldn't replicate it in the IRSuite. ```; t = hl.utils.range_table(1); t = t.annotate(s=hl.set({'foo'}), nested=hl.null(hl.tstruct(elt=hl.tstr))); t = t.key_by().drop('idx'); t.filter(t.s.contains(t.nested.elt))._force_count(); ```. - Doesn't matter if the set has one element or is empty; - Doesn't matter if it's an `annotate_globals` versus `annotate`; - If `nested` is a string, null value, or a struct with a null value, it works",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-430000941
https://github.com/hail-is/hail/issues/4522#issuecomment-430009988:19,Usability,simpl,simpler,19,"Great, this is way simpler. Is the key_by/drop necessary? What about just making nested a string?. Does this also fail?. ```; t = hl.utils.range_table(1); t = t.annotate(x = hl.bind(lambda s, nested: s.contains(nested), hl.set({'foo'}), hl.null(...))); t._force_count(); ```. I'm going to guess no and that it has something to do with writing the row (which is why isolating it in the IRSuite isn't working).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-430009988
https://github.com/hail-is/hail/issues/4522#issuecomment-430029805:340,Testability,log,log,340,"I was able to assist Jackie in generating a core on my machine using the snipped she posted. In examining the core, I saw that we call `Unsafe_GetNativeLong` with an `addr` argument of 8. A quick examination of the JVM code showed that this addr is promptly cast to a `void*` via `uintptr_t`, and then deferenced. Attached is the jvm crash log showing the java stack trace. [hs_err_pid13461.log](https://github.com/hail-is/hail/files/2480716/hs_err_pid13461.log)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-430029805
https://github.com/hail-is/hail/issues/4522#issuecomment-430029805:391,Testability,log,log,391,"I was able to assist Jackie in generating a core on my machine using the snipped she posted. In examining the core, I saw that we call `Unsafe_GetNativeLong` with an `addr` argument of 8. A quick examination of the JVM code showed that this addr is promptly cast to a `void*` via `uintptr_t`, and then deferenced. Attached is the jvm crash log showing the java stack trace. [hs_err_pid13461.log](https://github.com/hail-is/hail/files/2480716/hs_err_pid13461.log)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-430029805
https://github.com/hail-is/hail/issues/4522#issuecomment-430029805:458,Testability,log,log,458,"I was able to assist Jackie in generating a core on my machine using the snipped she posted. In examining the core, I saw that we call `Unsafe_GetNativeLong` with an `addr` argument of 8. A quick examination of the JVM code showed that this addr is promptly cast to a `void*` via `uintptr_t`, and then deferenced. Attached is the jvm crash log showing the java stack trace. [hs_err_pid13461.log](https://github.com/hail-is/hail/files/2480716/hs_err_pid13461.log)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-430029805
https://github.com/hail-is/hail/issues/4526#issuecomment-431683150:22,Energy Efficiency,schedul,scheduler,22,see `org.apache.spark.scheduler.JobWaiter`. I think this will be fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4526#issuecomment-431683150
https://github.com/hail-is/hail/issues/4526#issuecomment-431683286:23,Energy Efficiency,schedul,scheduler,23,also `org.apache.spark.scheduler.DAGScheduler.submitJob`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4526#issuecomment-431683286
https://github.com/hail-is/hail/issues/4527#issuecomment-429050886:0,Usability,simpl,simpler,0,"simpler:; ```; In [12]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx), t1kg.row_idx), alleles=['A','T']); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429050886
https://github.com/hail-is/hail/issues/4527#issuecomment-429051397:543,Availability,error,error,543,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397
https://github.com/hail-is/hail/issues/4527#issuecomment-429051397:583,Performance,optimiz,optimization,583,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397
https://github.com/hail-is/hail/issues/4527#issuecomment-429051397:0,Usability,simpl,simpler,0,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397
https://github.com/hail-is/hail/issues/4527#issuecomment-429057183:53,Performance,optimiz,optimization,53,"with a code change:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}entriesIndex:3}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}entriesIndex:7}. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429057183
https://github.com/hail-is/hail/issues/4527#issuecomment-429062373:98,Performance,optimiz,optimizer,98,@cseed Can we reassign to someone else to determine:; 1. why the entries index is changing in the optimizer; 2. if this is actually a bug or not; 3. make a code change that either fixes the bug or fixes the equality check.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429062373
https://github.com/hail-is/hail/issues/4527#issuecomment-429178603:36,Integrability,message,message,36,"1. Because of a bug. 2. Yes. 3. The message in the exception should use BaseType.parseableType instead of pretty/toString, since that prints the missingness details.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429178603
https://github.com/hail-is/hail/issues/4527#issuecomment-429190070:490,Testability,assert,assert,490,"> this is the parsable type. It's not requiredness, it's entry array index. Oops, you're totally right, I scanned too fast. Right, it's that the rvRowType changed. So the rvRowType being part of the MatrixType is really a workaround until we have physical types. It shouldn't be part of the ""visible"" state of the MatrixType. Yeah, upcast via MatrixMapRows can do it:. ```; val newRVRow = newRow.typ.asInstanceOf[TStruct].fieldOption(MatrixType.entriesIdentifier) match {; case Some(f) =>; assert(f.typ == child.typ.entryArrayType); newRow; case None =>; InsertFields(newRow, Seq(; MatrixType.entriesIdentifier -> GetField(Ref(""va"", child.typ.rvRowType), MatrixType.entriesIdentifier))); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429190070
https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:4,Availability,error,error,4,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:30,Availability,ERROR,ERROR,30,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:61,Availability,ERROR,ERROR,61,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:2047,Safety,timeout,timeout,2047," to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=40m']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:936,Testability,test,test-,936,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:230,Usability,feedback,feedback,230,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:7806,Availability,ERROR,ERROR,7806,"........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:7837,Availability,ERROR,ERROR,7837,"........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:8827,Energy Efficiency,monitor,monitoring,8827,"ed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:10766,Energy Efficiency,monitor,monitoring,10766,"st-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip', '--properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh', '--master-machine-type=n1-standard-1', '--master-boot-disk-size=40GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-1', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=10m']' returned non-zero exit status 1. real	20m34.381s; user	0m6.329s; sys	0m1.522s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:1563,Performance,perform,performance,1563,"2de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip; Copying file://build/distributions/hail-python.zip [Content-Type=application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:1624,Performance,perform,performance,1624,"application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:1664,Performance,perform,performance,1664,"application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:9406,Safety,timeout,timeout,9406,"42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:11345,Safety,timeout,timeout,11345,"st-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip', '--properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh', '--master-machine-type=n1-standard-1', '--master-boot-disk-size=40GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-1', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=10m']' returned non-zero exit status 1. real	20m34.381s; user	0m6.329s; sys	0m1.522s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:832,Testability,test,test-,832,"```; + gsutil cp build/libs/hail-all-spark.jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar; Copying file://build/libs/hail-all-spark.jar [Content-Type=application/java-archive]...; / [0 files][ 0.0 B/ 27.2 MiB] ; / [1 files][ 27.2 MiB/ 27.2 MiB] ; -; Operation completed over 1 objects/27.2 MiB. . real	0m3.308s; user	0m1.212s; sys	0m0.548s; + gsutil cp build/distributions/hail-python.zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip; Copying file://build/distributions/hail-python.zip [Content-Type=application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; ...................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:8179,Testability,test,test-,8179,".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-b",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:8776,Testability,log,logging,8776,"...................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:9506,Testability,test,test-,9506,"42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:10118,Testability,test,test-,10118,"tandard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip', '--properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh', '--master-machine-type=n1-standard-1', '--master-boot-disk-size=40GB', '--num-master-loca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:10715,Testability,log,logging,10715,"st-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip', '--properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh', '--master-machine-type=n1-standard-1', '--master-boot-disk-size=40GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-1', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=10m']' returned non-zero exit status 1. real	20m34.381s; user	0m6.329s; sys	0m1.522s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:8006,Usability,feedback,feedback,8006,"..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
https://github.com/hail-is/hail/issues/4533#issuecomment-433530686:197,Testability,test,tests,197,"AFAICT, this has not happened since and I'm unaware of any way to programmatically check this state. I think we've essentially resolved this issue by having GitHub be aware that we are using these tests for automated tests. It appears they flagged us as OK since then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4533#issuecomment-433530686
https://github.com/hail-is/hail/issues/4533#issuecomment-433530686:217,Testability,test,tests,217,"AFAICT, this has not happened since and I'm unaware of any way to programmatically check this state. I think we've essentially resolved this issue by having GitHub be aware that we are using these tests for automated tests. It appears they flagged us as OK since then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4533#issuecomment-433530686
https://github.com/hail-is/hail/pull/4535#issuecomment-429153844:34,Testability,log,logistic,34,"The major changes are:; - Linear, logistic, linear mixed regression all return tables.; - The regression methods are found on RegressionModel.regress_rows; ; To do:; - This doesn't address the UKBB parallel regression concern. To fix that, I propose adding the ability for LinearRegressionMode.regress_rows to take a list of lists, each list of which is to be grouped for missingness separately. This is a back-compatible change from this model. I'd also propose that we add the ability for logistic and linear mixed models to take lists of phenotypes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-429153844
https://github.com/hail-is/hail/pull/4535#issuecomment-429153844:491,Testability,log,logistic,491,"The major changes are:; - Linear, logistic, linear mixed regression all return tables.; - The regression methods are found on RegressionModel.regress_rows; ; To do:; - This doesn't address the UKBB parallel regression concern. To fix that, I propose adding the ability for LinearRegressionMode.regress_rows to take a list of lists, each list of which is to be grouped for missingness separately. This is a back-compatible change from this model. I'd also propose that we add the ability for logistic and linear mixed models to take lists of phenotypes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-429153844
https://github.com/hail-is/hail/pull/4535#issuecomment-429487447:36,Integrability,interface,interface,36,"I think this is closer to the right interface, so I want to think about how to get it in while allowing for future changes that don't break interface. I think `RegressionModel` should be initialized with the data of `y` and `covariates`. It should have a `fit` method that returns a struct with the multivariate regression result, which I can add later without breaking interface. The underlying math is already there (e.g. when computing the null model), just a matter of packaging. It'd be nice for this to work on Tables too. And then `regress_rows`, renamed fit_rows, should only take `x` and other parameters like `test` and `block_size`. This is basically how LMM works, but with the multivariate global math all on the python side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-429487447
https://github.com/hail-is/hail/pull/4535#issuecomment-429487447:140,Integrability,interface,interface,140,"I think this is closer to the right interface, so I want to think about how to get it in while allowing for future changes that don't break interface. I think `RegressionModel` should be initialized with the data of `y` and `covariates`. It should have a `fit` method that returns a struct with the multivariate regression result, which I can add later without breaking interface. The underlying math is already there (e.g. when computing the null model), just a matter of packaging. It'd be nice for this to work on Tables too. And then `regress_rows`, renamed fit_rows, should only take `x` and other parameters like `test` and `block_size`. This is basically how LMM works, but with the multivariate global math all on the python side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-429487447
https://github.com/hail-is/hail/pull/4535#issuecomment-429487447:370,Integrability,interface,interface,370,"I think this is closer to the right interface, so I want to think about how to get it in while allowing for future changes that don't break interface. I think `RegressionModel` should be initialized with the data of `y` and `covariates`. It should have a `fit` method that returns a struct with the multivariate regression result, which I can add later without breaking interface. The underlying math is already there (e.g. when computing the null model), just a matter of packaging. It'd be nice for this to work on Tables too. And then `regress_rows`, renamed fit_rows, should only take `x` and other parameters like `test` and `block_size`. This is basically how LMM works, but with the multivariate global math all on the python side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-429487447
https://github.com/hail-is/hail/pull/4535#issuecomment-429487447:620,Testability,test,test,620,"I think this is closer to the right interface, so I want to think about how to get it in while allowing for future changes that don't break interface. I think `RegressionModel` should be initialized with the data of `y` and `covariates`. It should have a `fit` method that returns a struct with the multivariate regression result, which I can add later without breaking interface. The underlying math is already there (e.g. when computing the null model), just a matter of packaging. It'd be nice for this to work on Tables too. And then `regress_rows`, renamed fit_rows, should only take `x` and other parameters like `test` and `block_size`. This is basically how LMM works, but with the multivariate global math all on the python side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-429487447
https://github.com/hail-is/hail/pull/4535#issuecomment-430342019:32,Deployability,pipeline,pipeline,32,"Looks ready. It'll break many a pipeline, so can you whip up a discuss post on the change that includes last commit before it goes in?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342019
https://github.com/hail-is/hail/pull/4535#issuecomment-430342969:249,Deployability,patch,patch,249,"yes, sure. will do that after practicing talk once more. Also, this did remove the logic that prevents a remapping of entries if the field is a top level entry field. This will make UKBB regression performance worst in the very near term. Should we patch this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969
https://github.com/hail-is/hail/pull/4535#issuecomment-430342969:198,Performance,perform,performance,198,"yes, sure. will do that after practicing talk once more. Also, this did remove the logic that prevents a remapping of entries if the field is a top level entry field. This will make UKBB regression performance worst in the very near term. Should we patch this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969
https://github.com/hail-is/hail/pull/4535#issuecomment-430342969:83,Testability,log,logic,83,"yes, sure. will do that after practicing talk once more. Also, this did remove the logic that prevents a remapping of entries if the field is a top level entry field. This will make UKBB regression performance worst in the very near term. Should we patch this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969
https://github.com/hail-is/hail/pull/4535#issuecomment-430343623:13,Performance,optimiz,optimization,13,"I think that optimization is worth keeping for linear regression, i care less about the others since they're more bottlenecked elsewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430343623
https://github.com/hail-is/hail/pull/4535#issuecomment-430343623:114,Performance,bottleneck,bottlenecked,114,"I think that optimization is worth keeping for linear regression, i care less about the others since they're more bottlenecked elsewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430343623
https://github.com/hail-is/hail/pull/4536#issuecomment-429328072:138,Availability,down,downloaded,138,thanks. I had an initial version of the artifacts stuff using `.log` extensions but that wasn't enough! I hypothesized that .log was auto downloaded and never changed it back from no-ext. :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429328072
https://github.com/hail-is/hail/pull/4536#issuecomment-429328072:64,Testability,log,log,64,thanks. I had an initial version of the artifacts stuff using `.log` extensions but that wasn't enough! I hypothesized that .log was auto downloaded and never changed it back from no-ext. :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429328072
https://github.com/hail-is/hail/pull/4536#issuecomment-429328072:125,Testability,log,log,125,thanks. I had an initial version of the artifacts stuff using `.log` extensions but that wasn't enough! I hypothesized that .log was auto downloaded and never changed it back from no-ext. :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429328072
https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:9,Availability,failure,failure,9,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417
https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:292,Availability,failure,failures,292,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417
https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:281,Integrability,depend,dependency,281,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417
https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:171,Security,secur,security,171,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417
https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:4,Testability,test,test,4,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417
https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:308,Testability,test,tests,308,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417
https://github.com/hail-is/hail/issues/4541#issuecomment-429915630:26,Testability,test,tests,26,It seems odd that all the tests passed? And the logs look right? What else was that pod doing?. https://storage.googleapis.com/hail-ci-0-1/ci/ee92f64477f68737987fd8f21411b0348a3d3420/e4ae86ea520fbc5d98b84811b2cdb83640163910/artifacts/index.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4541#issuecomment-429915630
https://github.com/hail-is/hail/issues/4541#issuecomment-429915630:48,Testability,log,logs,48,It seems odd that all the tests passed? And the logs look right? What else was that pod doing?. https://storage.googleapis.com/hail-ci-0-1/ci/ee92f64477f68737987fd8f21411b0348a3d3420/e4ae86ea520fbc5d98b84811b2cdb83640163910/artifacts/index.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4541#issuecomment-429915630
https://github.com/hail-is/hail/issues/4541#issuecomment-429915864:148,Availability,down,downloading,148,It's as if it was just spinning on `exit $BUILD_EXIT`. It had to have finished the last for loop because all the logs open in my browser instead of downloading.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4541#issuecomment-429915864
https://github.com/hail-is/hail/issues/4541#issuecomment-429915864:113,Testability,log,logs,113,It's as if it was just spinning on `exit $BUILD_EXIT`. It had to have finished the last for loop because all the logs open in my browser instead of downloading.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4541#issuecomment-429915864
https://github.com/hail-is/hail/pull/4543#issuecomment-429567973:44,Deployability,pipeline,pipeline,44,"I separated setting the email from enabling pipeline uploads, so people can set the email once and use hl.upload_log() without arguments independent of pipeline upload.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4543#issuecomment-429567973
https://github.com/hail-is/hail/pull/4543#issuecomment-429567973:152,Deployability,pipeline,pipeline,152,"I separated setting the email from enabling pipeline uploads, so people can set the email once and use hl.upload_log() without arguments independent of pipeline upload.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4543#issuecomment-429567973
https://github.com/hail-is/hail/pull/4545#issuecomment-429681232:299,Deployability,update,updated,299,"Pushed some more changes:; - first foray into RBAC; - created service account for batch; - batch run jobs in batch-pods namespace; - authorize with role binding; - hand-tested, batch is working. batch will now be found at `batch.default` instead of `batch` when running from batch-jobs namespace. I updated the batch Client to reflect this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232
https://github.com/hail-is/hail/pull/4545#issuecomment-429681232:133,Security,authoriz,authorize,133,"Pushed some more changes:; - first foray into RBAC; - created service account for batch; - batch run jobs in batch-pods namespace; - authorize with role binding; - hand-tested, batch is working. batch will now be found at `batch.default` instead of `batch` when running from batch-jobs namespace. I updated the batch Client to reflect this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232
https://github.com/hail-is/hail/pull/4545#issuecomment-429681232:169,Testability,test,tested,169,"Pushed some more changes:; - first foray into RBAC; - created service account for batch; - batch run jobs in batch-pods namespace; - authorize with role binding; - hand-tested, batch is working. batch will now be found at `batch.default` instead of `batch` when running from batch-jobs namespace. I updated the batch Client to reflect this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232
https://github.com/hail-is/hail/pull/4551#issuecomment-431455953:32,Testability,test,test,32,"yeah, how do I do this? We need test the new CI, build it, and use the new CI to build the rest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431455953
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:330,Deployability,deploy,deploys,330,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:373,Deployability,deploy,deploys,373,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:83,Testability,test,test,83,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:158,Testability,test,test,158,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:204,Testability,test,test-in-cluster,204,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:250,Testability,test,test,250,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:287,Testability,test,test,287,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:292,Testability,test,test-ci,292,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:345,Testability,test,test,345,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932
https://github.com/hail-is/hail/pull/4551#issuecomment-431459832:198,Security,access,access,198,"IMHO, I'd just merge it if you're happy with everything else. Breaking the build results page only affects developers and it doesn't even really break anything since we can use `gsutil` to directly access the results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431459832
https://github.com/hail-is/hail/pull/4551#issuecomment-433526513:20,Performance,queue,queue,20,"bump, this is in my queue right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-433526513
https://github.com/hail-is/hail/pull/4552#issuecomment-430367241:38,Deployability,deploy,deploy,38,1. Doesn't something need hail-ci-0-1-deploy?. 2. We should also document the names contents of the k8s secrets that package these tokes up. (Imagine someone trying to rebuild the cluster from scratch. Like me.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430367241
https://github.com/hail-is/hail/pull/4552#issuecomment-430393417:34,Deployability,deploy,deploy-hail-is-hail,34,You might be thinking of `hail-ci-deploy-hail-is-hail`? This is the secret mounted by the CI when running a deploy job for `github.com/hail-is/hail`. There is also `ci-deploy-0-1--nealelab-cloudtools`. These are in the `_deploy_secrets` map of `prs.py`. The CI itself does not care what is in these secrets beyond their existence. I'll add a commit documenting the secrets of hail more broadly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430393417
https://github.com/hail-is/hail/pull/4552#issuecomment-430393417:108,Deployability,deploy,deploy,108,You might be thinking of `hail-ci-deploy-hail-is-hail`? This is the secret mounted by the CI when running a deploy job for `github.com/hail-is/hail`. There is also `ci-deploy-0-1--nealelab-cloudtools`. These are in the `_deploy_secrets` map of `prs.py`. The CI itself does not care what is in these secrets beyond their existence. I'll add a commit documenting the secrets of hail more broadly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430393417
https://github.com/hail-is/hail/pull/4552#issuecomment-430393417:168,Deployability,deploy,deploy-,168,You might be thinking of `hail-ci-deploy-hail-is-hail`? This is the secret mounted by the CI when running a deploy job for `github.com/hail-is/hail`. There is also `ci-deploy-0-1--nealelab-cloudtools`. These are in the `_deploy_secrets` map of `prs.py`. The CI itself does not care what is in these secrets beyond their existence. I'll add a commit documenting the secrets of hail more broadly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430393417
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:207,Deployability,deploy,deployment,207,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:815,Energy Efficiency,adapt,adapt,815,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:815,Modifiability,adapt,adapt,815,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:591,Security,access,access,591,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:624,Security,password,password,624,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:633,Security,authenticat,authentication,633,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:707,Security,access,access,707,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:726,Security,password,password,726,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:870,Security,access,access,870,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:1044,Security,access,access,1044,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:749,Testability,test,test,749,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141
https://github.com/hail-is/hail/pull/4554#issuecomment-430455067:116,Performance,load,loader,116,"OK, this should be ready to go. I had to make one more fix, the HailClassLoader parent had to be the Hail jar class loader (via the class loader of HailClassLoader) instead of the system class loader (the default), otherwise AsmFunctionN was not found when executing compiled code. I'm not quite sure why this isn't failing elsewhere. I'm running dockerized vanilla Spark 2.2.0.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4554#issuecomment-430455067
https://github.com/hail-is/hail/pull/4554#issuecomment-430455067:138,Performance,load,loader,138,"OK, this should be ready to go. I had to make one more fix, the HailClassLoader parent had to be the Hail jar class loader (via the class loader of HailClassLoader) instead of the system class loader (the default), otherwise AsmFunctionN was not found when executing compiled code. I'm not quite sure why this isn't failing elsewhere. I'm running dockerized vanilla Spark 2.2.0.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4554#issuecomment-430455067
https://github.com/hail-is/hail/pull/4554#issuecomment-430455067:193,Performance,load,loader,193,"OK, this should be ready to go. I had to make one more fix, the HailClassLoader parent had to be the Hail jar class loader (via the class loader of HailClassLoader) instead of the system class loader (the default), otherwise AsmFunctionN was not found when executing compiled code. I'm not quite sure why this isn't failing elsewhere. I'm running dockerized vanilla Spark 2.2.0.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4554#issuecomment-430455067
https://github.com/hail-is/hail/pull/4555#issuecomment-430448109:161,Availability,error,error,161,"High level and the tests look great. I'll try to take a closer look tonight, but I'm basically ready to approve once the tests pass. I thought about throwing an error on hl.agg.filter if there isn't an aggregator inside, and I think I agree with you now. At least, if you use an hl.agg.filter, etc. inside an aggregation, like hl.agg.sum(hl.agg.filter(...)), that should given error rather than doing nothing. This is important given that this was the old syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109
https://github.com/hail-is/hail/pull/4555#issuecomment-430448109:377,Availability,error,error,377,"High level and the tests look great. I'll try to take a closer look tonight, but I'm basically ready to approve once the tests pass. I thought about throwing an error on hl.agg.filter if there isn't an aggregator inside, and I think I agree with you now. At least, if you use an hl.agg.filter, etc. inside an aggregation, like hl.agg.sum(hl.agg.filter(...)), that should given error rather than doing nothing. This is important given that this was the old syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109
https://github.com/hail-is/hail/pull/4555#issuecomment-430448109:19,Testability,test,tests,19,"High level and the tests look great. I'll try to take a closer look tonight, but I'm basically ready to approve once the tests pass. I thought about throwing an error on hl.agg.filter if there isn't an aggregator inside, and I think I agree with you now. At least, if you use an hl.agg.filter, etc. inside an aggregation, like hl.agg.sum(hl.agg.filter(...)), that should given error rather than doing nothing. This is important given that this was the old syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109
https://github.com/hail-is/hail/pull/4555#issuecomment-430448109:121,Testability,test,tests,121,"High level and the tests look great. I'll try to take a closer look tonight, but I'm basically ready to approve once the tests pass. I thought about throwing an error on hl.agg.filter if there isn't an aggregator inside, and I think I agree with you now. At least, if you use an hl.agg.filter, etc. inside an aggregation, like hl.agg.sum(hl.agg.filter(...)), that should given error rather than doing nothing. This is important given that this was the old syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109
https://github.com/hail-is/hail/pull/4555#issuecomment-430448925:66,Testability,log,logic,66,"Cool. There's some things that I want to rework, namely the split logic for `Group()` (either splitting it into a separate IR node as you suggested or reworking some of the CodeAggregator stuff so that the logic becomes the same), but I think that doesn't affect the workingness of this code so I was going to leave that part as-is for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448925
https://github.com/hail-is/hail/pull/4555#issuecomment-430448925:206,Testability,log,logic,206,"Cool. There's some things that I want to rework, namely the split logic for `Group()` (either splitting it into a separate IR node as you suggested or reworking some of the CodeAggregator stuff so that the logic becomes the same), but I think that doesn't affect the workingness of this code so I was going to leave that part as-is for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448925
https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:338,Availability,avail,available,338,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425
https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:575,Availability,avail,available,575,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425
https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:682,Energy Efficiency,reduce,reduce,682,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425
https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:78,Security,authenticat,authentication,78,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425
https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:427,Security,validat,validate,427,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425
https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:641,Security,validat,validates,641,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425
https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:713,Security,secur,securely,713,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425
https://github.com/hail-is/hail/issues/4556#issuecomment-449082131:133,Security,access,access,133,"This refers to setting up a fresh k8s cluster. In particular, when you're creating a k8s cluster from scratch, you need to create an access token to grant CI (or gateway, if gateway were to proxy requests to GitHub for CI) access to the PR ""merge"" endpoint. There are a couple other things like read access to our google buckets. These can all be generated by anyone with a sufficiently privileged GCP account and a broad login in the `hail` unix group (there are some credentials stored in cotton's home directory on the broad file system).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449082131
https://github.com/hail-is/hail/issues/4556#issuecomment-449082131:223,Security,access,access,223,"This refers to setting up a fresh k8s cluster. In particular, when you're creating a k8s cluster from scratch, you need to create an access token to grant CI (or gateway, if gateway were to proxy requests to GitHub for CI) access to the PR ""merge"" endpoint. There are a couple other things like read access to our google buckets. These can all be generated by anyone with a sufficiently privileged GCP account and a broad login in the `hail` unix group (there are some credentials stored in cotton's home directory on the broad file system).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449082131
https://github.com/hail-is/hail/issues/4556#issuecomment-449082131:300,Security,access,access,300,"This refers to setting up a fresh k8s cluster. In particular, when you're creating a k8s cluster from scratch, you need to create an access token to grant CI (or gateway, if gateway were to proxy requests to GitHub for CI) access to the PR ""merge"" endpoint. There are a couple other things like read access to our google buckets. These can all be generated by anyone with a sufficiently privileged GCP account and a broad login in the `hail` unix group (there are some credentials stored in cotton's home directory on the broad file system).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449082131
https://github.com/hail-is/hail/issues/4556#issuecomment-449082131:422,Testability,log,login,422,"This refers to setting up a fresh k8s cluster. In particular, when you're creating a k8s cluster from scratch, you need to create an access token to grant CI (or gateway, if gateway were to proxy requests to GitHub for CI) access to the PR ""merge"" endpoint. There are a couple other things like read access to our google buckets. These can all be generated by anyone with a sufficiently privileged GCP account and a broad login in the `hail` unix group (there are some credentials stored in cotton's home directory on the broad file system).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449082131
https://github.com/hail-is/hail/pull/4568#issuecomment-430784918:163,Deployability,update,updated,163,@cseed Can you take a quick look and make sure this is what you were envisioning?. Not sure if the environment passed in the Table and Matrix IR parsing should be updated with the new bindings or if the refMap from the Table or Matrix IR should replace the existing environment's refMap.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4568#issuecomment-430784918
https://github.com/hail-is/hail/pull/4572#issuecomment-430813198:70,Availability,alive,alive,70,Doing and administrator merge override because I'm afraid CI won't be alive long enough to successfully test this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4572#issuecomment-430813198
https://github.com/hail-is/hail/pull/4572#issuecomment-430813198:104,Testability,test,test,104,Doing and administrator merge override because I'm afraid CI won't be alive long enough to successfully test this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4572#issuecomment-430813198
https://github.com/hail-is/hail/pull/4573#issuecomment-431244990:82,Deployability,deploy,deployed-sha,82,"No, you're totally right, good catch! Fixed. How did this ever work? Because `get-deployed-sha.sh` was universally broken? I also added the (recently added) spark project to the list of projects.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4573#issuecomment-431244990
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:1205,Availability,down,down,1205,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:1104,Deployability,update,update,1104,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:953,Energy Efficiency,schedul,schedule,953,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:1460,Integrability,rout,routes,1460,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:993,Performance,optimiz,optimize,993,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:613,Safety,timeout,timeouts,613,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:364,Security,password,password,364,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:1160,Usability,simpl,simple,1160,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
https://github.com/hail-is/hail/pull/4576#issuecomment-431038364:24,Safety,avoid,avoiding,24,Definitely agree w.r.t. avoiding acronyms and codenames.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431038364
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1236,Availability,down,down,1236,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1445,Availability,toler,tolerate,1445,"ook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:2225,Availability,down,down,2225,"hink for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I thought we decided we; > preferred that. I thought it would take less time to get a subdirectory working than figure out; how to add a new domain and a cert and deal with DNS. Long term a subdomain makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1669,Deployability,update,update,1669,"information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1291,Energy Efficiency,schedul,scheduled,1291,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1371,Energy Efficiency,schedul,schedule,1371,"ain website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1504,Energy Efficiency,schedul,scheduled,1504,"ook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:2509,Integrability,rout,routes,2509,"hink for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I thought we decided we; > preferred that. I thought it would take less time to get a subdirectory working than figure out; how to add a new domain and a cert and deal with DNS. Long term a subdomain makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1555,Performance,optimiz,optimize,1555,"information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:477,Safety,avoid,avoid,477,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:884,Safety,timeout,timeouts,884,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1187,Safety,timeout,timeouts,1187,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1264,Safety,timeout,timeouts,1264,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:522,Security,password,password,522,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:796,Security,password,password,796,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:2177,Usability,simpl,simple,2177,"hink for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I thought we decided we; > preferred that. I thought it would take less time to get a subdirectory working than figure out; how to add a new domain and a cert and deal with DNS. Long term a subdomain makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1317,Deployability,update,updates-in-kubernetes-,1317,"'t be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:323,Energy Efficiency,schedul,scheduled,323,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:987,Energy Efficiency,schedul,schedule,987,"ord. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it wou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:877,Integrability,rout,routinely,877,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1925,Integrability,depend,depending,1925," (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write up instructions for adding a new domain to get certs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1259,Performance,cache,cache,1259,"r both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1305,Performance,scalab,scalability-updates-in-kubernetes-,1305,"'t be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:296,Safety,timeout,timeouts,296,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:36,Security,password,password,36,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1701,Security,password,password,1701," (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write up instructions for adding a new domain to get certs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:634,Usability,responsiv,responsive,634,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:655,Usability,feedback,feedback,655,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:669,Usability,responsiv,responsive,669,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:696,Usability,responsiv,responsive,696,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:723,Usability,responsiv,responsive,723,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:810,Usability,clear,clear,810,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:816,Usability,feedback,feedback,816,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
https://github.com/hail-is/hail/issues/4578#issuecomment-431602419:57,Deployability,deploy,deployment,57,which do you mean?. - switch our exclusive spark testing/deployment from 2.2.0 to 2.3.X?; - test and deploy against 2.3.X as well as 2.2.0?; - something else?. I know people are using Hail with 2.3 successfully right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431602419
https://github.com/hail-is/hail/issues/4578#issuecomment-431602419:101,Deployability,deploy,deploy,101,which do you mean?. - switch our exclusive spark testing/deployment from 2.2.0 to 2.3.X?; - test and deploy against 2.3.X as well as 2.2.0?; - something else?. I know people are using Hail with 2.3 successfully right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431602419
https://github.com/hail-is/hail/issues/4578#issuecomment-431602419:49,Testability,test,testing,49,which do you mean?. - switch our exclusive spark testing/deployment from 2.2.0 to 2.3.X?; - test and deploy against 2.3.X as well as 2.2.0?; - something else?. I know people are using Hail with 2.3 successfully right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431602419
https://github.com/hail-is/hail/issues/4578#issuecomment-431602419:92,Testability,test,test,92,which do you mean?. - switch our exclusive spark testing/deployment from 2.2.0 to 2.3.X?; - test and deploy against 2.3.X as well as 2.2.0?; - something else?. I know people are using Hail with 2.3 successfully right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431602419
https://github.com/hail-is/hail/issues/4578#issuecomment-431608757:128,Deployability,upgrade,upgrade,128,"It doesn't build out of the box, but it should. That just needs py4j and breeze version numbers. Separately, we should probably upgrade cloudtools to Dataproc 1.3-deb9 which has Spark 2.3.1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431608757
https://github.com/hail-is/hail/pull/4583#issuecomment-431457123:71,Deployability,deploy,deploy,71,Overriding and merging directly so it definitely gets included in next deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4583#issuecomment-431457123
https://github.com/hail-is/hail/pull/4585#issuecomment-431703672:57,Testability,test,test,57,"OK, I finally understand why this code change causes the test to pass, and am convinced that this is the wrong change. I removed the assertion in `upcast` (for value IRs) that checks that the requested type is a supertype of the IR type -- this assertion will definitely be violated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-431703672
https://github.com/hail-is/hail/pull/4585#issuecomment-431703672:133,Testability,assert,assertion,133,"OK, I finally understand why this code change causes the test to pass, and am convinced that this is the wrong change. I removed the assertion in `upcast` (for value IRs) that checks that the requested type is a supertype of the IR type -- this assertion will definitely be violated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-431703672
https://github.com/hail-is/hail/pull/4585#issuecomment-431703672:245,Testability,assert,assertion,245,"OK, I finally understand why this code change causes the test to pass, and am convinced that this is the wrong change. I removed the assertion in `upcast` (for value IRs) that checks that the requested type is a supertype of the IR type -- this assertion will definitely be violated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-431703672
https://github.com/hail-is/hail/pull/4585#issuecomment-431703700:20,Testability,assert,assertion,20,let's add back that assertion for now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-431703700
https://github.com/hail-is/hail/pull/4585#issuecomment-434111128:82,Availability,failure,failures,82,I had done this a while ago in a throwaway after talking to Tim. This causes more failures than just the new test that I added. I'm at a loss for how to proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-434111128
https://github.com/hail-is/hail/pull/4585#issuecomment-434111128:109,Testability,test,test,109,I had done this a while ago in a throwaway after talking to Tim. This causes more failures than just the new test that I added. I'm at a loss for how to proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-434111128
https://github.com/hail-is/hail/pull/4585#issuecomment-434121212:167,Availability,failure,failures,167,let's meet tomorrow either before lab meeting (~9:40) or after lunch to make a plan? There's definitely a problem with the way something is getting rebuilt. The extra failures should be things we can resolve as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-434121212
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:387,Availability,failure,failures,387,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:489,Availability,failure,failures,489,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:762,Availability,error,errors,762,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:30,Deployability,patch,patch,30,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:419,Security,attack,attack,419,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:46,Testability,test,test,46,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:360,Testability,assert,assertion,360,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:466,Testability,assert,assertion,466,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:521,Testability,test,tests,521,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:553,Testability,test,test,553,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:671,Testability,assert,assertions,671,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474
https://github.com/hail-is/hail/pull/4586#issuecomment-433087824:45,Testability,test,tests,45,I'm pretty sure this is broken and the batch tests are going into an infinite loop.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-433087824
https://github.com/hail-is/hail/pull/4586#issuecomment-434410954:22,Safety,timeout,timeout,22,I changed the default timeout to 60s.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-434410954
https://github.com/hail-is/hail/pull/4586#issuecomment-434412373:137,Availability,error,error,137,I also switched the `api` argument to `BatchClient` to `default_api` rather than `None`. I wonder if that was somehow creating a muffled error message. It should have erred as soon as someone tried to make a request with `api=None`. Not sure why it looped forever instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373
https://github.com/hail-is/hail/pull/4586#issuecomment-434412373:143,Integrability,message,message,143,I also switched the `api` argument to `BatchClient` to `default_api` rather than `None`. I wonder if that was somehow creating a muffled error message. It should have erred as soon as someone tried to make a request with `api=None`. Not sure why it looped forever instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373
https://github.com/hail-is/hail/issues/4591#issuecomment-431590377:108,Testability,log,log,108,I restarted hail ci and then saw batch claim to create jobs that never appear in `kubectl get pods`. [batch.log](https://github.com/hail-is/hail/files/2498270/batch.log); [hail-ci.log](https://github.com/hail-is/hail/files/2498271/hail-ci.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4591#issuecomment-431590377
https://github.com/hail-is/hail/issues/4591#issuecomment-431590377:165,Testability,log,log,165,I restarted hail ci and then saw batch claim to create jobs that never appear in `kubectl get pods`. [batch.log](https://github.com/hail-is/hail/files/2498270/batch.log); [hail-ci.log](https://github.com/hail-is/hail/files/2498271/hail-ci.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4591#issuecomment-431590377
https://github.com/hail-is/hail/issues/4591#issuecomment-431590377:180,Testability,log,log,180,I restarted hail ci and then saw batch claim to create jobs that never appear in `kubectl get pods`. [batch.log](https://github.com/hail-is/hail/files/2498270/batch.log); [hail-ci.log](https://github.com/hail-is/hail/files/2498271/hail-ci.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4591#issuecomment-431590377
https://github.com/hail-is/hail/issues/4591#issuecomment-431590377:239,Testability,log,log,239,I restarted hail ci and then saw batch claim to create jobs that never appear in `kubectl get pods`. [batch.log](https://github.com/hail-is/hail/files/2498270/batch.log); [hail-ci.log](https://github.com/hail-is/hail/files/2498271/hail-ci.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4591#issuecomment-431590377
https://github.com/hail-is/hail/issues/4592#issuecomment-431594049:14,Deployability,deploy,deploy,14,"Moreover, the deploy jobs keep trying to access things in the batch-pods namespace, but the things they need to deploy are in the regular batch namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049
https://github.com/hail-is/hail/issues/4592#issuecomment-431594049:112,Deployability,deploy,deploy,112,"Moreover, the deploy jobs keep trying to access things in the batch-pods namespace, but the things they need to deploy are in the regular batch namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049
https://github.com/hail-is/hail/issues/4592#issuecomment-431594049:41,Security,access,access,41,"Moreover, the deploy jobs keep trying to access things in the batch-pods namespace, but the things they need to deploy are in the regular batch namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049
https://github.com/hail-is/hail/issues/4592#issuecomment-431594790:14,Deployability,deploy,deployed,14,"I've manually deployed site to fix the broken links, but site will continue to point at 05a792599, while hail itself will successfully deploy (because it is deployed before batch is).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594790
https://github.com/hail-is/hail/issues/4592#issuecomment-431594790:135,Deployability,deploy,deploy,135,"I've manually deployed site to fix the broken links, but site will continue to point at 05a792599, while hail itself will successfully deploy (because it is deployed before batch is).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594790
https://github.com/hail-is/hail/issues/4592#issuecomment-431594790:157,Deployability,deploy,deployed,157,"I've manually deployed site to fix the broken links, but site will continue to point at 05a792599, while hail itself will successfully deploy (because it is deployed before batch is).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594790
https://github.com/hail-is/hail/issues/4592#issuecomment-432499924:6,Deployability,deploy,deploy,6,Fully deploy is now working again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-432499924
https://github.com/hail-is/hail/pull/4594#issuecomment-433123340:80,Deployability,configurat,configuration,80,Code looks good. It looks like there's something wrong with the gradle cpp test configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340
https://github.com/hail-is/hail/pull/4594#issuecomment-433123340:80,Modifiability,config,configuration,80,Code looks good. It looks like there's something wrong with the gradle cpp test configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340
https://github.com/hail-is/hail/pull/4594#issuecomment-433123340:75,Testability,test,test,75,Code looks good. It looks like there's something wrong with the gradle cpp test configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340
https://github.com/hail-is/hail/pull/4596#issuecomment-431613221:2,Deployability,deploy,deployed,2,"I deployed this to the cluster. There was a permissions bootstrap issue that batch could never deploy something that could deploy stuff, because it itself didn't have deploy privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221
https://github.com/hail-is/hail/pull/4596#issuecomment-431613221:95,Deployability,deploy,deploy,95,"I deployed this to the cluster. There was a permissions bootstrap issue that batch could never deploy something that could deploy stuff, because it itself didn't have deploy privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221
https://github.com/hail-is/hail/pull/4596#issuecomment-431613221:123,Deployability,deploy,deploy,123,"I deployed this to the cluster. There was a permissions bootstrap issue that batch could never deploy something that could deploy stuff, because it itself didn't have deploy privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221
https://github.com/hail-is/hail/pull/4596#issuecomment-431613221:167,Deployability,deploy,deploy,167,"I deployed this to the cluster. There was a permissions bootstrap issue that batch could never deploy something that could deploy stuff, because it itself didn't have deploy privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221
https://github.com/hail-is/hail/pull/4596#issuecomment-431735363:13,Testability,test,test,13,"Also added:. test namespace; test-svc service account with all privileges in test, use for test batch jobs by CI; disabled livenessProbe while restarting creates new pods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431735363
https://github.com/hail-is/hail/pull/4596#issuecomment-431735363:29,Testability,test,test-svc,29,"Also added:. test namespace; test-svc service account with all privileges in test, use for test batch jobs by CI; disabled livenessProbe while restarting creates new pods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431735363
https://github.com/hail-is/hail/pull/4596#issuecomment-431735363:77,Testability,test,test,77,"Also added:. test namespace; test-svc service account with all privileges in test, use for test batch jobs by CI; disabled livenessProbe while restarting creates new pods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431735363
https://github.com/hail-is/hail/pull/4596#issuecomment-431735363:91,Testability,test,test,91,"Also added:. test namespace; test-svc service account with all privileges in test, use for test batch jobs by CI; disabled livenessProbe while restarting creates new pods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431735363
https://github.com/hail-is/hail/pull/4601#issuecomment-431987841:22,Deployability,deploy,deploy-svc,22,I'll approve once the deploy-svc one goes in,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4601#issuecomment-431987841
https://github.com/hail-is/hail/pull/4603#issuecomment-431977104:46,Deployability,install,installers,46,@chrisvittal can you take a look at the linux installers? installing docker on linux seems really involved so I left that out.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-431977104
https://github.com/hail-is/hail/pull/4603#issuecomment-431977104:58,Deployability,install,installing,58,@chrisvittal can you take a look at the linux installers? installing docker on linux seems really involved so I left that out.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-431977104
https://github.com/hail-is/hail/pull/4603#issuecomment-435163425:80,Deployability,install,installation,80,@danking Thank you. This looks good for now. I'll prepare a follow on for linux installation.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435163425
https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:198,Deployability,deploy,deploy,198,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023
https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:14,Security,Access,AccessDeniedException,14,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023
https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:116,Security,access,access,116,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023
https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:134,Testability,test,test,134,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023
https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:153,Testability,test,tests,153,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023
https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:176,Testability,test,test,176,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023
https://github.com/hail-is/hail/issues/4605#issuecomment-433531464:89,Performance,queue,queue,89,"We opened this because we were worried that if a PR becomes unapproved it will block the queue for other approved PRs. This does not happen. When a PR is unapproved, the next refresh will realize that no approved PRs are running, and it will kick off another build. This is not instantaneous (as we might like) but it happens within the refresh duration, which seems fine. cc: @cseed, I think our current operational state is fine and the marginal benefit of eagerly starting a new job is not worth the effort.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4605#issuecomment-433531464
https://github.com/hail-is/hail/pull/4606#issuecomment-432055173:26,Testability,test,test,26,does it fix a bug? can we test it?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4606#issuecomment-432055173
https://github.com/hail-is/hail/pull/4606#issuecomment-432209343:99,Testability,test,test,99,"Yes, it fixes a bug that resulted in a failed check on the dimension of y. I've added a regression test. http://discuss.hail.is/t/breaking-change-hail-0-2-removed-kinshipmatrix-linear-mixed-regression/578/16",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4606#issuecomment-432209343
https://github.com/hail-is/hail/issues/4607#issuecomment-432082417:192,Availability,ERROR,ERROR,192,"Eventually the status calls returned, but something caused them to hang?; ```; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ERROR	| 2018-10-23 03:55:38,215 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); INFO	| 2018-10-23 03:59:30,821 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:59:30,826 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,828 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,830 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417
https://github.com/hail-is/hail/issues/4607#issuecomment-432082417:379,Availability,ERROR,ERROR,379,"Eventually the status calls returned, but something caused them to hang?; ```; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ERROR	| 2018-10-23 03:55:38,215 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); INFO	| 2018-10-23 03:59:30,821 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:59:30,826 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,828 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,830 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417
https://github.com/hail-is/hail/issues/4607#issuecomment-432082417:365,Safety,timeout,timeout,365,"Eventually the status calls returned, but something caused them to hang?; ```; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ERROR	| 2018-10-23 03:55:38,215 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); INFO	| 2018-10-23 03:59:30,821 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:59:30,826 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,828 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,830 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417
https://github.com/hail-is/hail/issues/4607#issuecomment-432082417:552,Safety,timeout,timeout,552,"Eventually the status calls returned, but something caused them to hang?; ```; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ERROR	| 2018-10-23 03:55:38,215 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); INFO	| 2018-10-23 03:59:30,821 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:59:30,826 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,828 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,830 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1717,Energy Efficiency,adapt,adapter,1717,"eadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1798,Energy Efficiency,adapt,adapters,1798,"llib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1717,Integrability,adapter,adapter,1717,"eadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1798,Integrability,adapter,adapters,1798,"llib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1717,Modifiability,adapt,adapter,1717,"eadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1798,Modifiability,adapt,adapters,1798,"llib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:757,Safety,timeout,timeout,757,"[batch.log](https://github.com/hail-is/hail/files/2504453/batch.log). Here's a snippet:; ```; INFO | 2018-10-23 03:27:48,536 | server.py | mark_complete:184 | job 151 complete, exit_code 2; INFO | 2018-10-23 03:27:48,541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:896,Safety,timeout,timeout,896,"[batch.log](https://github.com/hail-is/hail/files/2504453/batch.log). Here's a snippet:; ```; INFO | 2018-10-23 03:27:48,536 | server.py | mark_complete:184 | job 151 complete, exit_code 2; INFO | 2018-10-23 03:27:48,541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1222,Safety,timeout,timeout,1222,"541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1974,Safety,timeout,timeout,1974,"cent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,816 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,817 | server.py | refresh_k8s_state:385 | started k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:7,Testability,log,log,7,"[batch.log](https://github.com/hail-is/hail/files/2504453/batch.log). Here's a snippet:; ```; INFO | 2018-10-23 03:27:48,536 | server.py | mark_complete:184 | job 151 complete, exit_code 2; INFO | 2018-10-23 03:27:48,541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:64,Testability,log,log,64,"[batch.log](https://github.com/hail-is/hail/files/2504453/batch.log). Here's a snippet:; ```; INFO | 2018-10-23 03:27:48,536 | server.py | mark_complete:184 | job 151 complete, exit_code 2; INFO | 2018-10-23 03:27:48,541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:2163,Testability,log,log,2163,"tp://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,816 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,817 | server.py | refresh_k8s_state:385 | started k8s state refresh; INFO | 2018-10-23 03:59:31,183 | server.py | _create_pod:80 | created pod name: job-99-f4gwv for job 99; INFO | 2018-10-23 03:59:31,191 | server.py ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:2182,Testability,log,logs,2182,"tp://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,816 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,817 | server.py | refresh_k8s_state:385 | started k8s state refresh; INFO | 2018-10-23 03:59:31,183 | server.py | _create_pod:80 | created pod name: job-99-f4gwv for job 99; INFO | 2018-10-23 03:59:31,191 | server.py ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:2195,Testability,log,log,2195,"me}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,816 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,817 | server.py | refresh_k8s_state:385 | started k8s state refresh; INFO | 2018-10-23 03:59:31,183 | server.py | _create_pod:80 | created pod name: job-99-f4gwv for job 99; INFO | 2018-10-23 03:59:31,191 | server.py | _create_pod:80 | created pod name: job-150-h6snc for",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038
https://github.com/hail-is/hail/issues/4608#issuecomment-443356287:23,Testability,log,log,23,Happened again; [batch.log](https://github.com/hail-is/hail/files/2635158/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-443356287
https://github.com/hail-is/hail/issues/4608#issuecomment-443356287:80,Testability,log,log,80,Happened again; [batch.log](https://github.com/hail-is/hail/files/2635158/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-443356287
https://github.com/hail-is/hail/issues/4609#issuecomment-432084571:116,Deployability,deploy,deploy,116,"I gotta go to bed but it looks like maybe we need more role bindings all for `""get""` for all the objects we want to deploy in `batch-pods`? Or maybe we're applying that `deployment.yaml` to the wrong namespace? That one should be deployed in the default namespace. So maybe the answer is to ignore this until that PR gets merged? At this point I think we should just force merge the PR @cseed. NB: this is still without the `deploy-svc` PR merged due to some other yet undiagnosed weirdness, see #4608 for a taste.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571
https://github.com/hail-is/hail/issues/4609#issuecomment-432084571:170,Deployability,deploy,deployment,170,"I gotta go to bed but it looks like maybe we need more role bindings all for `""get""` for all the objects we want to deploy in `batch-pods`? Or maybe we're applying that `deployment.yaml` to the wrong namespace? That one should be deployed in the default namespace. So maybe the answer is to ignore this until that PR gets merged? At this point I think we should just force merge the PR @cseed. NB: this is still without the `deploy-svc` PR merged due to some other yet undiagnosed weirdness, see #4608 for a taste.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571
https://github.com/hail-is/hail/issues/4609#issuecomment-432084571:230,Deployability,deploy,deployed,230,"I gotta go to bed but it looks like maybe we need more role bindings all for `""get""` for all the objects we want to deploy in `batch-pods`? Or maybe we're applying that `deployment.yaml` to the wrong namespace? That one should be deployed in the default namespace. So maybe the answer is to ignore this until that PR gets merged? At this point I think we should just force merge the PR @cseed. NB: this is still without the `deploy-svc` PR merged due to some other yet undiagnosed weirdness, see #4608 for a taste.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571
https://github.com/hail-is/hail/issues/4609#issuecomment-432084571:425,Deployability,deploy,deploy-svc,425,"I gotta go to bed but it looks like maybe we need more role bindings all for `""get""` for all the objects we want to deploy in `batch-pods`? Or maybe we're applying that `deployment.yaml` to the wrong namespace? That one should be deployed in the default namespace. So maybe the answer is to ignore this until that PR gets merged? At this point I think we should just force merge the PR @cseed. NB: this is still without the `deploy-svc` PR merged due to some other yet undiagnosed weirdness, see #4608 for a taste.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:268,Availability,echo,echo,268,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:545,Availability,Error,Error,545,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:576,Availability,error,error,576,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1680,Availability,Error,Error,1680,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1711,Availability,error,error,1711,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2499,Availability,Error,Error,2499,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:6,Deployability,deploy,deployment,6,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:473,Deployability,deploy,deployment,473,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:494,Deployability,deploy,deployment,494,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:528,Deployability,deploy,deployment,528,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:606,Deployability,configurat,configuration,606,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:659,Deployability,deploy,deployments,659,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:711,Deployability,Deploy,Deployment,711,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:737,Deployability,deploy,deployment,737,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:825,Deployability,Deploy,Deployment,825,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:923,Deployability,deploy,deployment,923,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1014,Deployability,configurat,configuration,1014,"507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1442,Deployability,deploy,deployment,1442,"56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""b",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1460,Deployability,deploy,deployments,1460,"ge),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1484,Deployability,deploy,deployment,1484,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1549,Deployability,deploy,deploy-svc,1549,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1572,Deployability,deploy,deployments,1572,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1667,Deployability,deploy,deploy-svc,1667,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1741,Deployability,configurat,configuration,1741,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2061,Deployability,configurat,configuration,2061,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2209,Deployability,deploy,deployment,2209,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2297,Deployability,deploy,deploy-svc,2297,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2407,Deployability,deploy,deploy-svc,2407,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2452,Deployability,deploy,deploy-batch,2452,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2485,Deployability,deploy,deploy-batch,2485,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2106,Integrability,protocol,protocol,2106,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:606,Modifiability,config,configuration,606,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1014,Modifiability,config,configuration,1014,"507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1741,Modifiability,config,configuration,1741,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2061,Modifiability,config,configuration,2061,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:42,Testability,log,log,42,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:99,Testability,log,log,99,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914
https://github.com/hail-is/hail/issues/4609#issuecomment-432381399:54,Deployability,deploy,deployment,54,I don't understand why Kubernetes wants to put `batch-deployment` and the service `batch` in the `batch-pods` namespace.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432381399
https://github.com/hail-is/hail/issues/4609#issuecomment-432384646:107,Deployability,deploy,deploy-svc,107,"Ah, I bet that service accounts default all their API requests to the namespace they are a member of. For `deploy-svc` that's `batch-pods`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432384646
https://github.com/hail-is/hail/pull/4616#issuecomment-432424552:46,Deployability,deploy,deploy,46,Override merging this because it only touches deploy things (which aren't tested) and it was waiting on other approved PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4616#issuecomment-432424552
https://github.com/hail-is/hail/pull/4616#issuecomment-432424552:74,Testability,test,tested,74,Override merging this because it only touches deploy things (which aren't tested) and it was waiting on other approved PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4616#issuecomment-432424552
https://github.com/hail-is/hail/pull/4618#issuecomment-432384741:135,Deployability,deploy,deploy-svc,135,"Copying from issue #4609:; > Ah, I bet that service accounts default all their API requests to the namespace they are a member of. For deploy-svc that's batch-pods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4618#issuecomment-432384741
https://github.com/hail-is/hail/pull/4618#issuecomment-432384853:49,Deployability,deploy,deployments,49,We should be explicit about namespace in all our deployments,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4618#issuecomment-432384853
https://github.com/hail-is/hail/issues/4620#issuecomment-451593136:63,Testability,log,log,63,Eh. This has gotten pretty easy with `http://ci.hail.is/ui/job-log/NUMBER`. I'm not sure the rest of this is all that important.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4620#issuecomment-451593136
https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:784,Availability,down,down,784,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:560,Deployability,rolling,rolling,560,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:568,Deployability,update,updates,568,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:816,Deployability,update,updates,816,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:140,Modifiability,config,config,140,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:80,Security,encrypt,encrypt,80,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:224,Usability,simpl,simpler,224,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:280,Availability,avail,available,280,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145
https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:140,Deployability,release,release-notes,140,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145
https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:219,Deployability,release,released,219,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145
https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:303,Deployability,release,release,303,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145
https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:340,Deployability,release,released,340,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145
https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:165,Security,Access,Access,165,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145
https://github.com/hail-is/hail/pull/4624#issuecomment-433179293:220,Deployability,update,update,220,"I addressed the comments, except for the following, which I plan to do as separate PRs:; - ~~put list of domains in a file~~,; - least privileged: gateway-service-account (with certs read) and letsencrypt-sa (with certs update),; - second process in gateway pod to bump nginx on cert change",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-433179293
https://github.com/hail-is/hail/pull/4630#issuecomment-432739112:18,Testability,test,tests,18,"Also, do you have tests to know this is working?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4630#issuecomment-432739112
https://github.com/hail-is/hail/pull/4644#issuecomment-433528370:148,Security,access,access,148,"@tpoterba I don't quite see why anyone should use `hadoop_copy` if their tool supports exporting to a file handle. Unless they're doing some random access, but I think that's much rarer than `df.to_csv(f)` and using `hadoop_copy` for that is silly (AFAIK?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4644#issuecomment-433528370
https://github.com/hail-is/hail/pull/4644#issuecomment-433548391:26,Usability,clear,cleared,26,"Ok, I added a caution and cleared up the note. I chose 50 MB as the biggest recommended file size since that will take ~5 seconds to write.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4644#issuecomment-433548391
https://github.com/hail-is/hail/pull/4645#issuecomment-433180843:65,Deployability,deploy,deploy,65,I tested this by applying it to the cluster directly (I ran make deploy in the `gateway/` project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4645#issuecomment-433180843
https://github.com/hail-is/hail/pull/4645#issuecomment-433180843:139,Deployability,deploy,deploy,139,I tested this by applying it to the cluster directly (I ran make deploy in the `gateway/` project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4645#issuecomment-433180843
https://github.com/hail-is/hail/pull/4645#issuecomment-433180843:2,Testability,test,tested,2,I tested this by applying it to the cluster directly (I ran make deploy in the `gateway/` project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4645#issuecomment-433180843
https://github.com/hail-is/hail/issues/4646#issuecomment-433468727:767,Performance,optimiz,optimize,767,"The strictify is needed to ensure that the resulting entriesRVD is ordered by [row key, col key]. If the row key is empty, the only way to do this would be to explode the rows then shuffle to sort by col key. I'm not sure what behavior we want for this. The empty row key is just the extreme case of having too coarse a row key, so that the rows having a given row key are too large to fit one partition and sort the entries by [row key, col key]. Until we can dynamically estimate the data sizes, we have to make some assumption about when a key's worth of data fits on a partition, and otherwise we have to shuffle. If the col key was also empty, we could do this without a shuffle, because there is no ordering requirement on the entriesRVD. I could recognize and optimize that case as a partial solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433468727
https://github.com/hail-is/hail/issues/4646#issuecomment-433472683:209,Availability,error,error,209,"after a few more minutes of thought, a warning is probably not sufficient here. We have to assume that losing parallelism means that the computation will never finish. There aren't really any good options:. - error; - unkey the cols too; - shuffle",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433472683
https://github.com/hail-is/hail/issues/4646#issuecomment-433473971:109,Availability,error,error,109,Unkey the cols seems wrong. It makes the contract of `entries()` much more complicated. I guess a warning or error that makes the user decide between manually unkeying the columns or shuffling would be best? (I guess that means implementing a shuffling option for small data.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433473971
https://github.com/hail-is/hail/issues/4646#issuecomment-433473971:41,Integrability,contract,contract,41,Unkey the cols seems wrong. It makes the contract of `entries()` much more complicated. I guess a warning or error that makes the user decide between manually unkeying the columns or shuffling would be best? (I guess that means implementing a shuffling option for small data.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433473971
https://github.com/hail-is/hail/pull/4647#issuecomment-433188993:31,Deployability,deploy,deploy,31,"I tested this by running `make deploy` in the `image-fetcher/` directory. You can take a look at the pods with `kubectl`, they are in the default namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4647#issuecomment-433188993
https://github.com/hail-is/hail/pull/4647#issuecomment-433188993:2,Testability,test,tested,2,"I tested this by running `make deploy` in the `image-fetcher/` directory. You can take a look at the pods with `kubectl`, they are in the default namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4647#issuecomment-433188993
https://github.com/hail-is/hail/pull/4647#issuecomment-433222303:26,Integrability,depend,dependent,26,"@tpoterba this one is not dependent on the other two. When this one AND `notebook-server` lands, I need to add a fourth PR that hooks up the notebook server's image to this image-fetcher. What I really want is to be able to have a DAG in git, like:. ```; notebook-gateway <-- notebook-server <-- notebook-worker-image-fetch; |; notebook-image-fetcher <-----------------------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4647#issuecomment-433222303
https://github.com/hail-is/hail/pull/4648#issuecomment-433191315:65,Deployability,deploy,deploy,65,I tested this by applying it to the cluster directly (I ran make deploy in the gateway/ project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4648#issuecomment-433191315
https://github.com/hail-is/hail/pull/4648#issuecomment-433191315:137,Deployability,deploy,deploy,137,I tested this by applying it to the cluster directly (I ran make deploy in the gateway/ project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4648#issuecomment-433191315
https://github.com/hail-is/hail/pull/4648#issuecomment-433191315:2,Testability,test,tested,2,I tested this by applying it to the cluster directly (I ran make deploy in the gateway/ project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4648#issuecomment-433191315
https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:402,Availability,down,download,402,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724
https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:118,Deployability,install,install,118,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724
https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:144,Deployability,install,install,144,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724
https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:232,Integrability,depend,dependency,232,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724
https://github.com/hail-is/hail/issues/4653#issuecomment-434014181:25,Integrability,depend,dependency,25,Avoided by removing this dependency in #4659,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4653#issuecomment-434014181
https://github.com/hail-is/hail/issues/4653#issuecomment-434014181:0,Safety,Avoid,Avoided,0,Avoided by removing this dependency in #4659,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4653#issuecomment-434014181
https://github.com/hail-is/hail/pull/4659#issuecomment-433570508:0,Testability,Test,Testing,0,"Testing this is quite annoying, but it would be thoroughly tested by a Chaos Monkey in a staging set up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4659#issuecomment-433570508
https://github.com/hail-is/hail/pull/4659#issuecomment-433570508:59,Testability,test,tested,59,"Testing this is quite annoying, but it would be thoroughly tested by a Chaos Monkey in a staging set up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4659#issuecomment-433570508
https://github.com/hail-is/hail/pull/4663#issuecomment-433775154:220,Deployability,pipeline,pipeline,220,"Added support for a few more nodes, including {Insert, Select}Fields and Array{Range, Map, Filter} (deforested). I'm going to do ordering next. That should give us enough interesting stuff to play with when simple Table pipeline start working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-433775154
https://github.com/hail-is/hail/pull/4663#issuecomment-433775154:207,Usability,simpl,simple,207,"Added support for a few more nodes, including {Insert, Select}Fields and Array{Range, Map, Filter} (deforested). I'm going to do ordering next. That should give us enough interesting stuff to play with when simple Table pipeline start working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-433775154
https://github.com/hail-is/hail/pull/4663#issuecomment-434401329:19,Testability,test,tests,19,@cseed do you have tests for this? I don't see any right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-434401329
https://github.com/hail-is/hail/pull/4663#issuecomment-434401739:39,Testability,assert,assertEvalsTo,39,"oh wait, I see. they got bundled into `assertEvalsTo`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-434401739
https://github.com/hail-is/hail/pull/4663#issuecomment-434466425:24,Testability,assert,assertEvalsTo,24,"> they got bundled into assertEvalsTo. Yes, and assertEvalSame. Quite a bit of stuff is running through that, and it is definitely getting tested because I had to squash a bunch of bugs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-434466425
https://github.com/hail-is/hail/pull/4663#issuecomment-434466425:48,Testability,assert,assertEvalSame,48,"> they got bundled into assertEvalsTo. Yes, and assertEvalSame. Quite a bit of stuff is running through that, and it is definitely getting tested because I had to squash a bunch of bugs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-434466425
https://github.com/hail-is/hail/pull/4663#issuecomment-434466425:139,Testability,test,tested,139,"> they got bundled into assertEvalsTo. Yes, and assertEvalSame. Quite a bit of stuff is running through that, and it is definitely getting tested because I had to squash a bunch of bugs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-434466425
https://github.com/hail-is/hail/pull/4663#issuecomment-434492824:73,Deployability,update,updated,73,"ah, missed a comment--is there a reason the linux prebuilts needed to be updated?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-434492824
https://github.com/hail-is/hail/pull/4667#issuecomment-433727130:60,Testability,log,logs,60,This is part 1 of a 2-stage commit to embed all the project logs directly in the index page.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433727130
https://github.com/hail-is/hail/pull/4667#issuecomment-433943049:169,Usability,simpl,simple,169,@tpoterba why would these not go on the artifacts index page? It seems odd to couple the CI directly to the artifact structure of `hail-is/hail` when there's an equally simple alternative.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433943049
https://github.com/hail-is/hail/pull/4667#issuecomment-433943710:42,Testability,test,testing,42,the artifacts page has the same problem - testing multiple projects means you have to encode the full set of information somewhere...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433943710
https://github.com/hail-is/hail/pull/4667#issuecomment-433954860:97,Modifiability,coupling,coupling,97,"Once we fold in cloud tools, the CI will only be testing hail-is/hail, so all these questions of coupling between CI and hail-is/hail are moot. (Currently, these changes will introduce a few broken object links to the cloud tools repo. Which is mostly untidy rather than bad in anyway.). I created `artifacts/index.html` to work around a GCS limitation. Serving a bucket directly doesn't generate an `ls -l` style index.html for directories. nginx and apache are happy to do this. There are two layers:. - ci's ""job"" information (a log and a directory of artifacts); - project-specific artifacts. I think the friction is caused by CI & batch lacking a way to express a DAG of jobs. We've hacked this on via background processes, but now the ci's ""job"" page doesn't neatly correspond to one process log. Cotton's proposed batch DAG (& CI's use of it) resolves this by restoring the separation of ""job"" (or, now, ""jobs"") information and artifacts (which are truly just artifacts). All that said, if this makes your life easier, I don't mind the mild ugliness on cloud tools while we wait for batch DAG to land. Let me know and I'll approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433954860
https://github.com/hail-is/hail/pull/4667#issuecomment-433954860:495,Modifiability,layers,layers,495,"Once we fold in cloud tools, the CI will only be testing hail-is/hail, so all these questions of coupling between CI and hail-is/hail are moot. (Currently, these changes will introduce a few broken object links to the cloud tools repo. Which is mostly untidy rather than bad in anyway.). I created `artifacts/index.html` to work around a GCS limitation. Serving a bucket directly doesn't generate an `ls -l` style index.html for directories. nginx and apache are happy to do this. There are two layers:. - ci's ""job"" information (a log and a directory of artifacts); - project-specific artifacts. I think the friction is caused by CI & batch lacking a way to express a DAG of jobs. We've hacked this on via background processes, but now the ci's ""job"" page doesn't neatly correspond to one process log. Cotton's proposed batch DAG (& CI's use of it) resolves this by restoring the separation of ""job"" (or, now, ""jobs"") information and artifacts (which are truly just artifacts). All that said, if this makes your life easier, I don't mind the mild ugliness on cloud tools while we wait for batch DAG to land. Let me know and I'll approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433954860
https://github.com/hail-is/hail/pull/4667#issuecomment-433954860:49,Testability,test,testing,49,"Once we fold in cloud tools, the CI will only be testing hail-is/hail, so all these questions of coupling between CI and hail-is/hail are moot. (Currently, these changes will introduce a few broken object links to the cloud tools repo. Which is mostly untidy rather than bad in anyway.). I created `artifacts/index.html` to work around a GCS limitation. Serving a bucket directly doesn't generate an `ls -l` style index.html for directories. nginx and apache are happy to do this. There are two layers:. - ci's ""job"" information (a log and a directory of artifacts); - project-specific artifacts. I think the friction is caused by CI & batch lacking a way to express a DAG of jobs. We've hacked this on via background processes, but now the ci's ""job"" page doesn't neatly correspond to one process log. Cotton's proposed batch DAG (& CI's use of it) resolves this by restoring the separation of ""job"" (or, now, ""jobs"") information and artifacts (which are truly just artifacts). All that said, if this makes your life easier, I don't mind the mild ugliness on cloud tools while we wait for batch DAG to land. Let me know and I'll approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433954860
https://github.com/hail-is/hail/pull/4667#issuecomment-433954860:532,Testability,log,log,532,"Once we fold in cloud tools, the CI will only be testing hail-is/hail, so all these questions of coupling between CI and hail-is/hail are moot. (Currently, these changes will introduce a few broken object links to the cloud tools repo. Which is mostly untidy rather than bad in anyway.). I created `artifacts/index.html` to work around a GCS limitation. Serving a bucket directly doesn't generate an `ls -l` style index.html for directories. nginx and apache are happy to do this. There are two layers:. - ci's ""job"" information (a log and a directory of artifacts); - project-specific artifacts. I think the friction is caused by CI & batch lacking a way to express a DAG of jobs. We've hacked this on via background processes, but now the ci's ""job"" page doesn't neatly correspond to one process log. Cotton's proposed batch DAG (& CI's use of it) resolves this by restoring the separation of ""job"" (or, now, ""jobs"") information and artifacts (which are truly just artifacts). All that said, if this makes your life easier, I don't mind the mild ugliness on cloud tools while we wait for batch DAG to land. Let me know and I'll approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433954860
https://github.com/hail-is/hail/pull/4667#issuecomment-433954860:798,Testability,log,log,798,"Once we fold in cloud tools, the CI will only be testing hail-is/hail, so all these questions of coupling between CI and hail-is/hail are moot. (Currently, these changes will introduce a few broken object links to the cloud tools repo. Which is mostly untidy rather than bad in anyway.). I created `artifacts/index.html` to work around a GCS limitation. Serving a bucket directly doesn't generate an `ls -l` style index.html for directories. nginx and apache are happy to do this. There are two layers:. - ci's ""job"" information (a log and a directory of artifacts); - project-specific artifacts. I think the friction is caused by CI & batch lacking a way to express a DAG of jobs. We've hacked this on via background processes, but now the ci's ""job"" page doesn't neatly correspond to one process log. Cotton's proposed batch DAG (& CI's use of it) resolves this by restoring the separation of ""job"" (or, now, ""jobs"") information and artifacts (which are truly just artifacts). All that said, if this makes your life easier, I don't mind the mild ugliness on cloud tools while we wait for batch DAG to land. Let me know and I'll approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433954860
https://github.com/hail-is/hail/issues/4668#issuecomment-434267326:43,Availability,error,error,43,I am convinced this is actually just a bad error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326
https://github.com/hail-is/hail/issues/4668#issuecomment-434267326:49,Integrability,message,message,49,I am convinced this is actually just a bad error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326
https://github.com/hail-is/hail/issues/4685#issuecomment-451511775:26,Testability,log,log,26,"The unusual output in the log seems to be due to a k8s bug caused by getting the logs of a deleting pod. https://github.com/kubernetes/kubernetes/issues/59296. This does not explain why the job was reported to CI as a success. CI will only accept exit_code == 0 as a success, so somehow a pod was set to exit_code 0 but was also being deleted and so didn't have a log.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685#issuecomment-451511775
https://github.com/hail-is/hail/issues/4685#issuecomment-451511775:81,Testability,log,logs,81,"The unusual output in the log seems to be due to a k8s bug caused by getting the logs of a deleting pod. https://github.com/kubernetes/kubernetes/issues/59296. This does not explain why the job was reported to CI as a success. CI will only accept exit_code == 0 as a success, so somehow a pod was set to exit_code 0 but was also being deleted and so didn't have a log.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685#issuecomment-451511775
https://github.com/hail-is/hail/issues/4685#issuecomment-451511775:364,Testability,log,log,364,"The unusual output in the log seems to be due to a k8s bug caused by getting the logs of a deleting pod. https://github.com/kubernetes/kubernetes/issues/59296. This does not explain why the job was reported to CI as a success. CI will only accept exit_code == 0 as a success, so somehow a pod was set to exit_code 0 but was also being deleted and so didn't have a log.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685#issuecomment-451511775
https://github.com/hail-is/hail/pull/4694#issuecomment-434726391:2,Testability,test,tested,2,I tested this locally by building the image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4694#issuecomment-434726391
https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:45,Deployability,update,update,45,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362
https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:172,Deployability,update,update,172,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362
https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:52,Integrability,depend,dependencies,52,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362
https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:149,Integrability,depend,dependencies,149,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362
https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:126,Security,secur,security,126,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362
https://github.com/hail-is/hail/pull/4701#issuecomment-435205370:73,Safety,risk,risk,73,"> security. for kubernetes/flask/those things? I don't see much security risk for the other stuff right now (not including malicious packages as security risks, those seem to be in their own category). Agree to punt on this for now though ðŸ˜‰",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370
https://github.com/hail-is/hail/pull/4701#issuecomment-435205370:154,Safety,risk,risks,154,"> security. for kubernetes/flask/those things? I don't see much security risk for the other stuff right now (not including malicious packages as security risks, those seem to be in their own category). Agree to punt on this for now though ðŸ˜‰",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370
https://github.com/hail-is/hail/pull/4701#issuecomment-435205370:2,Security,secur,security,2,"> security. for kubernetes/flask/those things? I don't see much security risk for the other stuff right now (not including malicious packages as security risks, those seem to be in their own category). Agree to punt on this for now though ðŸ˜‰",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370
https://github.com/hail-is/hail/pull/4701#issuecomment-435205370:64,Security,secur,security,64,"> security. for kubernetes/flask/those things? I don't see much security risk for the other stuff right now (not including malicious packages as security risks, those seem to be in their own category). Agree to punt on this for now though ðŸ˜‰",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370
https://github.com/hail-is/hail/pull/4701#issuecomment-435205370:145,Security,secur,security,145,"> security. for kubernetes/flask/those things? I don't see much security risk for the other stuff right now (not including malicious packages as security risks, those seem to be in their own category). Agree to punt on this for now though ðŸ˜‰",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370
https://github.com/hail-is/hail/pull/4709#issuecomment-435169908:102,Deployability,install,install,102,"For testing, we start a server with `python ci/ci.py`. That doesn't appear to work. Do we need to pip install it instead?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4709#issuecomment-435169908
https://github.com/hail-is/hail/pull/4709#issuecomment-435169908:4,Testability,test,testing,4,"For testing, we start a server with `python ci/ci.py`. That doesn't appear to work. Do we need to pip install it instead?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4709#issuecomment-435169908
https://github.com/hail-is/hail/issues/4716#issuecomment-435389708:97,Deployability,deploy,deploying,97,Can we follow this up with getting the file into git and checking that import-count works before deploying it?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4716#issuecomment-435389708
https://github.com/hail-is/hail/issues/4718#issuecomment-440537473:21,Testability,test,test,21,@chrisvittal can you test again on the changes in #4809? (either on that branch or on master once it goes in?),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718#issuecomment-440537473
https://github.com/hail-is/hail/pull/4720#issuecomment-437412681:18,Availability,ping,ping,18,@tpoterba @jigold ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4720#issuecomment-437412681
https://github.com/hail-is/hail/pull/4720#issuecomment-437417490:212,Availability,ping,ping,212,"Iâ€™m out today. Iâ€™m good with the PR if Tim is. On Friday, November 9, 2018, Christopher Vittal <notifications@github.com>; wrote:. > @tpoterba <https://github.com/tpoterba> @jigold; > <https://github.com/jigold> ping; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/4720#issuecomment-437412681>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ABnWpLgmNZwI4lVd3I0vKAkwVhicd-4_ks5utawIgaJpZM4YLr0h>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4720#issuecomment-437417490
https://github.com/hail-is/hail/pull/4723#issuecomment-435512500:36,Deployability,deploy,deploy,36,"I fixed up refreshing from batch on deploy jobs recently, but now we might find a job from a previous CI instance (which was killed by an update say, or a node loss) and that job might have a different job id than the one we expected (if we eagerly launched a deploy before we refreshed from batch). That's totally OK, as long as the target SHAs match, because if the target SHAs match, then the job *must* be deploying the same thing we intended to deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500
https://github.com/hail-is/hail/pull/4723#issuecomment-435512500:138,Deployability,update,update,138,"I fixed up refreshing from batch on deploy jobs recently, but now we might find a job from a previous CI instance (which was killed by an update say, or a node loss) and that job might have a different job id than the one we expected (if we eagerly launched a deploy before we refreshed from batch). That's totally OK, as long as the target SHAs match, because if the target SHAs match, then the job *must* be deploying the same thing we intended to deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500
https://github.com/hail-is/hail/pull/4723#issuecomment-435512500:260,Deployability,deploy,deploy,260,"I fixed up refreshing from batch on deploy jobs recently, but now we might find a job from a previous CI instance (which was killed by an update say, or a node loss) and that job might have a different job id than the one we expected (if we eagerly launched a deploy before we refreshed from batch). That's totally OK, as long as the target SHAs match, because if the target SHAs match, then the job *must* be deploying the same thing we intended to deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500
https://github.com/hail-is/hail/pull/4723#issuecomment-435512500:410,Deployability,deploy,deploying,410,"I fixed up refreshing from batch on deploy jobs recently, but now we might find a job from a previous CI instance (which was killed by an update say, or a node loss) and that job might have a different job id than the one we expected (if we eagerly launched a deploy before we refreshed from batch). That's totally OK, as long as the target SHAs match, because if the target SHAs match, then the job *must* be deploying the same thing we intended to deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500
https://github.com/hail-is/hail/pull/4723#issuecomment-435512500:450,Deployability,deploy,deploy,450,"I fixed up refreshing from batch on deploy jobs recently, but now we might find a job from a previous CI instance (which was killed by an update say, or a node loss) and that job might have a different job id than the one we expected (if we eagerly launched a deploy before we refreshed from batch). That's totally OK, as long as the target SHAs match, because if the target SHAs match, then the job *must* be deploying the same thing we intended to deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500
https://github.com/hail-is/hail/pull/4726#issuecomment-435889718:220,Modifiability,rewrite,rewrite,220,"I actually think moving `root` into a `location` at the bottom of the file does not matter, but it seems easier to spot this way. I think the actual issue is that this server has never heard of `https://hail.is`, so the rewrite doesn't work?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4726#issuecomment-435889718
https://github.com/hail-is/hail/pull/4729#issuecomment-437161860:216,Availability,down,downstream,216,"@patrick-schultz I rewrote the `ReadIterator` stuff as a wrapper for a `Reader` object that actually manages the state. I've left the Region management out of this for now; it's currently pulling the region from the downstream ContextRDD, but the next step is definitely to move the region management into c++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4729#issuecomment-437161860
https://github.com/hail-is/hail/pull/4729#issuecomment-437161860:57,Integrability,wrap,wrapper,57,"@patrick-schultz I rewrote the `ReadIterator` stuff as a wrapper for a `Reader` object that actually manages the state. I've left the Region management out of this for now; it's currently pulling the region from the downstream ContextRDD, but the next step is definitely to move the region management into c++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4729#issuecomment-437161860
https://github.com/hail-is/hail/issues/4732#issuecomment-597125254:26,Performance,Optimiz,Optimization,26,expectation maximization. Optimization loops.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4732#issuecomment-597125254
https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:342,Availability,error,error,342,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754
https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:825,Availability,downtime,downtime,825,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754
https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:181,Deployability,release,releases,181,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754
https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:412,Deployability,install,install,412,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754
https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:740,Deployability,upgrade,upgrade,740,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754
https://github.com/hail-is/hail/issues/4733#issuecomment-443244929:238,Performance,LOAD,LOAD,238,It looks like the prebuilt hail/hail/prebuilt/lib/linux-x86-64/libhail.so is what requires GLIBC 2.14. What would it take to get this compiled with 2.12?. ```; objdump -p libhail.so. libhail.so: file format elf64-x86-64. Program Header:; LOAD off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**21; filesz 0x0000000000023e1d memsz 0x0000000000023e1d flags r-x; LOAD off 0x00000000000246f8 vaddr 0x00000000002246f8 paddr 0x00000000002246f8 align 2**21; filesz 0x0000000000000de0 memsz 0x0000000000001038 flags rw-; DYNAMIC off 0x0000000000024ca8 vaddr 0x0000000000224ca8 paddr 0x0000000000224ca8 align 2**3; filesz 0x00000000000001f0 memsz 0x00000000000001f0 flags rw-; NOTE off 0x00000000000001c8 vaddr 0x00000000000001c8 paddr 0x00000000000001c8 align 2**2; filesz 0x0000000000000024 memsz 0x0000000000000024 flags r--; EH_FRAME off 0x000000000001ffa4 vaddr 0x000000000001ffa4 paddr 0x000000000001ffa4 align 2**2; filesz 0x00000000000007dc memsz 0x00000000000007dc flags r--; STACK off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**4; filesz 0x0000000000000000 memsz 0x0000000000000000 flags rw-; RELRO off 0x00000000000246f8 vaddr 0x00000000002246f8 paddr 0x00000000002246f8 align 2**0; filesz 0x0000000000000908 memsz 0x0000000000000908 flags r--. Dynamic Section:; NEEDED libstdc++.so.6; NEEDED libm.so.6; NEEDED libgcc_s.so.1; NEEDED libc.so.6; INIT 0x000000000000b1c8; FINI 0x000000000001ea38; INIT_ARRAY 0x00000000002246f8; INIT_ARRAYSZ 0x0000000000000010; FINI_ARRAY 0x0000000000224708; FINI_ARRAYSZ 0x0000000000000008; GNU_HASH 0x00000000000001f0; STRTAB 0x0000000000003690; SYMTAB 0x0000000000000c90; STRSZ 0x0000000000005524; SYMENT 0x0000000000000018; PLTGOT 0x0000000000225000; PLTRELSZ 0x0000000000000e10; PLTREL 0x0000000000000007; JMPREL 0x000000000000a3b8; RELA 0x0000000000009038; RELASZ 0x0000000000001380; RELAENT 0x0000000000000018; VERNEED 0x0000000000008f38; VERNEEDNUM 0x0000000000000004; VERSYM 0x0000000000008bb4;,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-443244929
https://github.com/hail-is/hail/issues/4733#issuecomment-443244929:391,Performance,LOAD,LOAD,391,It looks like the prebuilt hail/hail/prebuilt/lib/linux-x86-64/libhail.so is what requires GLIBC 2.14. What would it take to get this compiled with 2.12?. ```; objdump -p libhail.so. libhail.so: file format elf64-x86-64. Program Header:; LOAD off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**21; filesz 0x0000000000023e1d memsz 0x0000000000023e1d flags r-x; LOAD off 0x00000000000246f8 vaddr 0x00000000002246f8 paddr 0x00000000002246f8 align 2**21; filesz 0x0000000000000de0 memsz 0x0000000000001038 flags rw-; DYNAMIC off 0x0000000000024ca8 vaddr 0x0000000000224ca8 paddr 0x0000000000224ca8 align 2**3; filesz 0x00000000000001f0 memsz 0x00000000000001f0 flags rw-; NOTE off 0x00000000000001c8 vaddr 0x00000000000001c8 paddr 0x00000000000001c8 align 2**2; filesz 0x0000000000000024 memsz 0x0000000000000024 flags r--; EH_FRAME off 0x000000000001ffa4 vaddr 0x000000000001ffa4 paddr 0x000000000001ffa4 align 2**2; filesz 0x00000000000007dc memsz 0x00000000000007dc flags r--; STACK off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**4; filesz 0x0000000000000000 memsz 0x0000000000000000 flags rw-; RELRO off 0x00000000000246f8 vaddr 0x00000000002246f8 paddr 0x00000000002246f8 align 2**0; filesz 0x0000000000000908 memsz 0x0000000000000908 flags r--. Dynamic Section:; NEEDED libstdc++.so.6; NEEDED libm.so.6; NEEDED libgcc_s.so.1; NEEDED libc.so.6; INIT 0x000000000000b1c8; FINI 0x000000000001ea38; INIT_ARRAY 0x00000000002246f8; INIT_ARRAYSZ 0x0000000000000010; FINI_ARRAY 0x0000000000224708; FINI_ARRAYSZ 0x0000000000000008; GNU_HASH 0x00000000000001f0; STRTAB 0x0000000000003690; SYMTAB 0x0000000000000c90; STRSZ 0x0000000000005524; SYMENT 0x0000000000000018; PLTGOT 0x0000000000225000; PLTRELSZ 0x0000000000000e10; PLTREL 0x0000000000000007; JMPREL 0x000000000000a3b8; RELA 0x0000000000009038; RELASZ 0x0000000000001380; RELAENT 0x0000000000000018; VERNEED 0x0000000000008f38; VERNEEDNUM 0x0000000000000004; VERSYM 0x0000000000008bb4;,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-443244929
https://github.com/hail-is/hail/issues/4733#issuecomment-444549354:64,Deployability,release,release,64,"We can investigate building against an old version of GLIBC for release. In the mean time, I don't see any reason why you shouldn't be able to build locally against an older version of glibc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-444549354
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:185,Availability,error,error,185,"@tpoterba . Thanks for pointing out the extra step. ; So I have compiled hail to run on Centos 6 and it is running python scripts fine locally (master=local[*]). However, the following error occurs when running it with yarn. Any suggestions on this? . ```; [Stage 0:> (0 + 1) / 292]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen_count.py in <module>; 10 mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); 11 mt.describe(); ---> 12 print(""Count:"",mt.count()); 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from containe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1393,Availability,Error,Error,1393,"--; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen_count.py in <module>; 10 mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); 11 mt.describe(); ---> 12 print(""Count:"",mt.count()); 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1630,Availability,failure,failure,1630," 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1687,Availability,failure,failure,1687,"/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:3201,Availability,failure,failure,3201,"rg.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.lau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:3258,Availability,failure,failure,3258,"che.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:891,Deployability,install,install,891,"@tpoterba . Thanks for pointing out the extra step. ; So I have compiled hail to run on Centos 6 and it is running python scripts fine locally (master=local[*]). However, the following error occurs when running it with yarn. Any suggestions on this? . ```; [Stage 0:> (0 + 1) / 292]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen_count.py in <module>; 10 mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); 11 mt.describe(); ---> 12 print(""Count:"",mt.count()); 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from containe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:2689,Performance,concurren,concurrent,2689,": Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:2750,Performance,concurren,concurrent,2750,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:2834,Performance,concurren,concurrent,2834,"Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:4260,Performance,concurren,concurrent,4260,"unch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:4321,Performance,concurren,concurrent,4321,"unch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:4405,Performance,concurren,concurrent,4405,"unch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1609,Safety,abort,aborted,1609," 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:3180,Safety,abort,aborted,3180,"rg.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.lau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705
https://github.com/hail-is/hail/issues/4733#issuecomment-448845809:324,Availability,error,error,324,"Yes. it runs fine on our regular cluster locally. As a workaround, we; are qsubing our hail jobs for each chromosome with --master local[16]; reading the matrix tables stored on our gpfs system. Those are running; fine on the UK Biobank data. When we try to running the same job with; master=yarn on the hadoop cluster that error occurs. An earlier version of; Hail (before the GLIBC error) was running fine on the Hadoop cluster. John. On Wed, Dec 19, 2018 at 6:28 PM Tim Poterba <notifications@github.com>; wrote:. > sorry to lose this! I think we're not as good at keeping on top of issue; > comments as forum posts/zulip/etc.; >; > I have no idea what this is coming from, never seen something like this; > before.; >; > Can you run pyspark stuff normally?; >; > â€”; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/4733#issuecomment-448783924>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AB3rDZkqSvk3zcRuS41UEh_xHLQ5ccabks5u6sk1gaJpZM4YPdeC>; > .; >. -- ; John Farrell, Ph.D.; Biomedical Genetics-Evans 218; Boston University Medical School; 72 East Concord Street; Boston, MA. ph: 617-358-3562 (New Number)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-448845809
https://github.com/hail-is/hail/issues/4733#issuecomment-448845809:384,Availability,error,error,384,"Yes. it runs fine on our regular cluster locally. As a workaround, we; are qsubing our hail jobs for each chromosome with --master local[16]; reading the matrix tables stored on our gpfs system. Those are running; fine on the UK Biobank data. When we try to running the same job with; master=yarn on the hadoop cluster that error occurs. An earlier version of; Hail (before the GLIBC error) was running fine on the Hadoop cluster. John. On Wed, Dec 19, 2018 at 6:28 PM Tim Poterba <notifications@github.com>; wrote:. > sorry to lose this! I think we're not as good at keeping on top of issue; > comments as forum posts/zulip/etc.; >; > I have no idea what this is coming from, never seen something like this; > before.; >; > Can you run pyspark stuff normally?; >; > â€”; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/4733#issuecomment-448783924>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AB3rDZkqSvk3zcRuS41UEh_xHLQ5ccabks5u6sk1gaJpZM4YPdeC>; > .; >. -- ; John Farrell, Ph.D.; Biomedical Genetics-Evans 218; Boston University Medical School; 72 East Concord Street; Boston, MA. ph: 617-358-3562 (New Number)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-448845809
https://github.com/hail-is/hail/issues/4733#issuecomment-453607151:203,Deployability,upgrade,upgrade,203,"@tpoterba @cseed . Has the latest 0.2 version been tested on Centos 6 yet? While we have compiled and run it locally, we are unable to get the compiled version running on our Centos 6 hadoop cluster. An upgrade to Centos 7 is not planned to the spring so it would be great to verify that this can run on Centos 6.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-453607151
https://github.com/hail-is/hail/issues/4733#issuecomment-453607151:51,Testability,test,tested,51,"@tpoterba @cseed . Has the latest 0.2 version been tested on Centos 6 yet? While we have compiled and run it locally, we are unable to get the compiled version running on our Centos 6 hadoop cluster. An upgrade to Centos 7 is not planned to the spring so it would be great to verify that this can run on Centos 6.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-453607151
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:345,Availability,failure,failure,345,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:402,Availability,failure,failure,402,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:84,Performance,optimiz,optimized,84,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:134,Performance,optimiz,optimized-os,134,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:324,Safety,abort,aborted,324,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:3,Testability,test,test,3,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:163,Testability,test,test,163,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:256,Testability,test,test,256,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:6031,Availability,error,error,6031,"conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper. ```. This is an error we are getting when submitted to the hadoop cluster. This runs fine locally. ```; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.4-d602a3d7472d; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-0019-0.2.4-d602a3d7472d.log; 2019-01-22 00:19:54 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 00:19:54 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7677,Availability,failure,failure,7677,"; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7734,Availability,failure,failure,7734,"; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuff",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8006,Availability,failure,failure,8006,"eles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFaile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8063,Availability,failure,failure,8063,"> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(O",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:11412,Availability,Error,Error,11412,"6); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:77); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:552); at is.hail.variant.MatrixTable.count(MatrixTable.scala:550); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.4-d602a3d7472d; Error summary: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:11468,Availability,failure,failure,11468,"6); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:77); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:552); at is.hail.variant.MatrixTable.count(MatrixTable.scala:550); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.4-d602a3d7472d; Error summary: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:11525,Availability,failure,failure,11525,"6); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:77); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:552); at is.hail.variant.MatrixTable.count(MatrixTable.scala:550); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.4-d602a3d7472d; Error summary: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7394,Deployability,install,install,7394,"4 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8278,Energy Efficiency,schedul,scheduler,8278,"il/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8318,Energy Efficiency,schedul,scheduler,8318,", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8416,Energy Efficiency,schedul,scheduler,8416,"0.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8513,Energy Efficiency,schedul,scheduler,8513,"hub/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8764,Energy Efficiency,schedul,scheduler,8764," (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8844,Energy Efficiency,schedul,scheduler,8844,"ted caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8949,Energy Efficiency,schedul,scheduler,8949,"ache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9097,Energy Efficiency,schedul,scheduler,9097,"D 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9185,Energy Efficiency,schedul,scheduler,9185,y one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9282,Energy Efficiency,schedul,scheduler,9282,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpre,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9377,Energy Efficiency,schedul,scheduler,9377,.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9540,Energy Efficiency,schedul,scheduler,9540,abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7656,Safety,abort,aborted,7656,"; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7985,Safety,abort,aborted,7985,"eles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFaile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8448,Safety,abort,abortStage,8448,"gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8545,Safety,abort,abortStage,8545,"stributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8787,Safety,abort,abortStage,8787,"executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at or",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:11447,Safety,abort,aborted,11447,"6); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:77); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:552); at is.hail.variant.MatrixTable.count(MatrixTable.scala:550); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.4-d602a3d7472d; Error summary: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:6210,Testability,LOG,LOGGING,6210,") 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper. ```. This is an error we are getting when submitted to the hadoop cluster. This runs fine locally. ```; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.4-d602a3d7472d; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-0019-0.2.4-d602a3d7472d.log; 2019-01-22 00:19:54 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 00:19:54 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); Fi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:6319,Testability,log,log,6319,".; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper. ```. This is an error we are getting when submitted to the hadoop cluster. This runs fine locally. ```; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.4-d602a3d7472d; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-0019-0.2.4-d602a3d7472d.log; 2019-01-22 00:19:54 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 00:19:54 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572
https://github.com/hail-is/hail/issues/4733#issuecomment-456486653:81,Testability,log,logs,81,"@jjfarrell Thanks, this is good information. Can you also get the YARN container logs from a failing run? those should have more details on what exactly went wrong. Also the hail.log file might have more information on the root cause of the `ExecutorLostFailure` issue. Also, there's been about 7 weeks of development since `d602a3d`, perhaps there's some issue with that version. (I suspect it's not that, but worth trying).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456486653
https://github.com/hail-is/hail/issues/4733#issuecomment-456486653:179,Testability,log,log,179,"@jjfarrell Thanks, this is good information. Can you also get the YARN container logs from a failing run? those should have more details on what exactly went wrong. Also the hail.log file might have more information on the root cause of the `ExecutorLostFailure` issue. Also, there's been about 7 weeks of development since `d602a3d`, perhaps there's some issue with that version. (I suspect it's not that, but worth trying).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456486653
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:261,Availability,echo,echo,261,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:567,Availability,echo,echo,567,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:2791,Availability,ERROR,ERROR,2791," --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail691432050703",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:953,Deployability,deploy,deploy-mode,953,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:1684,Deployability,deploy,deploy-mode,1684,"res 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:145,Performance,load,load,145,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:197,Performance,load,load,197,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:220,Performance,load,load,220,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:243,Performance,load,load,243,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:2698,Performance,load,load,2698,"--conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /da",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:4147,Performance,load,load,4147,".. using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:27); at is.hail.annotations.Region$.apply(Region.scala:10); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:4255,Performance,load,loadLibrary,4255,".. using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:27); at is.hail.annotations.Region$.apply(Region.scala:10); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:4357,Performance,load,load,4357,".. using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:27); at is.hail.annotations.Region$.apply(Region.scala:10); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:21,Testability,log,logs,21,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:2092,Testability,log,log,2092,"=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/user",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4174,Availability,AVAIL,AVAILABLE,4174,"nagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4298,Availability,AVAIL,AVAILABLE,4298,"r getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4420,Availability,AVAIL,AVAILABLE,4420,"1-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,A",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4548,Availability,AVAIL,AVAILABLE,4548,"22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4669,Availability,AVAIL,AVAILABLE,4669,"putCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4795,Availability,AVAIL,AVAILABLE,4795,".3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4922,Availability,AVAIL,AVAILABLE,4922,"Connector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environme",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5054,Availability,AVAIL,AVAILABLE,5054,"port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5180,Availability,AVAIL,AVAILABLE,5180,"; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5311,Availability,AVAIL,AVAILABLE,5311,"19-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5432,Availability,AVAIL,AVAILABLE,5432,"1-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/th",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5558,Availability,AVAIL,AVAILABLE,5558,"19-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5684,Availability,AVAIL,AVAILABLE,5684,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5815,Availability,AVAIL,AVAILABLE,5815,"2 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@8587",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5941,Availability,AVAIL,AVAILABLE,5941,"13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6072,Availability,AVAIL,AVAILABLE,6072,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6196,Availability,AVAIL,AVAILABLE,6196,"2 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6325,Availability,AVAIL,AVAILABLE,6325,"-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6460,Availability,AVAIL,AVAILABLE,6460,"1:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6600,Availability,AVAIL,AVAILABLE,6600,"Handler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6721,Availability,AVAIL,AVAILABLE,6721,"ler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.comp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6834,Availability,AVAIL,AVAILABLE,6834,"9-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputforma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:10966,Availability,AVAIL,AVAILABLE,10966,"nagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11090,Availability,AVAIL,AVAILABLE,11090,"r getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11212,Availability,AVAIL,AVAILABLE,11212,"1-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,A",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11340,Availability,AVAIL,AVAILABLE,11340,"22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11461,Availability,AVAIL,AVAILABLE,11461,"putCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11587,Availability,AVAIL,AVAILABLE,11587,".3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11714,Availability,AVAIL,AVAILABLE,11714,"Connector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environme",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11846,Availability,AVAIL,AVAILABLE,11846,"port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:11972,Availability,AVAIL,AVAILABLE,11972,"; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12103,Availability,AVAIL,AVAILABLE,12103,"19-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12224,Availability,AVAIL,AVAILABLE,12224,"1-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/th",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12350,Availability,AVAIL,AVAILABLE,12350,"19-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12476,Availability,AVAIL,AVAILABLE,12476,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12607,Availability,AVAIL,AVAILABLE,12607,"2 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@8587",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12733,Availability,AVAIL,AVAILABLE,12733,"13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12864,Availability,AVAIL,AVAILABLE,12864,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,A",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12988,Availability,AVAIL,AVAILABLE,12988,"2 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13117,Availability,AVAIL,AVAILABLE,13117,"-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13252,Availability,AVAIL,AVAILABLE,13252,"1:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13392,Availability,AVAIL,AVAILABLE,13392,"Handler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Ve",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13513,Availability,AVAIL,AVAILABLE,13513,"ler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13626,Availability,AVAIL,AVAILABLE,13626,"22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13744,Availability,AVAIL,AVAILABLE,13744,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13872,Availability,AVAIL,AVAILABLE,13872," 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:14004,Availability,AVAIL,AVAILABLE,14004,"1-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Cr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:20172,Availability,AVAIL,AVAILABLE,20172,"started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44817.; 2019-01-22 13:11:37 NettyBlockTransferService: INFO: Server created on 10.48.225.55:44817; 2019-01-22 13:11:37 BlockManager: INFO: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registering BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMasterEndpoint: INFO: Registering block manager 10.48.225.55:44817 with 2.5 GB RAM, BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registered BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManager: INFO: Initialized BlockManager: BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@77390398{/metrics/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:38 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.203:44870) with ID 8; 2019-01-22 13:11:38 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q19.scc.bu.edu:44319 with 21.2 GB RAM, BlockManagerId(8, scc-q19.scc.bu.edu, 44319, None); 2019-01-22 13:11:40 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.229:36354) with ID 7; 2019-01-22 13:11:40 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.202:50198) with ID 6; 2019-01-22 13:11:40 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q21.scc.bu.edu:33025 with 21.2 GB RAM, BlockManagerId(7, scc-q21.scc.bu.edu, 33025, None); 2019-01-22 13:11:40 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q18.scc.bu.edu:39123 with 21.2 GB RAM, BlockManagerId(6, scc-q18.scc.bu.edu, 39123, None); 2019-01-22 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52482,Availability,ERROR,ERROR,52482,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60953,Availability,ERROR,ERROR,60953,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:68097,Availability,ERROR,ERROR,68097,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:75536,Availability,ERROR,ERROR,75536,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:83270,Availability,ERROR,ERROR,83270,); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:91004,Availability,ERROR,ERROR,91004,); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:98443,Availability,ERROR,ERROR,98443,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:108548,Availability,ERROR,ERROR,108548,xecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115987,Availability,ERROR,ERROR,115987,erExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:126080,Availability,ERROR,ERROR,126080,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:136897,Availability,ERROR,ERROR,136897,"2019-01-22 13:11:58 DAGScheduler: INFO: Executor lost: 18 (epoch 10); 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(18, scc-q02.scc.bu.edu, 37258, None); 2019-01-22 13:11:58 BlockManagerMaster: INFO: Removed 18 successfully in removeExecutor; 2019-01-22 13:11:58 DAGScheduler: INFO: Shuffle files lost for executor: 18 (epoch 10); 2019-01-22 13:11:58 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 11.; 2019-01-22 13:11:58 DAGScheduler: INFO: Executor lost: 11 (epoch 11); 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(11, scc-q17.scc.bu.edu, 37841, None); 2019-01-22 13:11:58 BlockManagerMaster: INFO: Removed 11 successfully in removeExecutor; 2019-01-22 13:11:58 DAGScheduler: INFO: Shuffle files lost for executor: 11 (epoch 11); 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 11 on scc-q17.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:145676,Availability,ERROR,ERROR,145676,xecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:158806,Availability,ERROR,ERROR,158806,"-01-22 13:12:01 TaskSetManager: INFO: Starting task 38.1 in stage 0.0 (TID 58, scc-q10.scc.bu.edu, executor 19, partition 38, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:01 TaskSetManager: INFO: Starting task 8.1 in stage 0.0 (TID 59, scc-q10.scc.bu.edu, executor 19, partition 8, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q10.scc.bu.edu:42152 with 21.2 GB RAM, BlockManagerId(19, scc-q10.scc.bu.edu, 42152, None); 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 13.; 2019-01-22 13:12:02 DAGScheduler: INFO: Executor lost: 13 (epoch 13); 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(13, scc-q16.scc.bu.edu, 35524, None); 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removed 13 successfully in removeExecutor; 2019-01-22 13:12:02 DAGScheduler: INFO: Shuffle files lost for executor: 13 (epoch 13); 2019-01-22 13:12:02 YarnScheduler: ERROR: Lost executor 15 on scc-q10.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:169355,Availability,ERROR,ERROR,169355,"S_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.196:53938) with ID 21; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21, partition 39, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21, partition 9, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21, partition 19, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q03.scc.bu.edu:36955 with 21.2 GB RAM, BlockManagerId(12, scc-q03.scc.bu.edu, 36955, None); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21, partition 29, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:180854,Availability,ERROR,ERROR,180854,"2019-01-22 13:12:04 DAGScheduler: INFO: Executor lost: 21 (epoch 14); 2019-01-22 13:12:04 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:04 BlockManagerMaster: INFO: Removed 21 successfully in removeExecutor; 2019-01-22 13:12:04 DAGScheduler: INFO: Shuffle files lost for executor: 21 (epoch 14); 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 12.; 2019-01-22 13:12:05 DAGScheduler: INFO: Executor lost: 12 (epoch 15); 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(12, scc-q03.scc.bu.edu, 36955, None); 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removed 12 successfully in removeExecutor; 2019-01-22 13:12:05 DAGScheduler: INFO: Shuffle files lost for executor: 12 (epoch 15); 2019-01-22 13:12:05 YarnScheduler: ERROR: Lost executor 21 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:190210,Availability,ERROR,ERROR,190210,"g.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(14, scc-q05.scc.bu.edu, 42935, None); 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removed 14 successfully in removeExecutor; 2019-01-22 13:12:06 DAGScheduler: INFO: Shuffle files lost for executor: 14 (epoch 16); 2019-01-22 13:12:06 YarnScheduler: ERROR: Lost executor 12 on scc-q03.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:194331,Availability,ERROR,ERROR,194331,"rg.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.serv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199159,Availability,ERROR,ERROR,199159,t org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199212,Availability,down,down,199212,t org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202830,Availability,ERROR,ERROR,202830,scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206386,Availability,down,down,206386,"sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206418,Availability,ERROR,ERROR,206418,"sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206487,Availability,down,down,206487,"sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206577,Availability,down,down,206577,"ache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:6",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209487,Availability,down,down,209487,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209603,Availability,down,down,209603,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:210277,Availability,ERROR,ERROR,210277,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213882,Availability,failure,failure,213882,"1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214246,Availability,ERROR,ERROR,214246,"AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214253,Availability,Error,Error,214253,"AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215966,Availability,recover,recover,215966,hedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216037,Availability,recover,recover,216037,hedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionCon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217388,Availability,recover,recover,217388,able.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217459,Availability,recover,recover,217459,se$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:513,Deployability,configurat,configuration,513,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1339,Deployability,deploy,deployMode,1339,". using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1455,Deployability,install,install,1455,"019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1488,Deployability,install,install,1488,"NFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 Secu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1523,Deployability,install,install,1523,"ark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1588,Deployability,install,install,1588,"genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Cha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1621,Deployability,install,install,1621,"s/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1654,Deployability,install,install,1654,"or.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1739,Deployability,install,install,1739,"nces=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authenticat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2018,Deployability,install,install,2018,inputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2080,Deployability,install,install,2080,rk.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2200,Deployability,install,install,2200,github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2240,Deployability,install,install,2240,spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7311,Deployability,configurat,configuration,7311,"tHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8137,Deployability,deploy,deployMode,8137,". using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/lib",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8253,Deployability,install,install,8253,"019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: far",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8286,Deployability,install,install,8286,"NFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 Security",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8321,Deployability,install,install,8321,"ark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modif",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8386,Deployability,install,install,8386,"genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8419,Deployability,install,install,8419,"s/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8452,Deployability,install,install,8452,"or.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls grou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8537,Deployability,install,install,8537,"ances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8814,Deployability,install,install,8814,leinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started servi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8876,Deployability,install,install,8876,park.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracke,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8996,Deployability,install,install,8996,ro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9034,Deployability,install,install,9034,all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: U,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:15787,Deployability,install,install,15787," 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-spark.jar; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/pyspark.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/pyspark.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:15984,Deployability,install,install,15984,"SClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-spark.jar; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/pyspark.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/pyspark.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:14559,Energy Efficiency,allocate,allocate,14559," o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:17144,Energy Efficiency,Schedul,SchedulerExtensionServices,17144,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:22800,Energy Efficiency,Schedul,SchedulerBackend,22800,".scc.bu.edu:38836 with 21.2 GB RAM, BlockManagerId(4, scc-q07.scc.bu.edu, 38836, None); 2019-01-22 13:11:42 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56622) with ID 1; 2019-01-22 13:11:42 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q02.scc.bu.edu:37399 with 21.2 GB RAM, BlockManagerId(1, scc-q02.scc.bu.edu, 37399, None); 2019-01-22 13:11:43 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.192:52326) with ID 5; 2019-01-22 13:11:43 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q08.scc.bu.edu:44677 with 21.2 GB RAM, BlockManagerId(5, scc-q08.scc.bu.edu, 44677, None); 2019-01-22 13:11:43 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.196:53932) with ID 2; 2019-01-22 13:11:43 YarnClientSchedulerBackend: INFO: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 2019-01-22 13:11:43 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:34720 with 21.2 GB RAM, BlockManagerId(2, scc-q12.scc.bu.edu, 34720, None); 2019-01-22 13:11:43 Hail: INFO: SparkUI: http://10.48.225.55:4040; 2019-01-22 13:11:43 Hail: INFO: Running Hail version 0.2.4-d602a3d7472d; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.<init> instruction count: 3; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 28; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method1 instruction count: 112; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method2 instruction count: 82;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:22830,Energy Efficiency,schedul,scheduling,22830,".scc.bu.edu:38836 with 21.2 GB RAM, BlockManagerId(4, scc-q07.scc.bu.edu, 38836, None); 2019-01-22 13:11:42 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56622) with ID 1; 2019-01-22 13:11:42 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q02.scc.bu.edu:37399 with 21.2 GB RAM, BlockManagerId(1, scc-q02.scc.bu.edu, 37399, None); 2019-01-22 13:11:43 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.192:52326) with ID 5; 2019-01-22 13:11:43 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q08.scc.bu.edu:44677 with 21.2 GB RAM, BlockManagerId(5, scc-q08.scc.bu.edu, 44677, None); 2019-01-22 13:11:43 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.196:53932) with ID 2; 2019-01-22 13:11:43 YarnClientSchedulerBackend: INFO: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 2019-01-22 13:11:43 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:34720 with 21.2 GB RAM, BlockManagerId(2, scc-q12.scc.bu.edu, 34720, None); 2019-01-22 13:11:43 Hail: INFO: SparkUI: http://10.48.225.55:4040; 2019-01-22 13:11:43 Hail: INFO: Running Hail version 0.2.4-d602a3d7472d; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.<init> instruction count: 3; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 28; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method1 instruction count: 112; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method2 instruction count: 82;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199482,Energy Efficiency,schedul,scheduler,199482,ent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199631,Energy Efficiency,schedul,scheduler,199631,in : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cance,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199772,Energy Efficiency,schedul,scheduler,199772,nagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199976,Energy Efficiency,schedul,scheduler,199976,arnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200100,Energy Efficiency,schedul,scheduler,200100, 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200594,Energy Efficiency,schedul,scheduler,200594,scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200701,Energy Efficiency,schedul,scheduler,200701,n$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200851,Energy Efficiency,schedul,scheduler,200851,.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200941,Energy Efficiency,schedul,scheduler,200941,cala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200990,Energy Efficiency,schedul,scheduler,200990,erImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(D,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201104,Energy Efficiency,schedul,scheduler,201104,.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201153,Energy Efficiency,schedul,scheduler,201153,apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAG,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201259,Energy Efficiency,schedul,scheduler,201259,(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201308,Energy Efficiency,schedul,scheduler,201308,hMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGSchedul,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201477,Energy Efficiency,schedul,scheduler,201477,eachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201517,Energy Efficiency,schedul,scheduler,201517,shMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201615,Energy Efficiency,schedul,scheduler,201615,l$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201712,Energy Efficiency,schedul,scheduler,201712,hedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201963,Energy Efficiency,schedul,scheduler,201963,nfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.def,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202043,Energy Efficiency,schedul,scheduler,202043,y$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202148,Energy Efficiency,schedul,scheduler,202148,scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202296,Energy Efficiency,schedul,scheduler,202296,$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskScheduler,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202384,Energy Efficiency,schedul,scheduler,202384,29); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202481,Energy Efficiency,schedul,scheduler,202481,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202576,Energy Efficiency,schedul,scheduler,202576,.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.forea,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203129,Energy Efficiency,schedul,scheduler,203129,.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203278,Energy Efficiency,schedul,scheduler,203278,apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cance,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203419,Energy Efficiency,schedul,scheduler,203419,oOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203623,Energy Efficiency,schedul,scheduler,203623,Scheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203747,Energy Efficiency,schedul,scheduler,203747,: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204241,Energy Efficiency,schedul,scheduler,204241,scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204348,Energy Efficiency,schedul,scheduler,204348,n$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204498,Energy Efficiency,schedul,scheduler,204498,.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204588,Energy Efficiency,schedul,scheduler,204588,cala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204637,Energy Efficiency,schedul,scheduler,204637,erImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204751,Energy Efficiency,schedul,scheduler,204751,.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCanc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204800,Energy Efficiency,schedul,scheduler,204800,apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204906,Energy Efficiency,schedul,scheduler,204906,(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.Ev,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204955,Energy Efficiency,schedul,scheduler,204955,hMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205124,Energy Efficiency,schedul,scheduler,205124,"eachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205164,Energy Efficiency,schedul,scheduler,205164,"shMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205262,Energy Efficiency,schedul,scheduler,205262,"l$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGSchedule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205353,Energy Efficiency,schedul,scheduler,205353,"TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled be",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205462,Energy Efficiency,schedul,scheduler,205462,"cala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because Sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205563,Energy Efficiency,schedul,scheduler,205563,"at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205727,Energy Efficiency,schedul,scheduler,205727,"t org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGSched",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205811,Energy Efficiency,schedul,scheduler,205811,"eduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206603,Energy Efficiency,schedul,scheduler,206603,"bs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkConte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206714,Energy Efficiency,schedul,scheduler,206714,"e.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206888,Energy Efficiency,schedul,scheduler,206888,"ache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206982,Energy Efficiency,schedul,scheduler,206982,"or: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:207135,Energy Efficiency,schedul,scheduler,207135," 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.sca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:207503,Energy Efficiency,schedul,scheduler,207503,.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209408,Energy Efficiency,monitor,monitor,209408,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209629,Energy Efficiency,Schedul,SchedulerExtensionServices,209629,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209672,Energy Efficiency,Schedul,SchedulerExtensionServices,209672,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213966,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,213966,"lInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213994,Energy Efficiency,Schedul,ScheduledFutureTask,213994,"lInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214058,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,214058,"AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214785,Energy Efficiency,schedul,scheduler,214785,"xt: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214872,Energy Efficiency,schedul,scheduler,214872,"ored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215010,Energy Efficiency,schedul,scheduler,215010,"15b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215097,Energy Efficiency,schedul,scheduler,215097,"ted, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.Callb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:513,Modifiability,config,configuration,513,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7311,Modifiability,config,configuration,7311,"tHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:305,Performance,load,load,305,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7103,Performance,load,load,7103,"1-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:14253,Performance,load,loaded,14253,"s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to up",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:17551,Performance,queue,queue,17551,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:18891,Performance,queue,queue,18891," ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 YarnSchedulerBackend$YarnSchedulerEndpoint: INFO: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-22 13:11:36 YarnClientSchedulerBackend: INFO: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174), /proxy/application_1542127286896_0174; 2019-01-22 13:11:36 JettyUtils: INFO: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-22 13:11:37 Client: INFO: Application report for application_1542127286896_0174 (state: RUNNING); 2019-01-22 13:11:37 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.203; ApplicationMaster RPC port: 0; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:37 YarnClientSchedulerBackend: INFO: Application application_1542127286896_0174 has started running.; 2019-01-22 13:11:37 Utils: INFO: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44817.; 2019-01-22 13:11:37 NettyBlockTransferService: INFO: Server created on 10.48.225.55:44817; 2019-01-22 13:11:37 BlockManager: INFO: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registering BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMasterEndpoint: INFO: Registering block manager 10.48.225.55:44817 with 2.5 GB RAM, BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registered BlockManager BlockManagerId(driver, 10.4",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:24569,Performance,cache,cache,24569,"generated/C0.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method1 instruction count: 112; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method2 instruction count: 82; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method3 instruction count: 252; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.<init> instruction count: 3; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 28; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method1 instruction count: 100; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method2 instruction count: 106; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method3 instruction count: 252; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:46 root: INFO: Index reader cache queries: 4; 2019-01-22 13:11:46 root: INFO: Index reader cache hit rate: 0.25; 2019-01-22 13:11:46 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.185:44946) with ID 9; 2019-01-22 13:11:46 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q01.scc.bu.edu:38527 with 21.2 GB RAM, BlockManagerId(9, scc-q01.scc.bu.edu, 38527, None); 2019-01-22 13:11:46 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.193:42726) with ID 3; 2019-01-22 13:11:46 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q09.scc.bu.edu:41872 with 21.2 GB RAM, BlockManagerId(3, scc-q09.scc.bu.edu, 41872, None); 2019-01-22 13:11:47 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 13:11:47 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 13:11:47 Hail: INFO: Number of variants across all BGEN files: 1261158; 2019-01-22 13:11:48 MemorySto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:24632,Performance,cache,cache,24632,"generated/C0.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method1 instruction count: 112; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method2 instruction count: 82; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method3 instruction count: 252; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.<init> instruction count: 3; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 28; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method1 instruction count: 100; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method2 instruction count: 106; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C1.method3 instruction count: 252; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:46 root: INFO: Index reader cache queries: 4; 2019-01-22 13:11:46 root: INFO: Index reader cache hit rate: 0.25; 2019-01-22 13:11:46 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.185:44946) with ID 9; 2019-01-22 13:11:46 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q01.scc.bu.edu:38527 with 21.2 GB RAM, BlockManagerId(9, scc-q01.scc.bu.edu, 38527, None); 2019-01-22 13:11:46 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.193:42726) with ID 3; 2019-01-22 13:11:46 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q09.scc.bu.edu:41872 with 21.2 GB RAM, BlockManagerId(3, scc-q09.scc.bu.edu, 41872, None); 2019-01-22 13:11:47 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 13:11:47 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 13:11:47 Hail: INFO: Number of variants across all BGEN files: 1261158; 2019-01-22 13:11:48 MemorySto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:27331,Performance,cache,cache,27331,"nerated/C2.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method1 instruction count: 112; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method2 instruction count: 82; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method3 instruction count: 252; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.<init> instruction count: 3; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 28; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method1 instruction count: 100; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method2 instruction count: 106; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:27397,Performance,cache,cache,27397,"nerated/C2.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method1 instruction count: 112; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method2 instruction count: 82; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method3 instruction count: 252; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.<init> instruction count: 3; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 28; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method1 instruction count: 100; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method2 instruction count: 106; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:27464,Performance,optimiz,optimize,27464,"enerated/C2.method3 instruction count: 252; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.<init> instruction count: 3; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 28; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method1 instruction count: 100; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method2 instruction count: 106; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the ent",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:28032,Performance,optimiz,optimize,28032,":11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}"")); (SelectFields (); (Ref global))); (SelectFiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:28530,Performance,optimiz,optimize,28530,"tring},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}"")); (SelectFields (); (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:29201,Performance,optimiz,optimize,29201,"\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}"")); (SelectFields (); (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C4.<init> instruction count: 3; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C4.apply instruction count: 401; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C4.apply instruction count: 16; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C5.<init> instruction count: 3; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C5.apply instruction count: 28; 2019-01-22 13:11:51 root: INFO: is/hail/codegen/generated/C5.apply instruction count: 12; 2019-01-22 13:11:51 root: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:46724,Performance,concurren,concurrent,46724,9-01-22 13:11:53 DAGScheduler: INFO: Shuffle files lost for executor: 3 (epoch 9); 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:46785,Performance,concurren,concurrent,46785,xecutor: 3 (epoch 9); 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:46869,Performance,concurren,concurrent,46869,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:48051,Performance,concurren,concurrent,48051,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:48112,Performance,concurren,concurrent,48112,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:48196,Performance,concurren,concurrent,48196,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:49378,Performance,concurren,concurrent,49378,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:49439,Performance,concurren,concurrent,49439,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:49523,Performance,concurren,concurrent,49523,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:50705,Performance,concurren,concurrent,50705,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:50766,Performance,concurren,concurrent,50766,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:50850,Performance,concurren,concurrent,50850,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52032,Performance,concurren,concurrent,52032,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nod,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52093,Performance,concurren,concurrent,52093,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52177,Performance,concurren,concurrent,52177,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:53370,Performance,concurren,concurrent,53370,ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:53431,Performance,concurren,concurrent,53431,it code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:53515,Performance,concurren,concurrent,53515,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:54697,Performance,concurren,concurrent,54697,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:54758,Performance,concurren,concurrent,54758,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:54842,Performance,concurren,concurrent,54842,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:56149,Performance,concurren,concurrent,56149,"Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:56210,Performance,concurren,concurrent,56210,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:56294,Performance,concurren,concurrent,56294,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:57601,Performance,concurren,concurrent,57601,"Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:57662,Performance,concurren,concurrent,57662,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:57746,Performance,concurren,concurrent,57746,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:59051,Performance,concurren,concurrent,59051,": Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:59112,Performance,concurren,concurrent,59112,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:59196,Performance,concurren,concurrent,59196,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60503,Performance,concurren,concurrent,60503,"Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60564,Performance,concurren,concurrent,60564,utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60648,Performance,concurren,concurrent,60648,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:61841,Performance,concurren,concurrent,61841,"ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:61902,Performance,concurren,concurrent,61902,"it code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:61986,Performance,concurren,concurrent,61986,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:63293,Performance,concurren,concurrent,63293,"Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:63354,Performance,concurren,concurrent,63354,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:63438,Performance,concurren,concurrent,63438,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:64743,Performance,concurren,concurrent,64743,": Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:64804,Performance,concurren,concurrent,64804,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:64888,Performance,concurren,concurrent,64888,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:66195,Performance,concurren,concurrent,66195,"Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:66256,Performance,concurren,concurrent,66256,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:66340,Performance,concurren,concurrent,66340,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:67647,Performance,concurren,concurrent,67647,"Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:67708,Performance,concurren,concurrent,67708,utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:67792,Performance,concurren,concurrent,67792,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:68985,Performance,concurren,concurrent,68985,"ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:69046,Performance,concurren,concurrent,69046,"it code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:69130,Performance,concurren,concurrent,69130,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:70437,Performance,concurren,concurrent,70437,"Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:70498,Performance,concurren,concurrent,70498,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:70582,Performance,concurren,concurrent,70582,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:71889,Performance,concurren,concurrent,71889,"Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:71950,Performance,concurren,concurrent,71950,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:72034,Performance,concurren,concurrent,72034,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:73341,Performance,concurren,concurrent,73341,"Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:73402,Performance,concurren,concurrent,73402,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:73486,Performance,concurren,concurrent,73486,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:74791,Performance,concurren,concurrent,74791,": Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:74852,Performance,concurren,concurrent,74852,utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:74936,Performance,concurren,concurrent,74936,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.had,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:76424,Performance,concurren,concurrent,76424,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:76485,Performance,concurren,concurrent,76485,"xecutor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:76569,Performance,concurren,concurrent,76569,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:77876,Performance,concurren,concurrent,77876,"Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:77937,Performance,concurren,concurrent,77937,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:78021,Performance,concurren,concurrent,78021,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:79328,Performance,concurren,concurrent,79328,"Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:79389,Performance,concurren,concurrent,79389,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:79473,Performance,concurren,concurrent,79473,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:80780,Performance,concurren,concurrent,80780,"Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:80841,Performance,concurren,concurrent,80841,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:80925,Performance,concurren,concurrent,80925,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:82230,Performance,concurren,concurrent,82230,": Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:82291,Performance,concurren,concurrent,82291,utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:82375,Performance,concurren,concurrent,82375,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:84158,Performance,concurren,concurrent,84158,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:84219,Performance,concurren,concurrent,84219,"xecutor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:84303,Performance,concurren,concurrent,84303,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:85610,Performance,concurren,concurrent,85610,"Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:85671,Performance,concurren,concurrent,85671,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:85755,Performance,concurren,concurrent,85755,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:87062,Performance,concurren,concurrent,87062,"Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:87123,Performance,concurren,concurrent,87123,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:87207,Performance,concurren,concurrent,87207,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:88512,Performance,concurren,concurrent,88512,": Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:88573,Performance,concurren,concurrent,88573,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:88657,Performance,concurren,concurrent,88657,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:89964,Performance,concurren,concurrent,89964,"Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:90025,Performance,concurren,concurrent,90025,utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:90109,Performance,concurren,concurrent,90109,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:91892,Performance,concurren,concurrent,91892,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:91953,Performance,concurren,concurrent,91953,"xecutor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:92037,Performance,concurren,concurrent,92037,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:93344,Performance,concurren,concurrent,93344,"Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:93405,Performance,concurren,concurrent,93405,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:93489,Performance,concurren,concurrent,93489,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:94794,Performance,concurren,concurrent,94794,": Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:94855,Performance,concurren,concurrent,94855,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:94939,Performance,concurren,concurrent,94939,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:96246,Performance,concurren,concurrent,96246,"Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:96307,Performance,concurren,concurrent,96307,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:96391,Performance,concurren,concurrent,96391,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:97698,Performance,concurren,concurrent,97698,"Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:97759,Performance,concurren,concurrent,97759,utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:97843,Performance,concurren,concurrent,97843,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:99332,Performance,concurren,concurrent,99332,end$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:99393,Performance,concurren,concurrent,99393,ecutor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:99477,Performance,concurren,concurrent,99477,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:100659,Performance,concurren,concurrent,100659,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:100720,Performance,concurren,concurrent,100720,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:100804,Performance,concurren,concurrent,100804,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102113,Performance,concurren,concurrent,102113,"st task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102174,Performance,concurren,concurrent,102174,"or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102258,Performance,concurren,concurrent,102258,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103567,Performance,concurren,concurrent,103567,"st task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103628,Performance,concurren,concurrent,103628,"or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103712,Performance,concurren,concurrent,103712,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:105019,Performance,concurren,concurrent,105019,"Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:105080,Performance,concurren,concurrent,105080,or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:105164,Performance,concurren,concurrent,105164,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:106346,Performance,concurren,concurrent,106346,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:106407,Performance,concurren,concurrent,106407,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:106491,Performance,concurren,concurrent,106491,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107800,Performance,concurren,concurrent,107800,"st task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_017",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107861,Performance,concurren,concurrent,107861,or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107945,Performance,concurren,concurrent,107945,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:109436,Performance,concurren,concurrent,109436,"end$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:109497,Performance,concurren,concurrent,109497,"ecutor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:109581,Performance,concurren,concurrent,109581,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:110886,Performance,concurren,concurrent,110886,": Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:110947,Performance,concurren,concurrent,110947,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:111031,Performance,concurren,concurrent,111031,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:112338,Performance,concurren,concurrent,112338,"Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:112399,Performance,concurren,concurrent,112399,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:112483,Performance,concurren,concurrent,112483,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:113790,Performance,concurren,concurrent,113790,"Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:113851,Performance,concurren,concurrent,113851,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:113935,Performance,concurren,concurrent,113935,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115242,Performance,concurren,concurrent,115242,"Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115303,Performance,concurren,concurrent,115303,utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115387,Performance,concurren,concurrent,115387,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.had,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:116875,Performance,concurren,concurrent,116875,kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:116936,Performance,concurren,concurrent,116936,xecutor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:117020,Performance,concurren,concurrent,117020,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:118202,Performance,concurren,concurrent,118202,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:118263,Performance,concurren,concurrent,118263,"non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:118347,Performance,concurren,concurrent,118347,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:119654,Performance,concurren,concurrent,119654,"Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:119715,Performance,concurren,concurrent,119715,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:119799,Performance,concurren,concurrent,119799,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:121104,Performance,concurren,concurrent,121104,": Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:121165,Performance,concurren,concurrent,121165,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:121249,Performance,concurren,concurrent,121249,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:122556,Performance,concurren,concurrent,122556,"Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:122617,Performance,concurren,concurrent,122617,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:122701,Performance,concurren,concurrent,122701,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:124008,Performance,concurren,concurrent,124008,"Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:124069,Performance,concurren,concurrent,124069,utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:124153,Performance,concurren,concurrent,124153,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:125335,Performance,concurren,concurrent,125335,in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_0,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:125396,Performance,concurren,concurrent,125396,non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:125480,Performance,concurren,concurrent,125480,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.had,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:126968,Performance,concurren,concurrent,126968,"kend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:127029,Performance,concurren,concurrent,127029,"xecutor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:127113,Performance,concurren,concurrent,127113,"bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:128420,Performance,concurren,concurrent,128420,"Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:128481,Performance,concurren,concurrent,128481,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:128565,Performance,concurren,concurrent,128565,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:129870,Performance,concurren,concurrent,129870,": Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:129931,Performance,concurren,concurrent,129931,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:130015,Performance,concurren,concurrent,130015,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:131322,Performance,concurren,concurrent,131322,"Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:131383,Performance,concurren,concurrent,131383,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:131467,Performance,concurren,concurrent,131467,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:132774,Performance,concurren,concurrent,132774,"Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 3 requested; 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 3 from BlockManagerMaster.; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 3; 2019-01-22 13:11:57 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56628) with ID 18; 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18, partition 15,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:132835,Performance,concurren,concurrent,132835,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 3 requested; 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 3 from BlockManagerMaster.; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 3; 2019-01-22 13:11:57 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56628) with ID 18; 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18, partition 15, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:11:57 TaskSetManag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:132919,Performance,concurren,concurrent,132919,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 3 requested; 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 3 from BlockManagerMaster.; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 3; 2019-01-22 13:11:57 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56628) with ID 18; 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18, partition 15, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:137786,Performance,concurren,concurrent,137786,58 DAGScheduler: INFO: Shuffle files lost for executor: 11 (epoch 11); 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 11 on scc-q17.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:137847,Performance,concurren,concurrent,137847,poch 11); 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 11 on scc-q17.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:137931,Performance,concurren,concurrent,137931,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:139113,Performance,concurren,concurrent,139113,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:139174,Performance,concurren,concurrent,139174,"non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:139258,Performance,concurren,concurrent,139258,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140567,Performance,concurren,concurrent,140567,"st task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140628,Performance,concurren,concurrent,140628,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140712,Performance,concurren,concurrent,140712,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142021,Performance,concurren,concurrent,142021,"st task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142082,Performance,concurren,concurrent,142082,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142166,Performance,concurren,concurrent,142166,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143474,Performance,concurren,concurrent,143474,"ost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143535,Performance,concurren,concurrent,143535,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143619,Performance,concurren,concurrent,143619,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:144928,Performance,concurren,concurrent,144928,"st task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_01",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:144989,Performance,concurren,concurrent,144989,or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:145073,Performance,concurren,concurrent,145073,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:146565,Performance,concurren,concurrent,146565,nd$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:146626,Performance,concurren,concurrent,146626,cutor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:146710,Performance,concurren,concurrent,146710,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:147892,Performance,concurren,concurrent,147892,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:147953,Performance,concurren,concurrent,147953,"non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:148037,Performance,concurren,concurrent,148037,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149346,Performance,concurren,concurrent,149346,"st task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149407,Performance,concurren,concurrent,149407,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149491,Performance,concurren,concurrent,149491,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150800,Performance,concurren,concurrent,150800,"st task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150861,Performance,concurren,concurrent,150861,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150945,Performance,concurren,concurrent,150945,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152254,Performance,concurren,concurrent,152254,"st task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152315,Performance,concurren,concurrent,152315,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152399,Performance,concurren,concurrent,152399,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153707,Performance,concurren,concurrent,153707,"ost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153768,Performance,concurren,concurrent,153768,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:00 TaskSetMana",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153852,Performance,concurren,concurrent,153852,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:159695,Performance,concurren,concurrent,159695,02 DAGScheduler: INFO: Shuffle files lost for executor: 13 (epoch 13); 2019-01-22 13:12:02 YarnScheduler: ERROR: Lost executor 15 on scc-q10.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:159756,Performance,concurren,concurrent,159756,poch 13); 2019-01-22 13:12:02 YarnScheduler: ERROR: Lost executor 15 on scc-q10.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:159840,Performance,concurren,concurrent,159840,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:161022,Performance,concurren,concurrent,161022,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:161083,Performance,concurren,concurrent,161083,"non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:161167,Performance,concurren,concurrent,161167,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162476,Performance,concurren,concurrent,162476,"st task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162537,Performance,concurren,concurrent,162537,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162621,Performance,concurren,concurrent,162621,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:163930,Performance,concurren,concurrent,163930,"st task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:163991,Performance,concurren,concurrent,163991,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:164075,Performance,concurren,concurrent,164075,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165383,Performance,concurren,concurrent,165383,"ost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165444,Performance,concurren,concurrent,165444,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165528,Performance,concurren,concurrent,165528,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166837,Performance,concurren,concurrent,166837,"st task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166898,Performance,concurren,concurrent,166898,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition 25, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetMa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166982,Performance,concurren,concurrent,166982,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition 25, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:170244,Performance,concurren,concurrent,170244,"q12.scc.bu.edu, executor 21, partition 29, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:170305,Performance,concurren,concurrent,170305,7 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:170389,Performance,concurren,concurrent,170389,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:171571,Performance,concurren,concurrent,171571,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:171632,Performance,concurren,concurrent,171632,"non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:171716,Performance,concurren,concurrent,171716,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173024,Performance,concurren,concurrent,173024,"ost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173085,Performance,concurren,concurrent,173085,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173169,Performance,concurren,concurrent,173169,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174478,Performance,concurren,concurrent,174478,"st task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174539,Performance,concurren,concurrent,174539,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174623,Performance,concurren,concurrent,174623,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:175932,Performance,concurren,concurrent,175932,"st task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:175993,Performance,concurren,concurrent,175993,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:176077,Performance,concurren,concurrent,176077,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177386,Performance,concurren,concurrent,177386,"st task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177447,Performance,concurren,concurrent,177447,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.5 KB, free: 21.2 GB); 2019-01-22 13:12:04 BlockManagerInfo: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177531,Performance,concurren,concurrent,177531,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.5 KB, free: 21.2 GB); 2019-01-22 13:12:04 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q03.scc.bu.edu:36955 (size: 24.5 KB, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:181743,Performance,concurren,concurrent,181743,05 DAGScheduler: INFO: Shuffle files lost for executor: 12 (epoch 15); 2019-01-22 13:12:05 YarnScheduler: ERROR: Lost executor 21 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:181804,Performance,concurren,concurrent,181804,poch 15); 2019-01-22 13:12:05 YarnScheduler: ERROR: Lost executor 21 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:181888,Performance,concurren,concurrent,181888,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:183070,Performance,concurren,concurrent,183070,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:183131,Performance,concurren,concurrent,183131,"non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:183215,Performance,concurren,concurrent,183215,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184523,Performance,concurren,concurrent,184523,"ost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184584,Performance,concurren,concurrent,184584,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184668,Performance,concurren,concurrent,184668,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:185977,Performance,concurren,concurrent,185977,"st task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:186038,Performance,concurren,concurrent,186038,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:186122,Performance,concurren,concurrent,186122,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187431,Performance,concurren,concurrent,187431,"st task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187492,Performance,concurren,concurrent,187492,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187576,Performance,concurren,concurrent,187576,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:188885,Performance,concurren,concurrent,188885,"st task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 Blo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:188946,Performance,concurren,concurrent,188946,or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Removing block manager BlockMa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:189030,Performance,concurren,concurrent,189030,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(14, scc-q05.scc.bu.edu, 42935, None); 2019-01-22 13:12:06 BlockManagerMaster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:191099,Performance,concurren,concurrent,191099,06 DAGScheduler: INFO: Shuffle files lost for executor: 14 (epoch 16); 2019-01-22 13:12:06 YarnScheduler: ERROR: Lost executor 12 on scc-q03.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:191160,Performance,concurren,concurrent,191160,poch 16); 2019-01-22 13:12:06 YarnScheduler: ERROR: Lost executor 12 on scc-q03.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:191244,Performance,concurren,concurrent,191244,bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:192426,Performance,concurren,concurrent,192426,"in : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:192487,Performance,concurren,concurrent,192487,"non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:192571,Performance,concurren,concurrent,192571,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:193880,Performance,concurren,concurrent,193880,"st task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hado",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:193941,Performance,concurren,concurrent,193941,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:194025,Performance,concurren,concurrent,194025,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195427,Performance,concurren,concurrent,195427,"ost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195488,Performance,concurren,concurrent,195488,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195572,Performance,concurren,concurrent,195572,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:196881,Performance,concurren,concurrent,196881,"st task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:196942,Performance,concurren,concurrent,196942,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:197026,Performance,concurren,concurrent,197026,"Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198335,Performance,concurren,concurrent,198335,"st task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198396,Performance,concurren,concurrent,198396,or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198480,Performance,concurren,concurrent,198480,Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.sche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213253,Performance,concurren,concurrent,213253,"andlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213356,Performance,concurren,concurrent,213356,"tFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedExcepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213528,Performance,concurren,concurrent,213528,"lHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatche",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213625,Performance,concurren,concurrent,213625,"l.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213901,Performance,concurren,concurrent,213901,"actChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBack",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213955,Performance,concurren,concurrent,213955,"l.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(Yarn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214047,Performance,concurren,concurrent,214047,"); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$Yarn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214142,Performance,queue,queued,214142,"AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215224,Performance,concurren,concurrent,215224,hedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(Mo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215295,Performance,concurren,concurrent,215295,fter disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215366,Performance,concurren,concurrent,215366,v already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215454,Performance,concurren,concurrent,215454,a:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$D,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215547,Performance,concurren,concurrent,215547,rg.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Prom,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215642,Performance,concurren,concurrent,215642,.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215720,Performance,concurren,concurrent,215720,dpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215800,Performance,concurren,concurrent,215800,rnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215862,Performance,concurren,concurrent,215862,park$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.gua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215939,Performance,concurren,concurrent,215939,iver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216010,Performance,concurren,concurrent,216010,uler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216081,Performance,concurren,concurrent,216081,ache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.Callbac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216169,Performance,concurren,concurrent,216169,2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216262,Performance,concurren,concurrent,216262,Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216357,Performance,concurren,concurrent,216357,ala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216435,Performance,concurren,concurrent,216435,ct.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216515,Performance,concurren,concurrent,216515,cutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216577,Performance,concurren,concurrent,216577,l$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.gua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216654,Performance,concurren,concurrent,216654,allbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216721,Performance,concurren,concurrent,216721,rent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216788,Performance,concurren,concurrent,216788, scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216876,Performance,concurren,concurrent,216876,mise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$D,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216969,Performance,concurren,concurrent,216969,1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Prom,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217064,Performance,concurren,concurrent,217064,); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217142,Performance,concurren,concurrent,217142,rk_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217222,Performance,concurren,concurrent,217222,(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217284,Performance,concurren,concurrent,217284,ntextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.Batc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217361,Performance,concurren,concurrent,217361,t.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217432,Performance,concurren,concurrent,217432,ncurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217503,Performance,concurren,concurrent,217503, scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217591,Performance,concurren,concurrent,217591,mise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Ba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217684,Performance,concurren,concurrent,217684,ply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217779,Performance,concurren,concurrent,217779,ala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217857,Performance,concurren,concurrent,217857,ct.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217937,Performance,concurren,concurrent,217937,cutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(Bat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217999,Performance,concurren,concurrent,217999,l$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218076,Performance,concurren,concurrent,218076,allbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.Callback,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218143,Performance,concurren,concurrent,218143,rent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.im,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218210,Performance,concurren,concurrent,218210, scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218275,Performance,concurren,concurrent,218275,ala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218376,Performance,concurren,concurrent,218376,nonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.N,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218475,Performance,concurren,concurrent,218475,e.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apach,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218567,Performance,concurren,concurrent,218567,project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.r,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218659,Performance,concurren,concurrent,218659,java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218734,Performance,concurren,concurrent,218734,ecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.cl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218809,Performance,concurren,concurrent,218809,ecuteWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHand,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218898,Performance,concurren,concurrent,218898,lete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactiv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218978,Performance,concurren,concurrent,218978,a:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.Transpo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219058,Performance,concurren,concurrent,219058,3); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219136,Performance,concurren,concurrent,219136,a.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHan,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219216,Performance,concurren,concurrent,219216,impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219281,Performance,concurren,concurrent,219281,BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.net,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:222435,Performance,concurren,concurrent,222435,erContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 ShutdownHookManager: INFO: Shutdown hook called; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/pyspark-9e6e1242-84dd-4fa1-8e8a-210dac6042fb; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:222538,Performance,concurren,concurrent,222538,erContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 ShutdownHookManager: INFO: Shutdown hook called; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/pyspark-9e6e1242-84dd-4fa1-8e8a-210dac6042fb; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:222710,Performance,concurren,concurrent,222710,erContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 ShutdownHookManager: INFO: Shutdown hook called; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/pyspark-9e6e1242-84dd-4fa1-8e8a-210dac6042fb; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:222807,Performance,concurren,concurrent,222807,erContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 ShutdownHookManager: INFO: Shutdown hook called; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/pyspark-9e6e1242-84dd-4fa1-8e8a-210dac6042fb; 2019-01-22 13:12:06 ShutdownHookManager: INFO: Deleting directory /tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:194375,Safety,abort,aborting,194375,"p.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201647,Safety,abort,abortStage,201647,$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201744,Safety,abort,abortStage,201744,ancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 Y,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201986,Safety,abort,abortStage,201986,r$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:211331,Safety,timeout,timeout,211331,"rom /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:278); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecode",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215966,Safety,recover,recover,215966,hedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216037,Safety,recover,recover,216037,hedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionCon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217388,Safety,recover,recover,217388,able.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217459,Safety,recover,recover,217459,se$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:220513,Safety,timeout,timeout,220513,un$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:278); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecode,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2424,Security,Secur,SecurityManager,2424,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2499,Security,Secur,SecurityManager,2499,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2576,Security,Secur,SecurityManager,2576,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2650,Security,Secur,SecurityManager,2650,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2726,Security,Secur,SecurityManager,2726,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2749,Security,Secur,SecurityManager,2749,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2766,Security,authenticat,authentication,2766,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9218,Security,Secur,SecurityManager,9218,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9293,Security,Secur,SecurityManager,9293,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9370,Security,Secur,SecurityManager,9370,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9444,Security,Secur,SecurityManager,9444,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9520,Security,Secur,SecurityManager,9520,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9543,Security,Secur,SecurityManager,9543,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9560,Security,authenticat,authentication,9560,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16371,Security,Secur,SecurityManager,16371,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16446,Security,Secur,SecurityManager,16446,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16523,Security,Secur,SecurityManager,16523,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16597,Security,Secur,SecurityManager,16597,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16673,Security,Secur,SecurityManager,16673,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16696,Security,Secur,SecurityManager,16696,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16713,Security,authenticat,authentication,16713,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199429,Security,Hash,HashMap,199429,er(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199443,Security,Hash,HashMap,199443,olExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199921,Security,Hash,HashSet,199921,nagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199937,Security,Hash,HashSet,199937,; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200232,Security,Hash,HashMap,200232,chElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200265,Security,Hash,HashMap,200265,ey not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.schedul,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200312,Security,Hash,HashMap,200312,(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGSched,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200345,Security,Hash,HashMap,200345,at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndInde,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200392,Security,Hash,HashTable,200392, at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200421,Security,Hash,HashTable,200421,n.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutabl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200471,Security,Hash,HashMap,200471,.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200492,Security,Hash,HashMap,200492,er.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAG,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200539,Security,Hash,HashMap,200539,pply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$fa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200555,Security,Hash,HashMap,200555,fun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndInde,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201422,Security,Hash,HashSet,201422,able.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doO,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201438,Security,Hash,HashSet,201438,30); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAG,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203076,Security,Hash,HashMap,203076,eTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203090,Security,Hash,HashMap,203090,led$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203568,Security,Hash,HashSet,203568,ark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203584,Security,Hash,HashSet,203584,r.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203879,Security,Hash,HashMap,203879,chElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203912,Security,Hash,HashMap,203912,ey not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.schedul,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203959,Security,Hash,HashMap,203959,(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGSched,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203992,Security,Hash,HashMap,203992,at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndInde,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204039,Security,Hash,HashTable,204039, at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204068,Security,Hash,HashTable,204068,n.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutabl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204118,Security,Hash,HashMap,204118,.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204139,Security,Hash,HashMap,204139,er.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAG,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204186,Security,Hash,HashMap,204186,pply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$fa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204202,Security,Hash,HashMap,204202,fun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndInde,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205069,Security,Hash,HashSet,205069,"able.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205085,Security,Hash,HashSet,205085,"30); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205672,Security,Hash,HashSet,205672,"endentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGSche",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205688,Security,Hash,HashSet,205688,"$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206833,Security,Hash,HashSet,206833,"tProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206849,Security,Hash,HashSet,206849,".onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:15,Testability,log,log,15,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:65,Testability,log,logs,65,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:172,Testability,log,log,172,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1128,Testability,log,logConf,1128,"-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:3724,Testability,log,log,3724," 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:3735,Testability,Log,Logging,3735," 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6975,Testability,log,log,6975," o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/ge",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7926,Testability,log,logConf,7926,"/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:10516,Testability,log,log,10516,"22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:10527,Testability,Log,Logging,10527,"22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209969,Usability,clear,cleared,209969,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
https://github.com/hail-is/hail/pull/4734#issuecomment-436661120:334,Integrability,interface,interface,334,"This isn't what I intended with my original design and I'm trying to figure out what you did here. - I intended for entire file, not individual specs, to have a version number. That's why it is in RelationalSpec at the top level.; - FooSpec (e.g. RelationalSpec, RVDSpec, etc.) is supposed to be an abstract class that implements the interface used by the main code for the entity in question. E.g. CodecSpec knows how to build encoders and decoders. It can have many implementations.; - The file version essentially determines which Spec implementations are allowed in the file. Each spec should have a notion of when it was introduced (or deprecated, if we bump the major version) but each implementation should have its own name/version. I was imagining FooRVD2Spec if we needed a new version of the existing RVD specs.; - The type hints in the JSON determine what Spec implementation to use (e.g. UnpartitionedRVDSpec). With this design, you can't change name of an existing spec.; - The Spec is also the intended place for code which matches the interface (which might evolve) with the legacy data in the JSON file (which cannot change).; - The file format is only used to check compatibility. The Spec class hierarchy should drive the decoding of the Specs, not the file format.; - Specs that are not being generated anymore (but still need to be read) should go into a compatibility package. I think following this design will be cleaner. This is exactly the kind of forward-extensibility I designed it to handle cleanly, although I might very well have messed something up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436661120
https://github.com/hail-is/hail/pull/4734#issuecomment-436661120:1051,Integrability,interface,interface,1051,"This isn't what I intended with my original design and I'm trying to figure out what you did here. - I intended for entire file, not individual specs, to have a version number. That's why it is in RelationalSpec at the top level.; - FooSpec (e.g. RelationalSpec, RVDSpec, etc.) is supposed to be an abstract class that implements the interface used by the main code for the entity in question. E.g. CodecSpec knows how to build encoders and decoders. It can have many implementations.; - The file version essentially determines which Spec implementations are allowed in the file. Each spec should have a notion of when it was introduced (or deprecated, if we bump the major version) but each implementation should have its own name/version. I was imagining FooRVD2Spec if we needed a new version of the existing RVD specs.; - The type hints in the JSON determine what Spec implementation to use (e.g. UnpartitionedRVDSpec). With this design, you can't change name of an existing spec.; - The Spec is also the intended place for code which matches the interface (which might evolve) with the legacy data in the JSON file (which cannot change).; - The file format is only used to check compatibility. The Spec class hierarchy should drive the decoding of the Specs, not the file format.; - Specs that are not being generated anymore (but still need to be read) should go into a compatibility package. I think following this design will be cleaner. This is exactly the kind of forward-extensibility I designed it to handle cleanly, although I might very well have messed something up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436661120
https://github.com/hail-is/hail/pull/4734#issuecomment-436661120:1074,Modifiability,evolve,evolve,1074,"This isn't what I intended with my original design and I'm trying to figure out what you did here. - I intended for entire file, not individual specs, to have a version number. That's why it is in RelationalSpec at the top level.; - FooSpec (e.g. RelationalSpec, RVDSpec, etc.) is supposed to be an abstract class that implements the interface used by the main code for the entity in question. E.g. CodecSpec knows how to build encoders and decoders. It can have many implementations.; - The file version essentially determines which Spec implementations are allowed in the file. Each spec should have a notion of when it was introduced (or deprecated, if we bump the major version) but each implementation should have its own name/version. I was imagining FooRVD2Spec if we needed a new version of the existing RVD specs.; - The type hints in the JSON determine what Spec implementation to use (e.g. UnpartitionedRVDSpec). With this design, you can't change name of an existing spec.; - The Spec is also the intended place for code which matches the interface (which might evolve) with the legacy data in the JSON file (which cannot change).; - The file format is only used to check compatibility. The Spec class hierarchy should drive the decoding of the Specs, not the file format.; - Specs that are not being generated anymore (but still need to be read) should go into a compatibility package. I think following this design will be cleaner. This is exactly the kind of forward-extensibility I designed it to handle cleanly, although I might very well have messed something up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436661120
https://github.com/hail-is/hail/pull/4734#issuecomment-436684642:102,Integrability,interface,interface,102,"Also, the RVDSpec IS versioned: the version is the name of the concrete spec implementing the RVDSpec interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436684642
https://github.com/hail-is/hail/pull/4734#issuecomment-436689558:77,Modifiability,rewrite,rewrite,77,"I think I'm on board with the RVDSpec stuff you've proposed. I still want to rewrite the names of the MatrixTableSpec and TableSpec, since these should have been abstract from the beginning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436689558
https://github.com/hail-is/hail/pull/4734#issuecomment-436733090:79,Modifiability,rewrite,rewrite,79,"> I think I'm on board with the RVDSpec stuff you've proposed. I still want to rewrite the names of the MatrixTableSpec and TableSpec, since these should have been abstract from the beginning. You can't rename concrete Specs. RelationalSpec was the abstract one. You can debate about what should be in RelationalSpec vs. the concrete members (e.g. maybe partitionCounts shouldn't be in there.). RelationalSpecs are very abstract: they are just a collection of ""components"", whatever those are, with an assumption there is one component called the ""global"" component.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436733090
https://github.com/hail-is/hail/pull/4734#issuecomment-437405727:143,Modifiability,evolve,evolve,143,"Just want to verify, first cut for EType will be like the existing Type (with missingness), right? Then, Type, EType and PType will be free to evolve separately: we can remove missingness for Type, add non-encoded alternate representations to PTypes, and add alternate encodings (e.g. struct of arrays as arrays of structs) to ETypes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-437405727
https://github.com/hail-is/hail/pull/4734#issuecomment-437416979:145,Modifiability,evolve,evolve,145,"> Just want to verify, first cut for EType will be like the existing Type (with missingness), right? Then, Type, EType and PType will be free to evolve separately: we can remove missingness for Type, add non-encoded alternate representations to PTypes, and add alternate encodings (e.g. struct of arrays as arrays of structs) to ETypes. Yes, exactly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-437416979
https://github.com/hail-is/hail/issues/4738#issuecomment-436428753:59,Performance,optimiz,optimize,59,I'm in favor of keeping this as a feature. It means we can optimize things way more and more easily.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4738#issuecomment-436428753
https://github.com/hail-is/hail/pull/4745#issuecomment-436859081:39,Deployability,update,update,39,I renamed a Makefile target but didn't update the deploy script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4745#issuecomment-436859081
https://github.com/hail-is/hail/pull/4745#issuecomment-436859081:50,Deployability,deploy,deploy,50,I renamed a Makefile target but didn't update the deploy script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4745#issuecomment-436859081
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:64,Availability,error,error,64,"With Hail 0.2-721af83bc30a, it is now getting a java heap space error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:148,Availability,avail,available,148,"With Hail 0.2-721af83bc30a, it is now getting a java heap space error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2196,Availability,Error,Error,2196,", **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2504,Availability,failure,failure,2504,", output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2562,Availability,failure,failure,2562,"147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterato",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10674,Availability,Error,Error,10674," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10724,Availability,ERROR,ERROR,10724," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:11434,Availability,Error,Error,11434," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:11518,Availability,Error,Error,11518," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1057,Deployability,install,install,1057," error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1420,Deployability,install,install,1420,"iobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1705,Deployability,install,install,1705,"ojectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2018,Deployability,install,install,2018,"pec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10834,Deployability,install,install,10834," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:11180,Deployability,install,install,11180," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:11343,Deployability,install,install,11343," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5023,Energy Efficiency,schedul,scheduler,5023,onfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5063,Energy Efficiency,schedul,scheduler,5063,textRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5161,Energy Efficiency,schedul,scheduler,5161,thIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5258,Energy Efficiency,schedul,scheduler,5258,un$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5509,Energy Efficiency,schedul,scheduler,5509,$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5589,Energy Efficiency,schedul,scheduler,5589,ator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5694,Energy Efficiency,schedul,scheduler,5694,eneric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5842,Energy Efficiency,schedul,scheduler,5842,ction.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5930,Energy Efficiency,schedul,scheduler,5930,rsableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:6027,Energy Efficiency,schedul,scheduler,6027,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.ge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:6122,Energy Efficiency,schedul,scheduler,6122,.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.Ma,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:6285,Energy Efficiency,schedul,scheduler,6285,abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1128,Integrability,wrap,wrapper,1128,"_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1179,Integrability,wrap,wrapper,1179,"_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1384,Integrability,wrap,wrapper,1384,"_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10987,Integrability,protocol,protocol,10987," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:11491,Integrability,protocol,protocol,11491," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:3465,Performance,Load,LoadVCF,3465,"borted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:3511,Performance,Load,LoadVCF,3511,": Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:7160,Performance,Load,LoadVCF,7160,ssLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.scala:1008); at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:647); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); at is.hail.variant.MatrixTable.write(MatrixTable.scala:1238); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:7222,Performance,Load,LoadVCF,7222,EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.scala:1008); at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:647); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); at is.hail.variant.MatrixTable.write(MatrixTable.scala:1238); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.r,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:7282,Performance,Load,LoadVCF,7282,k.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.scala:1008); at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:647); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); at is.hail.variant.MatrixTable.write(MatrixTable.scala:1238); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Threa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:9124,Performance,Load,LoadVCF,9124,.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:9170,Performance,Load,LoadVCF,9170,te(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2483,Safety,abort,aborted,2483,", output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5193,Safety,abort,abortStage,5193,ly$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5290,Safety,abort,abortStage,5290,y$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5532,Safety,abort,abortStage,5532,); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:288,Testability,LOG,LOGGING,288,"With Hail 0.2-721af83bc30a, it is now getting a java heap space error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:394,Testability,log,log,394,"With Hail 0.2-721af83bc30a, it is now getting a java heap space error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635
https://github.com/hail-is/hail/issues/4755#issuecomment-439415237:92,Availability,error,error,92,This was running locally rather than the spark cluster which I expect relates to the memory error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-439415237
https://github.com/hail-is/hail/pull/4759#issuecomment-439925430:16,Testability,test,tests,16,"ah, need to fix tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4759#issuecomment-439925430
https://github.com/hail-is/hail/pull/4763#issuecomment-441718711:4,Testability,test,tests,4,"The tests are being rerun, they were failing a second ago. @danking it's known that we want to be able to view previous test runs in ci exactly for cases like this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4763#issuecomment-441718711
https://github.com/hail-is/hail/pull/4763#issuecomment-441718711:120,Testability,test,test,120,"The tests are being rerun, they were failing a second ago. @danking it's known that we want to be able to view previous test runs in ci exactly for cases like this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4763#issuecomment-441718711
https://github.com/hail-is/hail/issues/4770#issuecomment-452847982:710,Availability,error,error,710,"So the issue is that we used to have `filter` and `explode` inside of aggregations (like `counter` and `collect_as_set`). Now they're placed outside of these operations. There was [a forum post](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701) announcing this breaking change. The above to examples should instead be written as:. ```; cut_dict = {'pop': hl.agg.filter(hl.is_defined(mt.meta.pop), hl.agg.counter(mt.meta.pop)),; 'subpop': hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.agg.collect_as_set(hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; ```. The fix for this issue is to change the assertion into an `if` with a `raise` of an error message, probably one that references that discuss post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982
https://github.com/hail-is/hail/issues/4770#issuecomment-452847982:260,Integrability,interface,interface,260,"So the issue is that we used to have `filter` and `explode` inside of aggregations (like `counter` and `collect_as_set`). Now they're placed outside of these operations. There was [a forum post](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701) announcing this breaking change. The above to examples should instead be written as:. ```; cut_dict = {'pop': hl.agg.filter(hl.is_defined(mt.meta.pop), hl.agg.counter(mt.meta.pop)),; 'subpop': hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.agg.collect_as_set(hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; ```. The fix for this issue is to change the assertion into an `if` with a `raise` of an error message, probably one that references that discuss post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982
https://github.com/hail-is/hail/issues/4770#issuecomment-452847982:716,Integrability,message,message,716,"So the issue is that we used to have `filter` and `explode` inside of aggregations (like `counter` and `collect_as_set`). Now they're placed outside of these operations. There was [a forum post](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701) announcing this breaking change. The above to examples should instead be written as:. ```; cut_dict = {'pop': hl.agg.filter(hl.is_defined(mt.meta.pop), hl.agg.counter(mt.meta.pop)),; 'subpop': hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.agg.collect_as_set(hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; ```. The fix for this issue is to change the assertion into an `if` with a `raise` of an error message, probably one that references that discuss post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982
https://github.com/hail-is/hail/issues/4770#issuecomment-452847982:666,Testability,assert,assertion,666,"So the issue is that we used to have `filter` and `explode` inside of aggregations (like `counter` and `collect_as_set`). Now they're placed outside of these operations. There was [a forum post](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701) announcing this breaking change. The above to examples should instead be written as:. ```; cut_dict = {'pop': hl.agg.filter(hl.is_defined(mt.meta.pop), hl.agg.counter(mt.meta.pop)),; 'subpop': hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.agg.collect_as_set(hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; ```. The fix for this issue is to change the assertion into an `if` with a `raise` of an error message, probably one that references that discuss post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982
https://github.com/hail-is/hail/issues/4775#issuecomment-438644309:179,Deployability,deploy,deploy,179,"Ack. The problem here is that the version on PyPI is a bit old. I changed the behavior of `get_1kg` to also keep the VCF, and changed the tutorial, a few weeks ago. I'm hoping to deploy a new version to PyPI by the end of this week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775#issuecomment-438644309
https://github.com/hail-is/hail/issues/4780#issuecomment-439073706:177,Availability,avail,available,177,"The script is running fine on the smaller chromosome 19 to 22 bgen files so far. However, I noticed each were running just 24 cores even though we have 16 nodes * 16 cores each available on the cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439073706
https://github.com/hail-is/hail/issues/4780#issuecomment-439414395:635,Availability,error,error,635,"I see what is happening. . The Hail cluster install instructions specify the following for a spark cluster:. export PYSPARK_SUBMIT_ARGS=""\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; pyspark-shell"". On our cluster, this will run as a local job. It needs a ""--master yarn"" for an argument. Running it locally probably is related to the out of memory error and the limited cores. I will rerun this with the --master yarn argument. . Regarding the bgen file versus matrix table, are you suggesting, it would be faster to run an analysis such as a logistic regression starting with the bgen file instead of the imported bgen mt file. The phenotypes would need to annotated the imported bgen mt every time. Just trying to understand the trade offs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395
https://github.com/hail-is/hail/issues/4780#issuecomment-439414395:44,Deployability,install,install,44,"I see what is happening. . The Hail cluster install instructions specify the following for a spark cluster:. export PYSPARK_SUBMIT_ARGS=""\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; pyspark-shell"". On our cluster, this will run as a local job. It needs a ""--master yarn"" for an argument. Running it locally probably is related to the out of memory error and the limited cores. I will rerun this with the --master yarn argument. . Regarding the bgen file versus matrix table, are you suggesting, it would be faster to run an analysis such as a logistic regression starting with the bgen file instead of the imported bgen mt file. The phenotypes would need to annotated the imported bgen mt every time. Just trying to understand the trade offs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395
https://github.com/hail-is/hail/issues/4780#issuecomment-439414395:830,Testability,log,logistic,830,"I see what is happening. . The Hail cluster install instructions specify the following for a spark cluster:. export PYSPARK_SUBMIT_ARGS=""\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; pyspark-shell"". On our cluster, this will run as a local job. It needs a ""--master yarn"" for an argument. Running it locally probably is related to the out of memory error and the limited cores. I will rerun this with the --master yarn argument. . Regarding the bgen file versus matrix table, are you suggesting, it would be faster to run an analysis such as a logistic regression starting with the bgen file instead of the imported bgen mt file. The phenotypes would need to annotated the imported bgen mt every time. Just trying to understand the trade offs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395
https://github.com/hail-is/hail/issues/4780#issuecomment-439415482:285,Energy Efficiency,efficient,efficiently,285,"> The phenotypes would need to annotated the imported bgen mt every time. This is very cheap, especially compared to the extra IO/decoding burden. I should note, though, that in the next year we'll start to develop new types of file encodings that should let us represent this data as efficiently as the BGEN in a faster way (using a faster compression codec than zlib)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439415482
https://github.com/hail-is/hail/pull/4784#issuecomment-439439892:67,Availability,error,errors,67,I'm going to close this. The problem is more systemic -- got other errors with the types not being correct for `Die` and `In`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4784#issuecomment-439439892
https://github.com/hail-is/hail/pull/4789#issuecomment-440805729:42,Testability,test,test,42,"odd, it looks like we're using python2 to test this file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4789#issuecomment-440805729
https://github.com/hail-is/hail/issues/4799#issuecomment-440070333:54,Deployability,pipeline,pipeline,54,Just got something like this on a completely separate pipeline. This appears to occur after grouping on something and not including a previous key. FWIW works with a4f79a3b3269. cc @jbloom22,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799#issuecomment-440070333
https://github.com/hail-is/hail/pull/4808#issuecomment-440499564:455,Security,audit,audit,455,"I got pretty fatigued from **hours** of iterating on `gradle doctest` and copy-paste after the ~25th iteration. I started sprinkling the NOTEST stuff in then. There are some places that seem unavoidable -- things that return dicts whose order may vary, for instance. I also added NOTEST directives sometimes when we print string representations of objects, because this was easier at the time and made the unit of work more manageable. I do want to fully audit the ones I've added, and have reasons for including each NOTEST. But I'd prefer to do that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4808#issuecomment-440499564
https://github.com/hail-is/hail/pull/4809#issuecomment-440375862:67,Testability,test,test,67,cc @danking can you take a look and make sure I didn't mess up the test reporting?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4809#issuecomment-440375862
https://github.com/hail-is/hail/pull/4809#issuecomment-440756951:44,Testability,test,test,44,"@patrick-schultz I made sure that a failing test would show up correctly on the report, but I forgot to save the link to the build report so I can't show it to you :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4809#issuecomment-440756951
https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:49,Availability,redundant,redundant,49,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512
https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:251,Deployability,deploy,deploy,251,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512
https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:635,Modifiability,variab,variables,635,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512
https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:602,Performance,load,load,602,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512
https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:49,Safety,redund,redundant,49,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512
https://github.com/hail-is/hail/issues/4816#issuecomment-440640835:15,Integrability,depend,dependency,15,Is there a new dependency? Just checked out that branch and got missing `catch.hpp`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4816#issuecomment-440640835
https://github.com/hail-is/hail/issues/4817#issuecomment-451506878:1197,Usability,guid,guides,1197,"s/random.rst:27: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; docs/functions/random.rst:30: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; docs/functions/random.rst:42: >>> hl.eval(hl.array([a, b, c])) # doctest: +NOTEST; docs/functions/random.rst:50: >>> table.show() # doctest: +NOTEST; docs/functions/random.rst:72: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:75: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:82: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:90: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:98: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:110: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/functions/random.rst:114: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:68: >>> mt # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:71: >>> mt.locus # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:83: >>> mt.DP.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:107: >>> mt.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:266: >>> mt_new.replicate_num.show() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:299: >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:307: >>> mt.aggregate_entries((agg.stats(mt.DP), agg.stats(mt.GQ))) # doctest: +NOTEST; docs/hailpedia/table.rst:63: >>> ht.describe() # doctest: +NOTEST; docs/hailpedia/table.rst:102: >>> ht # doctest: +NOTEST; docs/hailpedia/table.rst:105: >>> ht.ID # doctest: +NOTEST; experimental",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-451506878
https://github.com/hail-is/hail/issues/4817#issuecomment-451506878:1262,Usability,guid,guides,1262,"docs/functions/random.rst:30: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; docs/functions/random.rst:42: >>> hl.eval(hl.array([a, b, c])) # doctest: +NOTEST; docs/functions/random.rst:50: >>> table.show() # doctest: +NOTEST; docs/functions/random.rst:72: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:75: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:82: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:90: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:98: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:110: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/functions/random.rst:114: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:68: >>> mt # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:71: >>> mt.locus # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:83: >>> mt.DP.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:107: >>> mt.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:266: >>> mt_new.replicate_num.show() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:299: >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:307: >>> mt.aggregate_entries((agg.stats(mt.DP), agg.stats(mt.GQ))) # doctest: +NOTEST; docs/hailpedia/table.rst:63: >>> ht.describe() # doctest: +NOTEST; docs/hailpedia/table.rst:102: >>> ht # doctest: +NOTEST; docs/hailpedia/table.rst:105: >>> ht.ID # doctest: +NOTEST; experimental/import_gtf.py:56: >>> ht.describe() # doctest: +NOTEST; expr/aggrega",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-451506878
https://github.com/hail-is/hail/issues/4817#issuecomment-451506878:1328,Usability,guid,guides,1328,"ctest: +NOTEST; docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; docs/functions/random.rst:42: >>> hl.eval(hl.array([a, b, c])) # doctest: +NOTEST; docs/functions/random.rst:50: >>> table.show() # doctest: +NOTEST; docs/functions/random.rst:72: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:75: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:82: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:90: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:98: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:110: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/functions/random.rst:114: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:68: >>> mt # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:71: >>> mt.locus # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:83: >>> mt.DP.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:107: >>> mt.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:266: >>> mt_new.replicate_num.show() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:299: >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:307: >>> mt.aggregate_entries((agg.stats(mt.DP), agg.stats(mt.GQ))) # doctest: +NOTEST; docs/hailpedia/table.rst:63: >>> ht.describe() # doctest: +NOTEST; docs/hailpedia/table.rst:102: >>> ht # doctest: +NOTEST; docs/hailpedia/table.rst:105: >>> ht.ID # doctest: +NOTEST; experimental/import_gtf.py:56: >>> ht.describe() # doctest: +NOTEST; expr/aggregators/aggregators.py:365: >>> table1.aggregate(agg.counter(table1.S",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-451506878
https://github.com/hail-is/hail/issues/4817#issuecomment-506359198:9048,Usability,guid,guides,9048,"python/hail/expr/expressions/typed_expressions.py:934: >>> hl.eval(s1.add(10)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1296: >>> hl.eval(d.key_set()) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1312: >>> hl.eval(d.keys()) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1329: >>> hl.eval(d.map_values(lambda x: x * 10)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1366: >>> hl.eval(d.values()) # doctest: +NOTEST; Binary file /Users/dking/projects/hail/hail/python/hail/expr/expressions/__pycache__/typed_expressions.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/table.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/matrixtable.cpython-37.pyc matches; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:21: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:24: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:27: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:30: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:42: >>> hl.eval(hl.array([a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-506359198
https://github.com/hail-is/hail/issues/4817#issuecomment-506359198:9157,Usability,guid,guides,9157,"Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1296: >>> hl.eval(d.key_set()) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1312: >>> hl.eval(d.keys()) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1329: >>> hl.eval(d.map_values(lambda x: x * 10)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1366: >>> hl.eval(d.values()) # doctest: +NOTEST; Binary file /Users/dking/projects/hail/hail/python/hail/expr/expressions/__pycache__/typed_expressions.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/table.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/matrixtable.cpython-37.pyc matches; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:21: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:24: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:27: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:30: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:42: >>> hl.eval(hl.array([a, b, c])) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-506359198
https://github.com/hail-is/hail/issues/4817#issuecomment-506359198:9267,Usability,guid,guides,9267,") # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1312: >>> hl.eval(d.keys()) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1329: >>> hl.eval(d.map_values(lambda x: x * 10)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1366: >>> hl.eval(d.values()) # doctest: +NOTEST; Binary file /Users/dking/projects/hail/hail/python/hail/expr/expressions/__pycache__/typed_expressions.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/table.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/matrixtable.cpython-37.pyc matches; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:21: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:24: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:27: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:30: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:42: >>> hl.eval(hl.array([a, b, c])) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:50: >>> table.show() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-506359198
https://github.com/hail-is/hail/pull/4819#issuecomment-440772136:10,Availability,error,error,10,should we error or return NaN here? I'm not sure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4819#issuecomment-440772136
https://github.com/hail-is/hail/pull/4823#issuecomment-440801981:164,Testability,log,log,164,"@tpoterba Next step for me is to get hail build jobs into a batch dag format, teach CI to read such a format, and restructure our builds to use this. Then all this log stuff will be much more straightforward.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4823#issuecomment-440801981
https://github.com/hail-is/hail/pull/4824#issuecomment-441771321:94,Usability,feedback,feedback,94,"That's what I expected. Your name just came up in the roulette. I'll also want @cseed to give feedback, since I made these from his original work on https://github.com/cseed/hail/tree/partitioned-combiner.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4824#issuecomment-441771321
https://github.com/hail-is/hail/pull/4824#issuecomment-442568586:71,Deployability,patch,patched,71,"Yep, it's exactly the lack of filtering based on the exact interval. I patched htsjdk to not do the `getIntv` check and got identical results. To what I see for some of the queries of my reader.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4824#issuecomment-442568586
https://github.com/hail-is/hail/pull/4836#issuecomment-444967849:54,Testability,test,test,54,Don't worry about approving this yet. I still need to test it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4836#issuecomment-444967849
https://github.com/hail-is/hail/pull/4836#issuecomment-445967344:17,Availability,ping,ping,17,@patrick-schultz ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4836#issuecomment-445967344
https://github.com/hail-is/hail/pull/4838#issuecomment-442224856:6,Testability,test,test,6,added test and parse rule!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4838#issuecomment-442224856
https://github.com/hail-is/hail/issues/4844#issuecomment-473490897:407,Integrability,depend,dependent,407,"@cseed I've fixed string slicing to do the thing you describe (converting to utf16, slicing, converting back). The Java default is to replace invalid codepoints (e.g. if only one codepoint from a two-codepoint character is kept) with the replacement character \ufffd, but when converted back to UTF8 the replacement character is just `?`. I've written a test to reflect this, but this seems pretty encoding dependent and maybe like something we should just keep in mind and pick a consistent solution for.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4844#issuecomment-473490897
https://github.com/hail-is/hail/issues/4844#issuecomment-473490897:354,Testability,test,test,354,"@cseed I've fixed string slicing to do the thing you describe (converting to utf16, slicing, converting back). The Java default is to replace invalid codepoints (e.g. if only one codepoint from a two-codepoint character is kept) with the replacement character \ufffd, but when converted back to UTF8 the replacement character is just `?`. I've written a test to reflect this, but this seems pretty encoding dependent and maybe like something we should just keep in mind and pick a consistent solution for.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4844#issuecomment-473490897
https://github.com/hail-is/hail/pull/4846#issuecomment-442536971:17,Testability,test,test,17,I should add the test first!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4846#issuecomment-442536971
https://github.com/hail-is/hail/pull/4859#issuecomment-443817760:257,Integrability,wrap,wrapper,257,"currently, all the c++ code is acquiring things called ""ScalaRegions"" to minimize the amount of changes in this PR. One of the next steps will be to have c++ code rely on the underlying c++ region instead of the ""ScalaRegion"" which is intended to just be a wrapper for the Region object in Scala.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4859#issuecomment-443817760
https://github.com/hail-is/hail/pull/4863#issuecomment-443234023:48,Deployability,deploy,deploying,48,"Let's not approve this right now. I don't think deploying will make a difference, but I'd rather not risk it while the BroadE is happening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023
https://github.com/hail-is/hail/pull/4863#issuecomment-443234023:101,Safety,risk,risk,101,"Let's not approve this right now. I don't think deploying will make a difference, but I'd rather not risk it while the BroadE is happening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023
https://github.com/hail-is/hail/pull/4865#issuecomment-443314418:7,Testability,test,tests,7,Python tests catch the bug.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4865#issuecomment-443314418
https://github.com/hail-is/hail/pull/4871#issuecomment-443783236:31,Deployability,patch,patch,31,(would like to include that in patch notes),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4871#issuecomment-443783236
https://github.com/hail-is/hail/pull/4871#issuecomment-443787719:14,Modifiability,Extend,ExtendedOrdering,14,"But only when ExtendedOrdering is used, which I think is pretty rare, now, just in `Interpret` (so eval, top-level expressions) and aggregators.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4871#issuecomment-443787719
https://github.com/hail-is/hail/pull/4877#issuecomment-443822085:197,Deployability,install,install,197,"I'd prefer a solution that addresses https://github.com/hail-is/hail/issues/4875. I'm happy to make the required change, but I feel like this answer will look weird to people who normally use `pip install x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4877#issuecomment-443822085
https://github.com/hail-is/hail/pull/4882#issuecomment-444322864:32,Performance,Load,Load,32,"I renamed the array impls Array{Load, Addr}Impl.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4882#issuecomment-444322864
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:74,Availability,error,error,74,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:411,Integrability,Wrap,WrappedArray,411,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:919,Integrability,Wrap,WrappedArray,919,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:1381,Security,Hash,HashMap,1381,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:1395,Security,Hash,HashMap,1395,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:4,Testability,test,test,4,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:10,Testability,test,testMatrixUnionRowsMemo,10,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:1879,Testability,test,testMatrixUnionRowsMemo,1879,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952
https://github.com/hail-is/hail/pull/4891#issuecomment-444528789:49,Testability,test,tests,49,I'm not in an environment where I can easily run tests right now. Let's see what CI has to say about these changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444528789
https://github.com/hail-is/hail/pull/4895#issuecomment-444648897:252,Integrability,interface,interface,252,"At first glance, the tests look fine to me. More thorough tests could be done using the data files here; https://github.com/broadinstitute/picard/blob/e0bb690d57f73fd2495fc5a77b497e9696f51f81/src/test/java/picard/util/LiftoverVcfTest.java#L65-L99. The interface also looks fine in terms of a non-breaking way to add strand info.; Is the plan to use this to implement a hl.liftover(mt) function that includes the checks from https://github.com/broadinstitute/picard/blob/master/src/main/java/picard/vcf/LiftoverVcf.java#L350-L397 ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-444648897
https://github.com/hail-is/hail/pull/4895#issuecomment-444648897:21,Testability,test,tests,21,"At first glance, the tests look fine to me. More thorough tests could be done using the data files here; https://github.com/broadinstitute/picard/blob/e0bb690d57f73fd2495fc5a77b497e9696f51f81/src/test/java/picard/util/LiftoverVcfTest.java#L65-L99. The interface also looks fine in terms of a non-breaking way to add strand info.; Is the plan to use this to implement a hl.liftover(mt) function that includes the checks from https://github.com/broadinstitute/picard/blob/master/src/main/java/picard/vcf/LiftoverVcf.java#L350-L397 ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-444648897
https://github.com/hail-is/hail/pull/4895#issuecomment-444648897:58,Testability,test,tests,58,"At first glance, the tests look fine to me. More thorough tests could be done using the data files here; https://github.com/broadinstitute/picard/blob/e0bb690d57f73fd2495fc5a77b497e9696f51f81/src/test/java/picard/util/LiftoverVcfTest.java#L65-L99. The interface also looks fine in terms of a non-breaking way to add strand info.; Is the plan to use this to implement a hl.liftover(mt) function that includes the checks from https://github.com/broadinstitute/picard/blob/master/src/main/java/picard/vcf/LiftoverVcf.java#L350-L397 ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-444648897
https://github.com/hail-is/hail/pull/4895#issuecomment-444648897:196,Testability,test,test,196,"At first glance, the tests look fine to me. More thorough tests could be done using the data files here; https://github.com/broadinstitute/picard/blob/e0bb690d57f73fd2495fc5a77b497e9696f51f81/src/test/java/picard/util/LiftoverVcfTest.java#L65-L99. The interface also looks fine in terms of a non-breaking way to add strand info.; Is the plan to use this to implement a hl.liftover(mt) function that includes the checks from https://github.com/broadinstitute/picard/blob/master/src/main/java/picard/vcf/LiftoverVcf.java#L350-L397 ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-444648897
https://github.com/hail-is/hail/pull/4895#issuecomment-444706692:187,Safety,avoid,avoid,187,"oh, interesting.. I thought liftover processed each variant independently. Just out of curiosity, what's the purpose of the special while-loop infrastructure?; and will it be possible to avoid shuffling when sorting lifted-over variants?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-444706692
https://github.com/hail-is/hail/pull/4895#issuecomment-445015697:23,Testability,test,tests,23,"I looked at the picard tests and realized they just made a fake chain file with one interval that mapped to the negative strand in the destination reference. My existing test covers this scenario -- one interval maps to the positive strand of the destination and the other to the negative strand. Since I'm only passing the strand information from Picard, I'm happy with the tests I have already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-445015697
https://github.com/hail-is/hail/pull/4895#issuecomment-445015697:170,Testability,test,test,170,"I looked at the picard tests and realized they just made a fake chain file with one interval that mapped to the negative strand in the destination reference. My existing test covers this scenario -- one interval maps to the positive strand of the destination and the other to the negative strand. Since I'm only passing the strand information from Picard, I'm happy with the tests I have already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-445015697
https://github.com/hail-is/hail/pull/4895#issuecomment-445015697:375,Testability,test,tests,375,"I looked at the picard tests and realized they just made a fake chain file with one interval that mapped to the negative strand in the destination reference. My existing test covers this scenario -- one interval maps to the positive strand of the destination and the other to the negative strand. Since I'm only passing the strand information from Picard, I'm happy with the tests I have already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-445015697
https://github.com/hail-is/hail/issues/4896#issuecomment-444556003:28,Deployability,install,installed,28,"This failed with OpenJDK 11 installed. I don't know Scala, but my read of your checker is that it's looking for older versions, not newer ones. The docs explicitly say java 8 is required.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444556003
https://github.com/hail-is/hail/issues/4896#issuecomment-444557619:103,Deployability,update,update,103,@tpoterba We're stuck with 8 [due to spark](https://issues.igniterealtime.org/browse/SPARK-2017). I'll update the check. Thanks for the report @dekinsitro,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444557619
https://github.com/hail-is/hail/issues/4896#issuecomment-444558390:78,Deployability,install,installed,78,I vaguely remember things working on Java 9 when someone joined the group and installed that. Am I imagining that?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444558390
https://github.com/hail-is/hail/pull/4921#issuecomment-445271169:22,Deployability,deploy,deploy,22,"and yes, will tag and deploy to PyPI after it merges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445271169
https://github.com/hail-is/hail/pull/4921#issuecomment-445296624:43,Integrability,protocol,protocol,43,"Oh, shit, I approved. Do we have a working protocol for multi-user reviews? This was clearly not it. I guess the rule should be to dismiss your review if someone else has reviewed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445296624
https://github.com/hail-is/hail/pull/4921#issuecomment-445296624:85,Usability,clear,clearly,85,"Oh, shit, I approved. Do we have a working protocol for multi-user reviews? This was clearly not it. I guess the rule should be to dismiss your review if someone else has reviewed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445296624
https://github.com/hail-is/hail/pull/4921#issuecomment-445296790:4,Integrability,protocol,protocol,4,"Our protocol is that the assigned user is the only one who can approve, but that anyone else can block.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445296790
https://github.com/hail-is/hail/issues/4922#issuecomment-570346582:80,Testability,test,tests,80,"I'm taking some old tickets from Jon. For this, we just want to combine the two tests into one test method instead of two?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4922#issuecomment-570346582
https://github.com/hail-is/hail/issues/4922#issuecomment-570346582:95,Testability,test,test,95,"I'm taking some old tickets from Jon. For this, we just want to combine the two tests into one test method instead of two?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4922#issuecomment-570346582
https://github.com/hail-is/hail/pull/4924#issuecomment-446410064:0,Availability,ping,ping,0,ping @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-446410064
https://github.com/hail-is/hail/pull/4924#issuecomment-447199915:96,Performance,queue,queue,96,A root file was modified so it has to test every piece. One of its batch jobs gets stuck in the queue and triggers a timeout. Hopefully this passes since there's no traffic right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-447199915
https://github.com/hail-is/hail/pull/4924#issuecomment-447199915:117,Safety,timeout,timeout,117,A root file was modified so it has to test every piece. One of its batch jobs gets stuck in the queue and triggers a timeout. Hopefully this passes since there's no traffic right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-447199915
https://github.com/hail-is/hail/pull/4924#issuecomment-447199915:38,Testability,test,test,38,A root file was modified so it has to test every piece. One of its batch jobs gets stuck in the queue and triggers a timeout. Hopefully this passes since there's no traffic right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-447199915
https://github.com/hail-is/hail/pull/4924#issuecomment-447375781:27,Testability,test,tests,27,"something is wrong with CI tests, I'll look into it as soon as I can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-447375781
https://github.com/hail-is/hail/pull/4930#issuecomment-446008184:121,Deployability,deploy,deploy,121,"I had to make a few changes to CI to make it test against a local version of the latest batch. Long term, we'll actually deploy batch and CI into a fresh namespace to do testing. For now, we explicitly start batch in the test container for CI to test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184
https://github.com/hail-is/hail/pull/4930#issuecomment-446008184:45,Testability,test,test,45,"I had to make a few changes to CI to make it test against a local version of the latest batch. Long term, we'll actually deploy batch and CI into a fresh namespace to do testing. For now, we explicitly start batch in the test container for CI to test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184
https://github.com/hail-is/hail/pull/4930#issuecomment-446008184:170,Testability,test,testing,170,"I had to make a few changes to CI to make it test against a local version of the latest batch. Long term, we'll actually deploy batch and CI into a fresh namespace to do testing. For now, we explicitly start batch in the test container for CI to test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184
https://github.com/hail-is/hail/pull/4930#issuecomment-446008184:221,Testability,test,test,221,"I had to make a few changes to CI to make it test against a local version of the latest batch. Long term, we'll actually deploy batch and CI into a fresh namespace to do testing. For now, we explicitly start batch in the test container for CI to test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184
https://github.com/hail-is/hail/pull/4930#issuecomment-446008184:246,Testability,test,test,246,"I had to make a few changes to CI to make it test against a local version of the latest batch. Long term, we'll actually deploy batch and CI into a fresh namespace to do testing. For now, we explicitly start batch in the test container for CI to test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184
https://github.com/hail-is/hail/pull/4931#issuecomment-446443905:553,Performance,load,loading,553,"Example of tooling/debug (Firefox, built in to Chrome as well). <img width=""1308"" alt=""screen shot 2018-12-11 at 9 57 24 pm"" src=""https://user-images.githubusercontent.com/5543229/49844151-c0b46a80-fd8f-11e8-8ed5-de0d8293d538.png"">. Hidden cost of isomorphic SSR in React: small bundle hit: currently sends serialized state with initial / first page render. In case of scorecard, costs a few KB for the full /json response. No such cost incurred from client-side requests, but if the page that fetches those resources is the first-hit page, would incur loading indicator (on any other page could be prefetched).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-446443905
https://github.com/hail-is/hail/pull/4931#issuecomment-446604474:241,Deployability,install,installing,241,"> should the `haas/packages` folder be checked in?. I don't quite understand. haas is a monorepo containing all the bits that will make up the web/mobile/desktop/auth api and any other services that allow users to interact with Hail without installing it themselves. . https://danluu.com/monorepo/. Everything in packages is a separate package (web, api1), separately publishable to npm, but which allows dependencies to be shared. Does that help?. Edit: wrong link",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-446604474
https://github.com/hail-is/hail/pull/4931#issuecomment-446604474:405,Integrability,depend,dependencies,405,"> should the `haas/packages` folder be checked in?. I don't quite understand. haas is a monorepo containing all the bits that will make up the web/mobile/desktop/auth api and any other services that allow users to interact with Hail without installing it themselves. . https://danluu.com/monorepo/. Everything in packages is a separate package (web, api1), separately publishable to npm, but which allows dependencies to be shared. Does that help?. Edit: wrong link",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-446604474
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:463,Availability,avail,available,463,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:323,Integrability,rout,route-matching,323,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:171,Performance,perform,performance,171,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:1037,Performance,cache,cache,1037,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:355,Safety,avoid,avoid,355,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:956,Security,access,access,956,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:616,Testability,benchmark,benchmarker,616,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:753,Usability,usab,usable,753,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:1012,Energy Efficiency,efficient,efficient,1012,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:18,Integrability,depend,dependencies,18,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:315,Performance,optimiz,optimizations,315,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:425,Performance,optimiz,optimizations,425,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:580,Performance,concurren,concurrent,580,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:707,Safety,avoid,avoiding,707,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:1365,Security,authenticat,authentication,1365,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:1380,Testability,log,logic,1380,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:1587,Testability,log,logic,1587,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:1194,Usability,simpl,simplifies,1194,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:568,Availability,down,down,568,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:946,Availability,error,error,946,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:973,Availability,error,error,973,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:980,Availability,ERROR,ERROR,980,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:205,Integrability,depend,dependency,205,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:715,Integrability,Rout,Router,715,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:733,Integrability,rout,router,733,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:1011,Integrability,Rout,Router,1011,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:833,Modifiability,extend,extends,833,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:633,Performance,Load,Loading,633,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:1088,Performance,Load,Loading,1088,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:308,Usability,simpl,simplify,308,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:656,Availability,error,error,656,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:1517,Availability,down,down,1517," simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipped) by replacing the dark mode icon svg with a reference to the material-design-icons font.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:750,Deployability,integrat,integration,750,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:750,Integrability,integrat,integration,750,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:111,Performance,cache,cache,111,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:161,Performance,cache,cache,161,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:227,Performance,cache,cache,227,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:874,Performance,load,loading,874,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:100,Security,hash,hash-based,100,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:488,Security,validat,validation,488,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:605,Security,validat,validation,605,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:714,Security,validat,validated,714,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:762,Testability,test,tests,762,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:1539,Testability,log,logged,1539," simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipped) by replacing the dark mode icon svg with a reference to the material-design-icons font.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:1560,Testability,log,logged,1560," simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipped) by replacing the dark mode icon svg with a reference to the material-design-icons font.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:93,Usability,simpl,simple,93,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:537,Usability,simpl,simple,537,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:1593,Usability,simpl,simple,1593," simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipped) by replacing the dark mode icon svg with a reference to the material-design-icons font.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
https://github.com/hail-is/hail/pull/4931#issuecomment-450537877:12,Usability,UX,UX,12,"Research on UX/UI, and impact on customer acceptance. Is there a reason to invest in surface credibility (beyond functionality)?. 1) http://credibility.stanford.edu/pdf/p80-fogg.pdf",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-450537877
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3793,Availability,down,downward,3793," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:292,Deployability,install,install,292,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:533,Integrability,rout,routing,533,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:694,Integrability,depend,dependencies,694,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1661,Integrability,depend,dependencies,1661," a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:2362,Integrability,interface,interface,2362,"tic/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3337,Integrability,Rout,Routes,3337," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3443,Integrability,wrap,wrapped,3443," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1987,Modifiability,variab,variables,1987,"B .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:776,Performance,optimiz,optimizations,776,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1726,Performance,load,loaded,1726,"d. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_UR",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1774,Performance,load,load,1774,"d. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_UR",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3612,Performance,Perform,Performance,3612," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3759,Performance,perform,performant,3759," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:4044,Performance,load,load,4044," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:36,Testability,test,testing,36,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:107,Testability,test,tested,107,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:640,Testability,test,test,640,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:2262,Testability,log,log,2262,"*/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=htt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:2410,Testability,log,log,2410,".js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:274,Usability,simpl,simply,274,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:2378,Usability,clear,clear,2378,".js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3301,Usability,simpl,simple,3301," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
https://github.com/hail-is/hail/pull/4931#issuecomment-454272121:55,Deployability,deploy,deployed,55,"Oh, and the app is meant to operate behind HTTPS; when deployed, running the web app with ; `npm run start` instead of `npm run prod-test` will enable secureOnly cookies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121
https://github.com/hail-is/hail/pull/4931#issuecomment-454272121:151,Security,secur,secureOnly,151,"Oh, and the app is meant to operate behind HTTPS; when deployed, running the web app with ; `npm run start` instead of `npm run prod-test` will enable secureOnly cookies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121
https://github.com/hail-is/hail/pull/4931#issuecomment-454272121:133,Testability,test,test,133,"Oh, and the app is meant to operate behind HTTPS; when deployed, running the web app with ; `npm run start` instead of `npm run prod-test` will enable secureOnly cookies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121
https://github.com/hail-is/hail/pull/4931#issuecomment-454272823:141,Deployability,install,installs,141,"Lastly, a bunch of LOC come from the lock files. Those can be ignored for review purposes; they just maintain versioning information, ensure installs are consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272823
https://github.com/hail-is/hail/pull/4931#issuecomment-454606562:104,Energy Efficiency,reduce,reduced,104,"@danking Set auth0 callback based on window (this turns out to be cleanest), cleaned up styles, further reduced bundle size a bit by removing use of a state management tool (that wraps any arbitrary object in an observable that can be watched), and added a basic header menu to allow logout. I think the most challenging part of using this web architecture will be managing actions on server vs browser. That is probably the only piece that isn't obvious. I added a few comments that may help; namely _app.js 's constructor runs before everything else (as it wraps all other components), but lifecycle functions (componentDidMount) run from the inner child out to the parent. Constructor runs both on server and client (since the class contains the needed functions that are translated into HTML). getInitialProps is the only lifecycle event that runs on both server and client. All other events are client only. To require something to only run on the server or browser, from getInitialProps, or constructor, check for (typeof window === 'undefined'). Not incredibly elegant, but not terribly problematic either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562
https://github.com/hail-is/hail/pull/4931#issuecomment-454606562:179,Integrability,wrap,wraps,179,"@danking Set auth0 callback based on window (this turns out to be cleanest), cleaned up styles, further reduced bundle size a bit by removing use of a state management tool (that wraps any arbitrary object in an observable that can be watched), and added a basic header menu to allow logout. I think the most challenging part of using this web architecture will be managing actions on server vs browser. That is probably the only piece that isn't obvious. I added a few comments that may help; namely _app.js 's constructor runs before everything else (as it wraps all other components), but lifecycle functions (componentDidMount) run from the inner child out to the parent. Constructor runs both on server and client (since the class contains the needed functions that are translated into HTML). getInitialProps is the only lifecycle event that runs on both server and client. All other events are client only. To require something to only run on the server or browser, from getInitialProps, or constructor, check for (typeof window === 'undefined'). Not incredibly elegant, but not terribly problematic either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562
https://github.com/hail-is/hail/pull/4931#issuecomment-454606562:559,Integrability,wrap,wraps,559,"@danking Set auth0 callback based on window (this turns out to be cleanest), cleaned up styles, further reduced bundle size a bit by removing use of a state management tool (that wraps any arbitrary object in an observable that can be watched), and added a basic header menu to allow logout. I think the most challenging part of using this web architecture will be managing actions on server vs browser. That is probably the only piece that isn't obvious. I added a few comments that may help; namely _app.js 's constructor runs before everything else (as it wraps all other components), but lifecycle functions (componentDidMount) run from the inner child out to the parent. Constructor runs both on server and client (since the class contains the needed functions that are translated into HTML). getInitialProps is the only lifecycle event that runs on both server and client. All other events are client only. To require something to only run on the server or browser, from getInitialProps, or constructor, check for (typeof window === 'undefined'). Not incredibly elegant, but not terribly problematic either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562
https://github.com/hail-is/hail/pull/4931#issuecomment-454606562:284,Testability,log,logout,284,"@danking Set auth0 callback based on window (this turns out to be cleanest), cleaned up styles, further reduced bundle size a bit by removing use of a state management tool (that wraps any arbitrary object in an observable that can be watched), and added a basic header menu to allow logout. I think the most challenging part of using this web architecture will be managing actions on server vs browser. That is probably the only piece that isn't obvious. I added a few comments that may help; namely _app.js 's constructor runs before everything else (as it wraps all other components), but lifecycle functions (componentDidMount) run from the inner child out to the parent. Constructor runs both on server and client (since the class contains the needed functions that are translated into HTML). getInitialProps is the only lifecycle event that runs on both server and client. All other events are client only. To require something to only run on the server or browser, from getInitialProps, or constructor, check for (typeof window === 'undefined'). Not incredibly elegant, but not terribly problematic either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562
https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:499,Performance,perform,performance-tests,499,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:577,Performance,perform,performance-challenge-with-famous-front-end-framework-,577,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:912,Performance,optimiz,optimized,912,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:70,Testability,log,logged,70,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:94,Testability,Log,Logged,94,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:511,Testability,test,tests,511,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:269,Usability,guid,guidance,269,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
https://github.com/hail-is/hail/pull/4931#issuecomment-454796376:83,Integrability,rout,routing,83,"Regarding SSR-only mode. This is the default behavior. SSR is mostly a function of routing. If we allow the client to handle routes, we save the roundtrip in reconciling current app state (current DOM) with the next state (next page's DOM). To ""enable"" this functionality, instead of using `<Link>` use `<a>`. Nextjs has excellent documentation and a responsive maintainer base: https://github.com/zeit/next.js/issues/575",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454796376
https://github.com/hail-is/hail/pull/4931#issuecomment-454796376:125,Integrability,rout,routes,125,"Regarding SSR-only mode. This is the default behavior. SSR is mostly a function of routing. If we allow the client to handle routes, we save the roundtrip in reconciling current app state (current DOM) with the next state (next page's DOM). To ""enable"" this functionality, instead of using `<Link>` use `<a>`. Nextjs has excellent documentation and a responsive maintainer base: https://github.com/zeit/next.js/issues/575",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454796376
https://github.com/hail-is/hail/pull/4931#issuecomment-454796376:351,Usability,responsiv,responsive,351,"Regarding SSR-only mode. This is the default behavior. SSR is mostly a function of routing. If we allow the client to handle routes, we save the roundtrip in reconciling current app state (current DOM) with the next state (next page's DOM). To ""enable"" this functionality, instead of using `<Link>` use `<a>`. Nextjs has excellent documentation and a responsive maintainer base: https://github.com/zeit/next.js/issues/575",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454796376
https://github.com/hail-is/hail/pull/4937#issuecomment-446733609:665,Availability,redundant,redundant,665,"This is really great. . I have some thoughts below, mostly brain storming. Don't take any of it too seriously. Some thoughts:. 1. I thought you wanted to support f-strings. By making the batch file quote double curly parens, that means if you use them in an f-string, you need to write `f'{{{{foo}}}}'` which is a bit much. But that means that non-batch uses of `{}` need to be double-quoted, so `awk '{{ ... }}'` and `f""awk '{{{{ ... }}}}'""`. Hmm. Maybe using the same escape syntax as f-strings is not ideal. . I don't have a no-brainer suggestion. Happy to brainstorm ideas offline. I think ultimately this is a minor syntactic choice. 2. Inputs seem ... almost redundant, because they also appear in the command strings. What about:. ```; .command('shapeit --bed-file {{<subset.ofile}} --chr ' + contig + ' --out {{>ofile}}'); ```. Then the question becomes, how do associate `subset` with the corresponding Python variable? You could use the task label, but then the user has to maintain two sets of names, which isn't ideal. Hmm, maybe this doesn't work. 3. I like arrays of resources!. > `.command('cat {{files}} >> {{ofile}}')`. I wonder, will we ever want arrays to be formatted other than joined with spaces? I worry the user will want more flexibility in formatting, and we'll want that in Python. What about if the argument is a function, it takes a dictionary from resource names to their string representation, and you can format however you want? Then you could write the last command as:. ```; .command(lambda rs: f'cat {' '.join(rs['files'])} >> {rs['ofile']}'); ```. 4. I was confused by this:. > `p.write_output(merger.ofile + "".haps"", ...)`. What's the left hand side? Why isn't this just `merger.ofile`?. This suggests another issue: what if you want to use `ofile` in a plink command, but plink outputs some files with various extensions with `ofile` as the base? We might need an `outputs` that lists (docker local) output files based on a base path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609
https://github.com/hail-is/hail/pull/4937#issuecomment-446733609:919,Modifiability,variab,variable,919,"This is really great. . I have some thoughts below, mostly brain storming. Don't take any of it too seriously. Some thoughts:. 1. I thought you wanted to support f-strings. By making the batch file quote double curly parens, that means if you use them in an f-string, you need to write `f'{{{{foo}}}}'` which is a bit much. But that means that non-batch uses of `{}` need to be double-quoted, so `awk '{{ ... }}'` and `f""awk '{{{{ ... }}}}'""`. Hmm. Maybe using the same escape syntax as f-strings is not ideal. . I don't have a no-brainer suggestion. Happy to brainstorm ideas offline. I think ultimately this is a minor syntactic choice. 2. Inputs seem ... almost redundant, because they also appear in the command strings. What about:. ```; .command('shapeit --bed-file {{<subset.ofile}} --chr ' + contig + ' --out {{>ofile}}'); ```. Then the question becomes, how do associate `subset` with the corresponding Python variable? You could use the task label, but then the user has to maintain two sets of names, which isn't ideal. Hmm, maybe this doesn't work. 3. I like arrays of resources!. > `.command('cat {{files}} >> {{ofile}}')`. I wonder, will we ever want arrays to be formatted other than joined with spaces? I worry the user will want more flexibility in formatting, and we'll want that in Python. What about if the argument is a function, it takes a dictionary from resource names to their string representation, and you can format however you want? Then you could write the last command as:. ```; .command(lambda rs: f'cat {' '.join(rs['files'])} >> {rs['ofile']}'); ```. 4. I was confused by this:. > `p.write_output(merger.ofile + "".haps"", ...)`. What's the left hand side? Why isn't this just `merger.ofile`?. This suggests another issue: what if you want to use `ofile` in a plink command, but plink outputs some files with various extensions with `ofile` as the base? We might need an `outputs` that lists (docker local) output files based on a base path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609
https://github.com/hail-is/hail/pull/4937#issuecomment-446733609:665,Safety,redund,redundant,665,"This is really great. . I have some thoughts below, mostly brain storming. Don't take any of it too seriously. Some thoughts:. 1. I thought you wanted to support f-strings. By making the batch file quote double curly parens, that means if you use them in an f-string, you need to write `f'{{{{foo}}}}'` which is a bit much. But that means that non-batch uses of `{}` need to be double-quoted, so `awk '{{ ... }}'` and `f""awk '{{{{ ... }}}}'""`. Hmm. Maybe using the same escape syntax as f-strings is not ideal. . I don't have a no-brainer suggestion. Happy to brainstorm ideas offline. I think ultimately this is a minor syntactic choice. 2. Inputs seem ... almost redundant, because they also appear in the command strings. What about:. ```; .command('shapeit --bed-file {{<subset.ofile}} --chr ' + contig + ' --out {{>ofile}}'); ```. Then the question becomes, how do associate `subset` with the corresponding Python variable? You could use the task label, but then the user has to maintain two sets of names, which isn't ideal. Hmm, maybe this doesn't work. 3. I like arrays of resources!. > `.command('cat {{files}} >> {{ofile}}')`. I wonder, will we ever want arrays to be formatted other than joined with spaces? I worry the user will want more flexibility in formatting, and we'll want that in Python. What about if the argument is a function, it takes a dictionary from resource names to their string representation, and you can format however you want? Then you could write the last command as:. ```; .command(lambda rs: f'cat {' '.join(rs['files'])} >> {rs['ofile']}'); ```. 4. I was confused by this:. > `p.write_output(merger.ofile + "".haps"", ...)`. What's the left hand side? Why isn't this just `merger.ofile`?. This suggests another issue: what if you want to use `ofile` in a plink command, but plink outputs some files with various extensions with `ofile` as the base? We might need an `outputs` that lists (docker local) output files based on a base path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609
https://github.com/hail-is/hail/pull/4937#issuecomment-446742931:117,Modifiability,variab,variable,117,"All of these things I completely agree with!. 1. I was lazy and used the Jinja template engine to parse and find the variable declarations. I need to write a custom parser, but wanted to figure out exactly what we're going to support. Which makes me worried that I don't want to implement an expr language or it should be minimal. 2. Tim suggested something similar: `%%IN bfile%%` and `%%OUT ofile%%. Requires the custom parser. See comment 1 above. 3. I was also concerned about the formatting of arrays. I tried using lambdas for comment 4 and it got complicated. I like your proposal but want to think about it more. 4. PLINK, etc. output lots of files and you specify the file root and then it outputs a bunch of files with different extensions. We must be able to support this and make it easy for users. I agree with your suggestion. I'll try that in the example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446742931
https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:917,Deployability,Pipeline,Pipeline,917,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039
https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:931,Deployability,Pipeline,Pipeline,931,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039
https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:378,Integrability,depend,dependencies,378,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039
https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:625,Integrability,depend,dependencies,625,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039
https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:530,Modifiability,variab,variables,530,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039
https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:519,Safety,detect,detect,519,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039
https://github.com/hail-is/hail/pull/4937#issuecomment-451295662:1095,Deployability,Pipeline,Pipeline,1095,"oblems outlined above. The tradeoff made is that we have to refer to the object (ex: `subset`). So instead of thinking of writing the command as a template where `inputs` specifies how to substitute into the template, writing commands in this interface is the same as using Python to generate the correct string to output. I'm not sure that I like this better. One of the advantages of writing commands as templates is they are reusable. In the latter case, the strings are reusable if they are written as `.format()` templates instead of `f-strings`. So maybe it's approximately the same, but there's an extra step to define the inputs to `format`. I tried hacking the Python AST to not have to refer to the object, but I think it's going to be difficult to get the AST parsing exactly right and not have too many implicit rules within our language. I also considered writing a DSL, but found that it's hard to specify the part with the `shapeit_output` in a DSL. ```python3; from pyapi import Pipeline, resource_group; p = Pipeline(). input_bfile = p.new_resource_group(bed=""gs://jigold/input_root.bed"",; bim=""gs://jigold/input_root.bim"",; fam=""gs://jigold/input_root.fam""). def bfile(root):; return resource_group(root, lambda x: {""bed"": x + "".bed"", ""bim"": x + "".bim"", ""fam"": x + "".fam""}). subset = (p.new_task(); .label('subset')); subset = subset; .command(f'plink --bfile {input_bfile} --make-bed {bfile(subset.tmp1)}'); .command(""awk '{print $1, $2}'"" +; subset.tmp1.fam +; "" | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > "" +; subset.tmp2); .command(f""plink --bfile {input_bfile} --remove {subset.tmp2} --make-bed {bfile(subset.ofile)}"")). def shapeit_output(root):; return resource_group(root, lambda x: {""haps"": x + "".haps"", ""log"": x + "".log""}). for contig in [str(x) for x in range(1, 4)]:; shapeit = (p.new_task(); 		.label('shapeit')); shapeit = (shapeit; 		.command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit_output(shapeit.ofile)}')). merger = (p.new_t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662
https://github.com/hail-is/hail/pull/4937#issuecomment-451295662:1125,Deployability,Pipeline,Pipeline,1125,"oblems outlined above. The tradeoff made is that we have to refer to the object (ex: `subset`). So instead of thinking of writing the command as a template where `inputs` specifies how to substitute into the template, writing commands in this interface is the same as using Python to generate the correct string to output. I'm not sure that I like this better. One of the advantages of writing commands as templates is they are reusable. In the latter case, the strings are reusable if they are written as `.format()` templates instead of `f-strings`. So maybe it's approximately the same, but there's an extra step to define the inputs to `format`. I tried hacking the Python AST to not have to refer to the object, but I think it's going to be difficult to get the AST parsing exactly right and not have too many implicit rules within our language. I also considered writing a DSL, but found that it's hard to specify the part with the `shapeit_output` in a DSL. ```python3; from pyapi import Pipeline, resource_group; p = Pipeline(). input_bfile = p.new_resource_group(bed=""gs://jigold/input_root.bed"",; bim=""gs://jigold/input_root.bim"",; fam=""gs://jigold/input_root.fam""). def bfile(root):; return resource_group(root, lambda x: {""bed"": x + "".bed"", ""bim"": x + "".bim"", ""fam"": x + "".fam""}). subset = (p.new_task(); .label('subset')); subset = subset; .command(f'plink --bfile {input_bfile} --make-bed {bfile(subset.tmp1)}'); .command(""awk '{print $1, $2}'"" +; subset.tmp1.fam +; "" | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > "" +; subset.tmp2); .command(f""plink --bfile {input_bfile} --remove {subset.tmp2} --make-bed {bfile(subset.ofile)}"")). def shapeit_output(root):; return resource_group(root, lambda x: {""haps"": x + "".haps"", ""log"": x + "".log""}). for contig in [str(x) for x in range(1, 4)]:; shapeit = (p.new_task(); 		.label('shapeit')); shapeit = (shapeit; 		.command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit_output(shapeit.ofile)}')). merger = (p.new_t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662
https://github.com/hail-is/hail/pull/4937#issuecomment-451295662:343,Integrability,interface,interface,343,"@cseed Sorry if this doesn't make sense -- we can discuss in person. Here's my attempt to fix the problems outlined above. The tradeoff made is that we have to refer to the object (ex: `subset`). So instead of thinking of writing the command as a template where `inputs` specifies how to substitute into the template, writing commands in this interface is the same as using Python to generate the correct string to output. I'm not sure that I like this better. One of the advantages of writing commands as templates is they are reusable. In the latter case, the strings are reusable if they are written as `.format()` templates instead of `f-strings`. So maybe it's approximately the same, but there's an extra step to define the inputs to `format`. I tried hacking the Python AST to not have to refer to the object, but I think it's going to be difficult to get the AST parsing exactly right and not have too many implicit rules within our language. I also considered writing a DSL, but found that it's hard to specify the part with the `shapeit_output` in a DSL. ```python3; from pyapi import Pipeline, resource_group; p = Pipeline(). input_bfile = p.new_resource_group(bed=""gs://jigold/input_root.bed"",; bim=""gs://jigold/input_root.bim"",; fam=""gs://jigold/input_root.fam""). def bfile(root):; return resource_group(root, lambda x: {""bed"": x + "".bed"", ""bim"": x + "".bim"", ""fam"": x + "".fam""}). subset = (p.new_task(); .label('subset')); subset = subset; .command(f'plink --bfile {input_bfile} --make-bed {bfile(subset.tmp1)}'); .command(""awk '{print $1, $2}'"" +; subset.tmp1.fam +; "" | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > "" +; subset.tmp2); .command(f""plink --bfile {input_bfile} --remove {subset.tmp2} --make-bed {bfile(subset.ofile)}"")). def shapeit_output(root):; return resource_group(root, lambda x: {""haps"": x + "".haps"", ""log"": x + "".log""}). for contig in [str(x) for x in range(1, 4)]:; shapeit = (p.new_task(); 		.label('shapeit')); shapeit = (shapeit; 		.command(f'shapeit -",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662
https://github.com/hail-is/hail/pull/4937#issuecomment-451295662:1846,Testability,log,log,1846,"is the same as using Python to generate the correct string to output. I'm not sure that I like this better. One of the advantages of writing commands as templates is they are reusable. In the latter case, the strings are reusable if they are written as `.format()` templates instead of `f-strings`. So maybe it's approximately the same, but there's an extra step to define the inputs to `format`. I tried hacking the Python AST to not have to refer to the object, but I think it's going to be difficult to get the AST parsing exactly right and not have too many implicit rules within our language. I also considered writing a DSL, but found that it's hard to specify the part with the `shapeit_output` in a DSL. ```python3; from pyapi import Pipeline, resource_group; p = Pipeline(). input_bfile = p.new_resource_group(bed=""gs://jigold/input_root.bed"",; bim=""gs://jigold/input_root.bim"",; fam=""gs://jigold/input_root.fam""). def bfile(root):; return resource_group(root, lambda x: {""bed"": x + "".bed"", ""bim"": x + "".bim"", ""fam"": x + "".fam""}). subset = (p.new_task(); .label('subset')); subset = subset; .command(f'plink --bfile {input_bfile} --make-bed {bfile(subset.tmp1)}'); .command(""awk '{print $1, $2}'"" +; subset.tmp1.fam +; "" | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > "" +; subset.tmp2); .command(f""plink --bfile {input_bfile} --remove {subset.tmp2} --make-bed {bfile(subset.ofile)}"")). def shapeit_output(root):; return resource_group(root, lambda x: {""haps"": x + "".haps"", ""log"": x + "".log""}). for contig in [str(x) for x in range(1, 4)]:; shapeit = (p.new_task(); 		.label('shapeit')); shapeit = (shapeit; 		.command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit_output(shapeit.ofile)}')). merger = (p.new_task(); .label('merge')); merger = (merger; .command('cat {files} >> {ofile}'.format(files="" "".join([task.ofile.haps for task in p.select_tasks(""shapeit"")]), ofile=merger.ofile)). p.write_output(merger.ofile, ""gs://jigold/final_output.txt""); p.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662
https://github.com/hail-is/hail/pull/4937#issuecomment-451295662:1858,Testability,log,log,1858,"is the same as using Python to generate the correct string to output. I'm not sure that I like this better. One of the advantages of writing commands as templates is they are reusable. In the latter case, the strings are reusable if they are written as `.format()` templates instead of `f-strings`. So maybe it's approximately the same, but there's an extra step to define the inputs to `format`. I tried hacking the Python AST to not have to refer to the object, but I think it's going to be difficult to get the AST parsing exactly right and not have too many implicit rules within our language. I also considered writing a DSL, but found that it's hard to specify the part with the `shapeit_output` in a DSL. ```python3; from pyapi import Pipeline, resource_group; p = Pipeline(). input_bfile = p.new_resource_group(bed=""gs://jigold/input_root.bed"",; bim=""gs://jigold/input_root.bim"",; fam=""gs://jigold/input_root.fam""). def bfile(root):; return resource_group(root, lambda x: {""bed"": x + "".bed"", ""bim"": x + "".bim"", ""fam"": x + "".fam""}). subset = (p.new_task(); .label('subset')); subset = subset; .command(f'plink --bfile {input_bfile} --make-bed {bfile(subset.tmp1)}'); .command(""awk '{print $1, $2}'"" +; subset.tmp1.fam +; "" | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > "" +; subset.tmp2); .command(f""plink --bfile {input_bfile} --remove {subset.tmp2} --make-bed {bfile(subset.ofile)}"")). def shapeit_output(root):; return resource_group(root, lambda x: {""haps"": x + "".haps"", ""log"": x + "".log""}). for contig in [str(x) for x in range(1, 4)]:; shapeit = (p.new_task(); 		.label('shapeit')); shapeit = (shapeit; 		.command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit_output(shapeit.ofile)}')). merger = (p.new_task(); .label('merge')); merger = (merger; .command('cat {files} >> {ofile}'.format(files="" "".join([task.ofile.haps for task in p.select_tasks(""shapeit"")]), ofile=merger.ofile)). p.write_output(merger.ofile, ""gs://jigold/final_output.txt""); p.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:332,Deployability,Pipeline,Pipeline,332,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:370,Deployability,Pipeline,Pipeline,370,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:396,Deployability,pipeline,pipeline,396,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:2105,Deployability,pipeline,pipeline,2105,"link --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(); ```. ```bash; #! /usr/bash; set -ex. # define tmp directory; __TMP_DIR__=/tmp//pipeline.yG41vqpS/. # __TASK__0 write_input; cp gs://hail-jigold/random_file.txt ${__TMP_DIR__}/rsfKylng. # __TASK__1 write_input; cp gs://hail-jigold/input.bed ${__TMP_DIR__}/xJONBVn7.bed. # __TASK__2 write_input; cp gs://hail-jigold/input.bim ${__TMP_DIR__}/xJONBVn7.bim. # __TASK__3 write_input; cp gs://hail-jigold/input.fam ${__TMP_DIR__}/xJONBVn7.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=${__TMP_DIR__}/xJONBVn7; __RESOURCE_GROUP__1=${__TMP_DIR__}/TB7ZUbj8; __RESOURCE__6=${__TMP_DIR__}/TB7ZUbj8.fam; __RESOURCE__10=${__TMP_DIR__}/EVeRHf7V; __RESOURCE__1=${__TMP_DIR__}/xJONBVn7.bed; __RESOURCE__2=${__TMP_DIR__}/xJONBVn7.bim; __RESOURCE__3=${__TMP_DIR__}/xJONBVn7.fam; __RESOURCE_GROUP__2=${__TMP_DIR__}/MXBQugBx; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESO",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:2203,Deployability,pipeline,pipeline,2203," $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(); ```. ```bash; #! /usr/bash; set -ex. # define tmp directory; __TMP_DIR__=/tmp//pipeline.yG41vqpS/. # __TASK__0 write_input; cp gs://hail-jigold/random_file.txt ${__TMP_DIR__}/rsfKylng. # __TASK__1 write_input; cp gs://hail-jigold/input.bed ${__TMP_DIR__}/xJONBVn7.bed. # __TASK__2 write_input; cp gs://hail-jigold/input.bim ${__TMP_DIR__}/xJONBVn7.bim. # __TASK__3 write_input; cp gs://hail-jigold/input.fam ${__TMP_DIR__}/xJONBVn7.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=${__TMP_DIR__}/xJONBVn7; __RESOURCE_GROUP__1=${__TMP_DIR__}/TB7ZUbj8; __RESOURCE__6=${__TMP_DIR__}/TB7ZUbj8.fam; __RESOURCE__10=${__TMP_DIR__}/EVeRHf7V; __RESOURCE__1=${__TMP_DIR__}/xJONBVn7.bed; __RESOURCE__2=${__TMP_DIR__}/xJONBVn7.bim; __RESOURCE__3=${__TMP_DIR__}/xJONBVn7.fam; __RESOURCE_GROUP__2=${__TMP_DIR__}/MXBQugBx; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:33,Integrability,interface,interface,33,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:160,Testability,test,tests,160,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:627,Testability,log,log,627,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:639,Testability,log,log,639,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741
https://github.com/hail-is/hail/pull/4937#issuecomment-452825568:86,Deployability,pipeline,pipeline,86,@danking suggested we move this to a separate project from `batch`. Possibly call it `pipeline`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452825568
https://github.com/hail-is/hail/pull/4937#issuecomment-452846400:194,Deployability,pipeline,pipeline,194,"This really does look great! I have two small suggestions:. 1. I feel like you should say `read_input` and rather than `write_input`. I'm thinking these commands are from the perspective of the pipeline since they are on Pipeline. 2. Rather than building a group and then declaring it, I think you can do both at once:. ```; # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset.declare_resource_groups(tmp1={bed=""{root}.bed"", bim=""{root}.bim"", fam=""{root}.fam""}, ; ofile={...}); subset = (subset; .label('subset'); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452846400
https://github.com/hail-is/hail/pull/4937#issuecomment-452846400:221,Deployability,Pipeline,Pipeline,221,"This really does look great! I have two small suggestions:. 1. I feel like you should say `read_input` and rather than `write_input`. I'm thinking these commands are from the perspective of the pipeline since they are on Pipeline. 2. Rather than building a group and then declaring it, I think you can do both at once:. ```; # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset.declare_resource_groups(tmp1={bed=""{root}.bed"", bim=""{root}.bim"", fam=""{root}.fam""}, ; ofile={...}); subset = (subset; .label('subset'); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452846400
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:286,Deployability,pipeline,pipeline,286,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:302,Deployability,Pipeline,Pipeline,302,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:316,Deployability,Pipeline,Pipeline,316,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:342,Deployability,pipeline,pipeline,342,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:1992,Deployability,pipeline,pipeline,1992,"-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQihd; __RESOURCE__1=33qZtfwg.bed; __RESOURCE__2=33qZtfwg.bim; __RESOURCE__3=33qZtfwg.fam; __RESOURCE_GROUP__2=YXS0tQKi; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}. # __TASK__5 shapeit; __RESOU",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:2098,Deployability,pipeline,pipeline,2098,"| awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQihd; __RESOURCE__1=33qZtfwg.bed; __RESOURCE__2=33qZtfwg.bim; __RESOURCE__3=33qZtfwg.fam; __RESOURCE_GROUP__2=YXS0tQKi; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}. # __TASK__5 shapeit; __RESOURCE_GROUP__2=YXS0tQKi; __RESOURCE_GROUP__3=gidGmbcC; shapeit --bed-file ${__RESOURCE_GROUP__",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:143,Integrability,interface,interface,143,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:78,Testability,test,tests,78,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:194,Testability,test,tests,194,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:1517,Testability,log,log,1517,"; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:1531,Testability,log,log,1531,"_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQihd; __RESOURCE_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282
https://github.com/hail-is/hail/pull/4937#issuecomment-453621583:200,Modifiability,variab,variables,200,"Other things to add in separate PRs:; - logging; - concept of a ResourceDirectory where you want to copy the files in/out from a directory; - change the temp dir to be per task; - Support environment variables, cpu, memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453621583
https://github.com/hail-is/hail/pull/4937#issuecomment-453621583:40,Testability,log,logging,40,"Other things to add in separate PRs:; - logging; - concept of a ResourceDirectory where you want to copy the files in/out from a directory; - change the temp dir to be per task; - Support environment variables, cpu, memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453621583
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:116,Deployability,pipeline,pipeline,116,"@catoverdrive Here's the output with docker commands:. ```bash; #!/bin/bash. # change cd to tmp directory; cd /tmp//pipeline.S9YTZap5/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:450,Deployability,pipeline,pipeline,450,"@catoverdrive Here's the output with docker commands:. ```bash; #!/bin/bash. # change cd to tmp directory; cd /tmp//pipeline.S9YTZap5/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:475,Deployability,pipeline,pipeline,475,"@catoverdrive Here's the output with docker commands:. ```bash; #!/bin/bash. # change cd to tmp directory; cd /tmp//pipeline.S9YTZap5/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
