id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8369,Modifiability,evolve,evolve,8369,"plying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_m",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:254,Performance,optimiz,optimizations,254,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:364,Performance,optimiz,optimization,364,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:382,Performance,perform,performed,382,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:481,Performance,optimiz,optimizer,481,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:632,Performance,optimiz,optimization,632,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:744,Performance,optimiz,optimizations,744,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1153,Performance,optimiz,optimizer,1153,"==. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <std",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1176,Performance,optimiz,optimizations,1176,"==. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <std",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1258,Performance,optimiz,optimizer,1258,"h can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1360,Performance,optimiz,optimization,1360,"ion when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:2761,Performance,optimiz,optimizer,2761,"`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or lib",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:2839,Performance,optimiz,optimizer,2839," foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:2953,Performance,optimiz,optimizer,2953,"void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3133,Performance,optimiz,optimizer,3133,"e <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super too",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3282,Performance,optimiz,optimizer,3282,"n:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3335,Performance,optimiz,optimizer,3335,"n.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3480,Performance,optimiz,optimizer,3480,"ple, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3606,Performance,optimiz,optimizer,3606,"code file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3839,Performance,optimiz,optimization,3839,"ves ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:4255,Performance,optimiz,optimizer,4255,"r driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; fro",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:4447,Performance,optimiz,optimizer,4447,"nformation collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:5139,Performance,optimiz,optimizer,5139,"is increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:5261,Performance,optimiz,optimizer,5261,"is increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:5367,Performance,optimiz,optimizer,5367,"nker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implemented in a shared object libLTO. This allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the share",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:6381,Performance,load,loaded,6381,"nker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implemented in a shared object libLTO. This allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the shared object is lazily loaded. Phase 2 : Symbol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7172,Performance,optimiz,optimizer,7172,"s allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the shared object is lazily loaded. Phase 2 : Symbol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7335,Performance,optimiz,optimization,7335,"s allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the shared object is lazily loaded. Phase 2 : Symbol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7390,Performance,optimiz,optimization,7390,"ol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7484,Performance,optimiz,optimized,7484,"It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object fil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7877,Performance,perform,performs,7877," Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8208,Performance,optimiz,optimizer,8208,"ators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8346,Performance,optimiz,optimizer,8346,"plying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_m",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:10029,Performance,load,loaded,10029,"`, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_module_dispose(lto_module_t). The linker can introspect the non-native object file by getting the number of; symbols and getting the name and attributes of each symbol via:. .. code-block:: c. lto_module_get_num_symbols(lto_module_t); lto_module_get_symbol_name(lto_module_t, unsigned int); lto_module_get_symbol_attribute(lto_module_t, unsigned int). The attributes of a symbol include the alignment, visibility, and kind. Tools working with object files on Darwin (e.g. lipo) may need to know properties like the CPU type:. .. code-block:: c. lto_module_get_macho_cputype(lto_module_t mod, unsigned int *out_cputype, unsigned int *out_cpusubtype). ``lto_code_gen_t``; ------------------. Once the linker has loaded each non-native object files into an; ``lto_module_t``, it can request ``libLTO`` to process them all and generate a; native object file. This is done in a couple of steps. First, a code generator; is created with:. .. code-block:: c. lto_codegen_create(). Then, each non-native object file is added to the code generator with:. .. code-block:: c. lto_codegen_add_module(lto_code_gen_t, lto_module_t). The linker then has the option of setting some codegen options. Whether or not; to generate DWARF debug info is set with:. .. code-block:: c. lto_codegen_set_debug_model(lto_code_gen_t). which kind of position independence is set with:. .. code-block:: c. lto_codegen_set_pic_model(lto_code_gen_t). And each symbol that is referenced by a native object file or otherwise must not; be optimized away is set with:. .. code-block:: c. lto_codegen_add_must_preserve_symbol(lto_code_gen_t, const char*). After all these settings are done, the linker requests that a native object file; be created from the modules with the settings using:. .. code-block:: c.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:10822,Performance,optimiz,optimized,10822,"-block:: c. lto_module_dispose(lto_module_t). The linker can introspect the non-native object file by getting the number of; symbols and getting the name and attributes of each symbol via:. .. code-block:: c. lto_module_get_num_symbols(lto_module_t); lto_module_get_symbol_name(lto_module_t, unsigned int); lto_module_get_symbol_attribute(lto_module_t, unsigned int). The attributes of a symbol include the alignment, visibility, and kind. Tools working with object files on Darwin (e.g. lipo) may need to know properties like the CPU type:. .. code-block:: c. lto_module_get_macho_cputype(lto_module_t mod, unsigned int *out_cputype, unsigned int *out_cpusubtype). ``lto_code_gen_t``; ------------------. Once the linker has loaded each non-native object files into an; ``lto_module_t``, it can request ``libLTO`` to process them all and generate a; native object file. This is done in a couple of steps. First, a code generator; is created with:. .. code-block:: c. lto_codegen_create(). Then, each non-native object file is added to the code generator with:. .. code-block:: c. lto_codegen_add_module(lto_code_gen_t, lto_module_t). The linker then has the option of setting some codegen options. Whether or not; to generate DWARF debug info is set with:. .. code-block:: c. lto_codegen_set_debug_model(lto_code_gen_t). which kind of position independence is set with:. .. code-block:: c. lto_codegen_set_pic_model(lto_code_gen_t). And each symbol that is referenced by a native object file or otherwise must not; be optimized away is set with:. .. code-block:: c. lto_codegen_add_must_preserve_symbol(lto_code_gen_t, const char*). After all these settings are done, the linker requests that a native object file; be created from the modules with the settings using:. .. code-block:: c. lto_codegen_compile(lto_code_gen_t, size*). which returns a pointer to a buffer containing the generated native object file.; The linker then parses that and links it with the rest of the native object; files.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1271,Safety,avoid,avoid,1271,"h can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11713,Availability,redundant,redundant,11713,"=====================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:23037,Availability,error,error,23037,",; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invariant Code Motion (:ref:`-licm <passes-licm>`).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:5746,Deployability,pipeline,pipeline,5746,"ase structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow in its body; a loop that is not nested inside; another loop can still be part of an outer cycle; and there can be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13869,Deployability,update,updated,13869," simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13970,Deployability,update,update,13970," simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:5976,Energy Efficiency,power,power,5976,"en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow in its body; a loop that is not nested inside; another loop can still be part of an outer cycle; and there can be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10241,Energy Efficiency,schedul,scheduling,10241,"There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11904,Energy Efficiency,schedul,scheduling,11904,"ly inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9298,Integrability,interface,interface,9298,"). If the loop should; not be executed at all, a **loop guard** must skip the entire loop:. .. image:: ./loop-guard.svg; :width: 500 px. Since the first thing a loop header might do is to check whether there; is another execution and if not, immediately exit without doing any work; (also see :ref:`loop-terminology-loop-rotate`), loop trip count is not; the best measure of a loop's number of iterations. For instance, the; number of header executions of the code below for a non-positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when sc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9504,Integrability,interface,interface,9504,"ny work; (also see :ref:`loop-terminology-loop-rotate`), loop trip count is not; the best measure of a loop's number of iterations. For instance, the; number of header executions of the code below for a non-positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14622,Modifiability,variab,variable,14622,"ccordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops that are in LCSSA form.; The evolution of a scalar (loop-variant) expression that; SCEV can analyze is, by definition, relative to a loop.; An expression is represented in LLVM by an; `llvm::Instruction <",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14701,Modifiability,variab,variable,14701," = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops that are in LCSSA form.; The evolution of a scalar (loop-variant) expression that; SCEV can analyze is, by definition, relative to a loop.; An expression is represented in LLVM by an; `llvm::Instruction <https://llvm.org/doxygen/classllvm_1_1Instruction.html>`_.; If the expression is inside two (or more) loops (which",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:15034,Modifiability,variab,variable,15034," closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops that are in LCSSA form.; The evolution of a scalar (loop-variant) expression that; SCEV can analyze is, by definition, relative to a loop.; An expression is represented in LLVM by an; `llvm::Instruction <https://llvm.org/doxygen/classllvm_1_1Instruction.html>`_.; If the expression is inside two (or more) loops (which can only; happen if the loops are nested, like in the example above) and you want; to get an analysis of its evolution (from SCEV),; you have to also specify relative to what Loop you want it.; Specifically, you have to use; `getSCEVAtScope() <https://llvm.org/doxygen/classllv",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:258,Performance,optimiz,optimizer,258,".. _loop-terminology:. ===========================================; LLVM Loop Terminology (and Canonical Forms); ===========================================. .. contents::; :local:. Loop Definition; ===============. Loops are an important concept for a code optimizer. In LLVM, detection; of loops in a control-flow graph is done by :ref:`loopinfo`. It is based; on the following definition. A loop is a subset of nodes from the control-flow graph (CFG; where; nodes represent basic blocks) with the following properties:. 1. The induced subgraph (which is the subgraph that contains all the; edges from the CFG within the loop) is strongly connected; (every node is reachable from all others). 2. All edges from outside the subset into the subset point to the same; node, called the **header**. As a consequence, the header dominates; all nodes in the loop (i.e. every execution path to any of the loop's; node will have to pass through the header). 3. The loop is the maximum subset with these properties. That is, no; additional nodes from the CFG can be added such that the induced; subgraph would still be strongly connected and the header would; remain the same. In computer science literature, this is often called a *natural loop*.; In LLVM, a more generalized definition is called a; :ref:`cycle <cycle-terminology>`. Terminology; -----------. The definition of a loop comes with some additional terminology:. * An **entering block** (or **loop predecessor**) is a non-loop node; that has an edge into the loop (necessarily the header). If there is; only one entering block, and its only edge is to the; header, it is also called the loop's **preheader**. The preheader; dominates the loop without itself being part of the loop. * A **latch** is a loop node that has an edge to the header. * A **backedge** is an edge from a latch to the header. * An **exiting edge** is an edge from inside the loop to a node outside; of the loop. The source of such an edge is called an **exiting block**, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:5727,Performance,optimiz,optimization,5727,"ase structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow in its body; a loop that is not nested inside; another loop can still be part of an outer cycle; and there can be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:7160,Performance,optimiz,optimizer,7160,"+i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never; taken, the optimizer can change the conditional branch into an; unconditional one. If a is loop is annotated with; :ref:`llvm.loop.mustprogress <langref_llvm_loop_mustprogress>` metadata,; the compiler is allowed to assume that it will eventually terminate, even; if it cannot prove it. For instance, it may remove a mustprogress-loop; that does not have any side-effect in its body even though the program; could be stuck in that loop forever. Languages such as C and; `C++ <https://eel.is/c++draft/intro.progress#1>`_ have such; forward-progress guarantees for some loops. Also see the; :ref:`mustprogress <langref_mustprogress>` and; :ref:`willreturn <langref_willreturn>` function attributes, as well as; the older :ref:`llvm.sideeffect <llvm_sideeffect>` intrinsic. * The number of executions of the loop header before leaving the loop is; the **loop trip count** (or **",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:7362,Performance,optimiz,optimizer,7362," have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never; taken, the optimizer can change the conditional branch into an; unconditional one. If a is loop is annotated with; :ref:`llvm.loop.mustprogress <langref_llvm_loop_mustprogress>` metadata,; the compiler is allowed to assume that it will eventually terminate, even; if it cannot prove it. For instance, it may remove a mustprogress-loop; that does not have any side-effect in its body even though the program; could be stuck in that loop forever. Languages such as C and; `C++ <https://eel.is/c++draft/intro.progress#1>`_ have such; forward-progress guarantees for some loops. Also see the; :ref:`mustprogress <langref_mustprogress>` and; :ref:`willreturn <langref_willreturn>` function attributes, as well as; the older :ref:`llvm.sideeffect <llvm_sideeffect>` intrinsic. * The number of executions of the loop header before leaving the loop is; the **loop trip count** (or **iteration count**). If the loop should; not be executed at all, a **loop guard** must skip the entire loop:. .. image:: ./loop-guard.svg; :width: 50",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9727,Performance,optimiz,optimization,9727,"positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); ===========",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9825,Performance,optimiz,optimization,9825,"r (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11943,Performance,optimiz,optimizations,11943,"lue that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12922,Performance,optimiz,optimizations,12922," scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14358,Performance,perform,perform,14358,"de of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14482,Performance,perform,performs,14482,"de of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22257,Performance,load,loads,22257,"r, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.next, %latch ]; br label %latch. latch:; %i.next = add nsw i32 %i2, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %loop.exit. loop.exit:; br label %exit. exit:; ret void; }. .. image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22494,Performance,load,loading,22494,"image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22594,Performance,load,load,22594,"plicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22855,Performance,load,load,22855,"der basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22968,Performance,load,loading,22968,"xit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:278,Safety,detect,detection,278,".. _loop-terminology:. ===========================================; LLVM Loop Terminology (and Canonical Forms); ===========================================. .. contents::; :local:. Loop Definition; ===============. Loops are an important concept for a code optimizer. In LLVM, detection; of loops in a control-flow graph is done by :ref:`loopinfo`. It is based; on the following definition. A loop is a subset of nodes from the control-flow graph (CFG; where; nodes represent basic blocks) with the following properties:. 1. The induced subgraph (which is the subgraph that contains all the; edges from the CFG within the loop) is strongly connected; (every node is reachable from all others). 2. All edges from outside the subset into the subset point to the same; node, called the **header**. As a consequence, the header dominates; all nodes in the loop (i.e. every execution path to any of the loop's; node will have to pass through the header). 3. The loop is the maximum subset with these properties. That is, no; additional nodes from the CFG can be added such that the induced; subgraph would still be strongly connected and the header would; remain the same. In computer science literature, this is often called a *natural loop*.; In LLVM, a more generalized definition is called a; :ref:`cycle <cycle-terminology>`. Terminology; -----------. The definition of a loop comes with some additional terminology:. * An **entering block** (or **loop predecessor**) is a non-loop node; that has an edge into the loop (necessarily the header). If there is; only one entering block, and its only edge is to the; header, it is also called the loop's **preheader**. The preheader; dominates the loop without itself being part of the loop. * A **latch** is a loop node that has an edge to the header. * A **backedge** is an edge from a latch to the header. * An **exiting edge** is an edge from inside the loop to a node outside; of the loop. The source of such an edge is called an **exiting block**, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:4237,Safety,detect,detect,4237,"de set can; be a subset of another loop with a different loop header. The loop; hierarchy in a function forms a forest: Each top-level loop is the; root of the tree of the loops nested inside it. .. image:: ./loop-nested.svg; :width: 350 px. * It is not possible that two loops share only a few of their nodes.; Two loops are either disjoint or one is nested inside the other. In; the example below the left and right subsets both violate the; maximality condition. Only the merge of both sets is considered a loop. .. image:: ./loop-nonmaximal.svg; :width: 250 px. * It is also possible that two logical loops share a header, but are; considered a single loop by LLVM:. .. code-block:: C. for (int i = 0; i < 128; ++i); for (int j = 0; j < 128; ++j); body(i,j);. which might be represented in LLVM-IR as follows. Note that there is; only a single header and hence just a single loop. .. image:: ./loop-merge.svg; :width: 400 px. The :ref:`LoopSimplify <loop-terminology-loop-simplify>` pass will; detect the loop and ensure separate headers for the outer and inner loop. .. image:: ./loop-separate.svg; :width: 400 px. * A cycle in the CFG does not imply there is a loop. The example below; shows such a CFG, where there is no header node that dominates all; other nodes in the cycle. This is called **irreducible control-flow**. .. image:: ./loop-irreducible.svg; :width: 150 px. The term reducible results from the ability to collapse the CFG into a; single node by successively replacing one of three base structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:6345,Safety,abort,abort,6345,"n be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:6398,Safety,abort,abort,6398,"ef:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never; taken, the optimizer can change the conditional branch into an; unconditional one. If a is loop is a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9454,Safety,detect,detection,9454,"is to check whether there; is another execution and if not, immediately exit without doing any work; (also see :ref:`loop-terminology-loop-rotate`), loop trip count is not; the best measure of a loop's number of iterations. For instance, the; number of header executions of the code below for a non-positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11713,Safety,redund,redundant,11713,"=====================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:23151,Safety,avoid,avoid,23151,",; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invariant Code Motion (:ref:`-licm <passes-licm>`).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:23569,Safety,avoid,avoid,23569,",; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invariant Code Motion (:ref:`-licm <passes-licm>`).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:3836,Testability,log,logical,3836,"is trivially strongly connected. .. image:: ./loop-single.svg; :width: 300 px. In this case, the role of header, exiting block and latch fall to the; same node. :ref:`loopinfo` reports this as:. .. code-block:: console. $ opt input.ll -passes='print<loops>'; Loop at depth 1 containing: %for.body<header><latch><exiting>. * Loops can be nested inside each other. That is, a loop's node set can; be a subset of another loop with a different loop header. The loop; hierarchy in a function forms a forest: Each top-level loop is the; root of the tree of the loops nested inside it. .. image:: ./loop-nested.svg; :width: 350 px. * It is not possible that two loops share only a few of their nodes.; Two loops are either disjoint or one is nested inside the other. In; the example below the left and right subsets both violate the; maximality condition. Only the merge of both sets is considered a loop. .. image:: ./loop-nonmaximal.svg; :width: 250 px. * It is also possible that two logical loops share a header, but are; considered a single loop by LLVM:. .. code-block:: C. for (int i = 0; i < 128; ++i); for (int j = 0; j < 128; ++j); body(i,j);. which might be represented in LLVM-IR as follows. Note that there is; only a single header and hence just a single loop. .. image:: ./loop-merge.svg; :width: 400 px. The :ref:`LoopSimplify <loop-terminology-loop-simplify>` pass will; detect the loop and ensure separate headers for the outer and inner loop. .. image:: ./loop-separate.svg; :width: 400 px. * A cycle in the CFG does not imply there is a loop. The example below; shows such a CFG, where there is no header node that dominates all; other nodes in the cycle. This is called **irreducible control-flow**. .. image:: ./loop-irreducible.svg; :width: 150 px. The term reducible results from the ability to collapse the CFG into a; single node by successively replacing one of three base structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or sw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:18852,Testability,test,test,18852,"s only used as an example and does not pose any strict; requirements. For example, the value might dominate the current block; but we can still insert a PHI (as we do with LCSSA PHI nodes) *and*; use the original value afterwards (in which case the two live ranges overlap,; although in LCSSA (the whole point is that) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:18960,Testability,test,test,18960,"rent block; but we can still insert a PHI (as we do with LCSSA PHI nodes) *and*; use the original value afterwards (in which case the two live ranges overlap,; although in LCSSA (the whole point is that) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:19210,Testability,test,test,19210,"at) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:19293,Testability,test,test,19293,"l the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:19781,Testability,test,test,19781,"ation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %body. body:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %exit. exit:; ret void; }. .. image:: ./loop-terminology-rotated-loop.png; :width: 400 px. Note two things:. * The condition check was moved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:20309,Testability,test,test,20309,"f (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %body. body:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %exit. exit:; ret void; }. .. image:: ./loop-terminology-rotated-loop.png; :width: 400 px. Note two things:. * The condition check was moved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the loop to the latch.; * The compiler in this case can't deduce that the loop will; definitely execute at least once so the above transformation; is not valid. As mentioned above, a guard has to be inserted,; which is something that LoopRotate will do. This is how LoopRotate transforms this loop:. .. code-block:: none. define void @test(i32 %n) {; entry:; %guard_cond = icmp slt i32 0, %n; br i1 %guard_cond, label %loop.preheader, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:21125,Testability,test,test,21125,"pRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %body. body:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %exit. exit:; ret void; }. .. image:: ./loop-terminology-rotated-loop.png; :width: 400 px. Note two things:. * The condition check was moved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the loop to the latch.; * The compiler in this case can't deduce that the loop will; definitely execute at least once so the above transformation; is not valid. As mentioned above, a guard has to be inserted,; which is something that LoopRotate will do. This is how LoopRotate transforms this loop:. .. code-block:: none. define void @test(i32 %n) {; entry:; %guard_cond = icmp slt i32 0, %n; br i1 %guard_cond, label %loop.preheader, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.next, %latch ]; br label %latch. latch:; %i.next = add nsw i32 %i2, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %loop.exit. loop.exit:; br label %exit. exit:; ret void; }. .. image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:4215,Usability,simpl,simplify,4215,"de set can; be a subset of another loop with a different loop header. The loop; hierarchy in a function forms a forest: Each top-level loop is the; root of the tree of the loops nested inside it. .. image:: ./loop-nested.svg; :width: 350 px. * It is not possible that two loops share only a few of their nodes.; Two loops are either disjoint or one is nested inside the other. In; the example below the left and right subsets both violate the; maximality condition. Only the merge of both sets is considered a loop. .. image:: ./loop-nonmaximal.svg; :width: 250 px. * It is also possible that two logical loops share a header, but are; considered a single loop by LLVM:. .. code-block:: C. for (int i = 0; i < 128; ++i); for (int j = 0; j < 128; ++j); body(i,j);. which might be represented in LLVM-IR as follows. Note that there is; only a single header and hence just a single loop. .. image:: ./loop-merge.svg; :width: 400 px. The :ref:`LoopSimplify <loop-terminology-loop-simplify>` pass will; detect the loop and ensure separate headers for the outer and inner loop. .. image:: ./loop-separate.svg; :width: 400 px. * A cycle in the CFG does not imply there is a loop. The example below; shows such a CFG, where there is no header node that dominates all; other nodes in the cycle. This is called **irreducible control-flow**. .. image:: ./loop-irreducible.svg; :width: 150 px. The term reducible results from the ability to collapse the CFG into a; single node by successively replacing one of three base structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9928,Usability,simpl,simplify,9928,"*, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA for",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10071,Usability,simpl,simpler,10071,"or executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10148,Usability,simpl,simplify,10148,"There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10170,Usability,simpl,simplify,10170,"There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12530,Usability,clear,clearly,12530,", this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12937,Usability,simpl,simpler,12937," scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12962,Usability,simpl,simple,12962,"side of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13252,Usability,simpl,simple-loop-unswitch,13252,"LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that bec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13281,Usability,simpl,simple-loop-unswitch,13281,"LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that bec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:17382,Usability,intuit,intuition,17382,"an now just use; `getSCEV() <https://llvm.org/doxygen/classllvm_1_1ScalarEvolution.html#a30bd18ac905eacf3601bc6a553a9ff49>`_.; and which of these two llvm::Instructions you pass to it disambiguates; the context / scope / relative loop. .. rubric:: Footnotes. .. [#lcssa-construction] To insert these loop-closing PHI nodes, one has to; (re-)compute dominance frontiers (if the loop has multiple exits). .. [#point-of-use-phis] Considering the point of use of a PHI entry value; to be in the respective predecessor is a convention across the whole LLVM.; The reason is mostly practical; for example it preserves the dominance; property of SSA. It is also just an overapproximation of the actual; number of uses; the incoming block could branch to another block in which; case the value is not actually used but there are no side-effects (it might; increase its live range which is not relevant in LCSSA though).; Furthermore, we can gain some intuition if we consider liveness:; A PHI is *usually* inserted in the current block because the value can't; be used from this point and onwards (i.e. the current block is a dominance; frontier). It doesn't make sense to consider that the value is used in; the current block (because of the PHI) since the value stops being live; before the PHI. In some sense the PHI definition just ""replaces"" the original; value definition and doesn't actually use it. It should be stressed that; this analogy is only used as an example and does not pose any strict; requirements. For example, the value might dominate the current block; but we can still insert a PHI (as we do with LCSSA PHI nodes) *and*; use the original value afterwards (in which case the two live ranges overlap,; although in LCSSA (the whole point is that) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:21742,Usability,simpl,simplify,21742,"ved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the loop to the latch.; * The compiler in this case can't deduce that the loop will; definitely execute at least once so the above transformation; is not valid. As mentioned above, a guard has to be inserted,; which is something that LoopRotate will do. This is how LoopRotate transforms this loop:. .. code-block:: none. define void @test(i32 %n) {; entry:; %guard_cond = icmp slt i32 0, %n; br i1 %guard_cond, label %loop.preheader, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.next, %latch ]; br label %latch. latch:; %i.next = add nsw i32 %i2, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %loop.exit. loop.exit:; br label %exit. exit:; ret void; }. .. image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:795,Availability,robust,robust,795,"=======================================; LLVM's Optional Rich Disassembly Output; =======================================. .. contents::; :local:. Introduction; ============. LLVM's default disassembly output is raw text. To allow consumers more ability; to introspect the instructions' textual representation or to reformat for a more; user friendly display there is an optional rich disassembly output. This optional output is sufficient to reference into individual portions of the; instruction text. This is intended for clients like disassemblers, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:1564,Energy Efficiency,efficient,efficiently,1564,"rs, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-modifier-list ':' annotated-text '>'; tag-name: identifier; tag-modifier-list: comma delimited identifier list. The tag-name is an identifier which gives the type of the annotation. For the; first pass, this will be very simple, with memory references, registers, and; immediates having the tag names ""mem"", ""reg"", and ""imm"", respectively. The tag-modifier-list is typically additional target-specific context, such as; register class. Clients should accept and ignore any tag-names or tag-modifiers they do not; understand, allowing the annotations t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:2646,Performance,load,load,2646,"p annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-modifier-list ':' annotated-text '>'; tag-name: identifier; tag-modifier-list: comma delimited identifier list. The tag-name is an identifier which gives the type of the annotation. For the; first pass, this will be very simple, with memory references, registers, and; immediates having the tag names ""mem"", ""reg"", and ""imm"", respectively. The tag-modifier-list is typically additional target-specific context, such as; register class. Clients should accept and ignore any tag-names or tag-modifiers they do not; understand, allowing the annotations to grow in richness without breaking older; clients. For example, a possible annotation of an ARM load of a stack-relative location; might be annotated as:. .. code-block:: text. ldr <reg gpr:r0>, <mem regoffset:[<reg gpr:sp>, <imm:#4>]>. 1: For assembly dialects in which '<' and/or '>' are legal tokens, a literal token is escaped by following immediately with a repeat of the character. For example, a literal '<' character is output as '<<' in an annotated assembly string. C API Details; -------------. The intended consumers of this information use the C API, therefore the new C; API function for the disassembler will be added to provide an option to produce; disassembled instructions with annotations, ``LLVMSetDisasmOptions()`` and the; ``LLVMDisassembler_Option_UseMarkup`` option (see above).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:765,Usability,simpl,simple,765,"=======================================; LLVM's Optional Rich Disassembly Output; =======================================. .. contents::; :local:. Introduction; ============. LLVM's default disassembly output is raw text. To allow consumers more ability; to introspect the instructions' textual representation or to reformat for a more; user friendly display there is an optional rich disassembly output. This optional output is sufficient to reference into individual portions of the; instruction text. This is intended for clients like disassemblers, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:987,Usability,simpl,simply,987,"=======================================; LLVM's Optional Rich Disassembly Output; =======================================. .. contents::; :local:. Introduction; ============. LLVM's default disassembly output is raw text. To allow consumers more ability; to introspect the instructions' textual representation or to reformat for a more; user friendly display there is an optional rich disassembly output. This optional output is sufficient to reference into individual portions of the; instruction text. This is intended for clients like disassemblers, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:2219,Usability,simpl,simple,2219,"hen subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-modifier-list ':' annotated-text '>'; tag-name: identifier; tag-modifier-list: comma delimited identifier list. The tag-name is an identifier which gives the type of the annotation. For the; first pass, this will be very simple, with memory references, registers, and; immediates having the tag names ""mem"", ""reg"", and ""imm"", respectively. The tag-modifier-list is typically additional target-specific context, such as; register class. Clients should accept and ignore any tag-names or tag-modifiers they do not; understand, allowing the annotations to grow in richness without breaking older; clients. For example, a possible annotation of an ARM load of a stack-relative location; might be annotated as:. .. code-block:: text. ldr <reg gpr:r0>, <mem regoffset:[<reg gpr:sp>, <imm:#4>]>. 1: For assembly dialects in which '<' and/or '>' are legal tokens, a literal token is escaped by following immediately with a repeat of the character. For example, a literal '<' character is output as '<<' in an annotated assembly string. C API Details; -------------. The intended consumers of this information use the C API, therefore the new C; API function for the disassembler will be added to provide an option to produce; disassembled instructions with annotations",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:6586,Deployability,update,update,6586," shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::mapSectionAddress.; This should happen before the section memory is copied to its new; location. When MCJIT::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeDyldImpl stores the new; address in an internal data structure but does not update the code at this; time, since other sections are likely to change. When the client is finished remapping section addresses, it will call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations itera",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5265,Energy Efficiency,allocate,allocated,5265," including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:8685,Energy Efficiency,allocate,allocated,8685,"ll call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; functions, such as registering the EH frame information with a debugger. Finally, MCJIT calls the memory manager's finalizeMemory method. In this; method, the memory manager will invalidate the target code cache, if; necessary, and apply final permissions to the memory pages it has; allocated for code and data memory.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:1753,Integrability,wrap,wrapper,1753,"MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to Targ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3003,Integrability,depend,depending,3003,"ter to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3696,Integrability,wrap,wrapper,3696,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4130,Integrability,wrap,wraps,4130,"ontains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:1690,Modifiability,variab,variable,1690,"MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to Targ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:388,Performance,load,loading,388,"===============================; MCJIT Design and Implementation; ===============================. Introduction; ============. This document describes the internal workings of the MCJIT execution; engine and the RuntimeDyld component. It is intended as a high level; overview of the implementation, showing the flow and interactions of; objects throughout the code generation and dynamic loading process. Engine Creation; ===============. In most cases, an EngineBuilder object is used to create an instance of; the MCJIT execution engine. The EngineBuilder takes an llvm::Module; object as an argument to its constructor. The client may then set various; options that we control the later be passed along to the MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:1902,Performance,load,loaded,1902,"r::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:2501,Performance,cache,cached,2501,"rs to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object imag",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:2834,Performance,load,load,2834,"d the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3346,Performance,load,loaded,3346,"s described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3435,Performance,cache,cache,3435,"s described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3672,Performance,load,loaded,3672,"t uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3921,Performance,load,loadObject,3921,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3942,Performance,perform,perform,3942,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3963,Performance,load,loading,3963,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3994,Performance,load,load,3994,"r MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4021,Performance,load,loadObject,4021,"object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4368,Performance,load,loadObject,4368,"ust be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4550,Performance,load,loaded,4550,"ither through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject comp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4734,Performance,load,loadObject,4734,"ther RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to ap",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5122,Performance,load,load-object,5122,"ge, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5161,Performance,load,loadObject,5161," including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5246,Performance,load,loaded,5246," including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5545,Performance,load,loadObject,5545," data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::mapSectionAddress.; This should happen before the section memory is copied to its new; location. When MCJIT::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeD",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:7514,Performance,load,loaded,7514,"::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeDyldImpl stores the new; address in an internal data structure but does not update the code at this; time, since other sections are likely to change. When the client is finished remapping section addresses, it will call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; func",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:8607,Performance,cache,cache,8607,"ll call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; functions, such as registering the EH frame information with a debugger. Finally, MCJIT calls the memory manager's finalizeMemory method. In this; method, the memory manager will invalidate the target code cache, if; necessary, and apply final permissions to the memory pages it has; allocated for code and data memory.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4228,Security,access,access,4228,"ontains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:1119,Availability,resilien,resilient,1119,"=======================. Here are several ideas you can take into account when designing your specific; LLVM Social. Before you start, it is essential to make sure that the meetup is as welcoming; as any other event related to LLVM. Therefore you shall follow LLVM's; `Code of Conduct <https://llvm.org/docs/CodeOfConduct.html>`_. Other than that - your mileage may vary. Please adapt your social to what works; best for your specific situation. General suggestions; -------------------. * We highly recommend that you join the official LLVM meetup organization. In; addition to covering the cost of the meetup, all LLVM meetups are advertised; together and easily found by potential attendees. Please contact; arnaud.degrandmaison@llvm.org for more details.; * Beware of cultural differences: what works well in one region may not work in; other part of the world.; * Do not be alone to organize the meetup. Try to work with a couple other; organizers. This is more motivating as an organizer, and this makes the; meetup more resilient over time.; * Each event can have a different form such as a social event, or; a hackathon/workshop, or a 'mini-conference' with one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2303,Deployability,rolling,rolling,2303," one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * Its a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/caf with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., its not a good idea to have LLVM meetu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2408,Deployability,pipeline,pipeline,2408,"eekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * Its a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/caf with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., its not a good idea to have LLVM meetup on Thursday after; C++ meetup on Wednesday).; * Meetups on weekends may attract people who live far away",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:471,Energy Efficiency,adapt,adapt,471,"=====================================; How to start LLVM Social in your town; =====================================. Here are several ideas you can take into account when designing your specific; LLVM Social. Before you start, it is essential to make sure that the meetup is as welcoming; as any other event related to LLVM. Therefore you shall follow LLVM's; `Code of Conduct <https://llvm.org/docs/CodeOfConduct.html>`_. Other than that - your mileage may vary. Please adapt your social to what works; best for your specific situation. General suggestions; -------------------. * We highly recommend that you join the official LLVM meetup organization. In; addition to covering the cost of the meetup, all LLVM meetups are advertised; together and easily found by potential attendees. Please contact; arnaud.degrandmaison@llvm.org for more details.; * Beware of cultural differences: what works well in one region may not work in; other part of the world.; * Do not be alone to organize the meetup. Try to work with a couple other; organizers. This is more motivating as an organizer, and this makes the; meetup more resilient over time.; * Each event can have a different form such as a social event, or; a hackathon/workshop, or a 'mini-conference' with one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2247,Energy Efficiency,schedul,scheduled,2247," one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * Its a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/caf with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., its not a good idea to have LLVM meetu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2900,Energy Efficiency,charge,charge,2900,"oup. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * Its a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/caf with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., its not a good idea to have LLVM meetup on Thursday after; C++ meetup on Wednesday).; * Meetups on weekends may attract people who live far away from the city,; but the people who live in the city may not attend.; * Make a poll, but beware that not every responder will join (we had ~20 votes; on the poll, while only ~8 people attended). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:471,Modifiability,adapt,adapt,471,"=====================================; How to start LLVM Social in your town; =====================================. Here are several ideas you can take into account when designing your specific; LLVM Social. Before you start, it is essential to make sure that the meetup is as welcoming; as any other event related to LLVM. Therefore you shall follow LLVM's; `Code of Conduct <https://llvm.org/docs/CodeOfConduct.html>`_. Other than that - your mileage may vary. Please adapt your social to what works; best for your specific situation. General suggestions; -------------------. * We highly recommend that you join the official LLVM meetup organization. In; addition to covering the cost of the meetup, all LLVM meetups are advertised; together and easily found by potential attendees. Please contact; arnaud.degrandmaison@llvm.org for more details.; * Beware of cultural differences: what works well in one region may not work in; other part of the world.; * Do not be alone to organize the meetup. Try to work with a couple other; organizers. This is more motivating as an organizer, and this makes the; meetup more resilient over time.; * Each event can have a different form such as a social event, or; a hackathon/workshop, or a 'mini-conference' with one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6792,Availability,down,down,6792,"e.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14169,Deployability,update,updated,14169,"User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14197,Deployability,update,updated,14197,"User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14300,Deployability,update,update,14300,"d, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14430,Deployability,update,update,14430,"walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14533,Deployability,update,update,14533," U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19143,Deployability,update,updated,19143,"ions have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers'",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19204,Deployability,update,update,19204," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19474,Deployability,update,updates,19474," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11686,Energy Efficiency,reduce,reduce,11686,"is relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:8020,Integrability,depend,depends,8020,"at ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:3438,Modifiability,variab,variables,3438,"s that there is a *single*; ``Def`` chain that connects all the ``Def``\ s, either directly; or indirectly. For example in:. .. code-block:: llvm. b = MemoryDef(a); c = MemoryDef(b); d = MemoryDef(c). ``d`` is connected directly with ``c`` and indirectly with ``b``.; This means that ``d`` potentially clobbers (see below) ``c`` *or*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``MemoryPhi``\ s don't correspond to any; concrete operation. As such, ``BasicBlock``\ s are mapped to ``MemoryPhi``\ s; inside ``MemorySSA``, whereas ``Instruction``\ s are mapped to ``MemoryUse``\ s; and ``MemoryDef``\ s. Note also that in SSA, Phi nodes merge must-reach definitions (that is,; definitions that *must* be new versions of variables). In MemorySSA, PHI nodes; merge may-reach definitions (that is, until disambiguated, the versions that; reach a phi node may or may not clobber a given variable). ``MemoryUse``\ s are operations which use but don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:3601,Modifiability,variab,variable,3601,". b = MemoryDef(a); c = MemoryDef(b); d = MemoryDef(c). ``d`` is connected directly with ``c`` and indirectly with ``b``.; This means that ``d`` potentially clobbers (see below) ``c`` *or*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``MemoryPhi``\ s don't correspond to any; concrete operation. As such, ``BasicBlock``\ s are mapped to ``MemoryPhi``\ s; inside ``MemorySSA``, whereas ``Instruction``\ s are mapped to ``MemoryUse``\ s; and ``MemoryDef``\ s. Note also that in SSA, Phi nodes merge must-reach definitions (that is,; definitions that *must* be new versions of variables). In MemorySSA, PHI nodes; merge may-reach definitions (that is, until disambiguated, the versions that; reach a phi node may or may not clobber a given variable). ``MemoryUse``\ s are operations which use but don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9581,Modifiability,flexible,flexible,9581,"ruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16718,Modifiability,variab,variables,16718,"-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16805,Modifiability,variab,variable,16805,"-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16884,Modifiability,variab,variables,16884,"-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17413,Modifiability,variab,variables,17413," br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimiz",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17548,Modifiability,variab,variables,17548,"ause it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there ar",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17590,Modifiability,variab,variables,17590,"sion; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17715,Modifiability,variab,variables,17715,"if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18126,Modifiability,variab,variables,18126,", and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18289,Modifiability,variab,variable,18289,"nalysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18445,Modifiability,variab,variable,18445,"esent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:2094,Performance,load,load,2094,"ocument goes over how ``MemorySSA`` is structured, and some basic; intuition on how ``MemorySSA`` works. A paper on MemorySSA (with notes about how it's implemented in GCC) `can be; found here <http://www.airs.com/dnovillo/Papers/mem-ssa.pdf>`_. Though, it's; relatively out-of-date; the paper references multiple memory partitions, but GCC; eventually swapped to just using one, like we now have in LLVM. Like; GCC's, LLVM's MemorySSA is intraprocedural. MemorySSA Structure; ===================. MemorySSA is a virtual IR. After it's built, ``MemorySSA`` will contain a; structure that maps ``Instruction``\ s to ``MemoryAccess``\ es, which are; ``MemorySSA``'s parallel to LLVM ``Instruction``\ s. Each ``MemoryAccess`` can be one of three types:. - ``MemoryDef``; - ``MemoryPhi``; - ``MemoryUse``. ``MemoryDef``\ s are operations which may either modify memory, or which; introduce some kind of ordering constraints. Examples of ``MemoryDef``\ s; include ``store``\ s, function calls, ``load``\ s with ``acquire`` (or higher); ordering, volatile operations, memory fences, etc. A ``MemoryDef``; always introduces a new version of the entire memory and is linked with a single; ``MemoryDef/MemoryPhi`` which is the version of memory that the new; version is based on. This implies that there is a *single*; ``Def`` chain that connects all the ``Def``\ s, either directly; or indirectly. For example in:. .. code-block:: llvm. b = MemoryDef(a); c = MemoryDef(b); d = MemoryDef(c). ``d`` is connected directly with ``c`` and indirectly with ``b``.; This means that ``d`` potentially clobbers (see below) ``c`` *or*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``Memo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:3717,Performance,load,load,3717,"*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``MemoryPhi``\ s don't correspond to any; concrete operation. As such, ``BasicBlock``\ s are mapped to ``MemoryPhi``\ s; inside ``MemorySSA``, whereas ``Instruction``\ s are mapped to ``MemoryUse``\ s; and ``MemoryDef``\ s. Note also that in SSA, Phi nodes merge must-reach definitions (that is,; definitions that *must* be new versions of variables). In MemorySSA, PHI nodes; merge may-reach definitions (that is, until disambiguated, the versions that; reach a phi node may or may not clobber a given variable). ``MemoryUse``\ s are operations which use but don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6088,Performance,load,load,6088,"modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6168,Performance,load,load,6168,"modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6606,Performance,load,load,6606," of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or `",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7206,Performance,optimiz,optimization,7206,"ecede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; =======",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7609,Performance,load,load,7609," LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` .",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7827,Performance,load,load,7827,"g; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9439,Performance,optimiz,optimize,9439,"an then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you ch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10397,Performance,cache,cached,10397,"d; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10795,Performance,optimiz,optimize,10795,"ies ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10964,Performance,optimiz,optimization,10964," the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11071,Performance,optimiz,optimized,11071,"esults; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11119,Performance,optimiz,optimized,11119,"ingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed an",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11241,Performance,optimiz,optimized,11241,"on ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11622,Performance,optimiz,optimize,11622,"is relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11853,Performance,optimiz,optimization,11853,"iningAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12016,Performance,optimiz,optimize,12016," is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12481,Performance,optimiz,optimized,12481,"r and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12594,Performance,cache,cache,12594,"; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<Memo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12793,Performance,optimiz,optimized,12793,"the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. /",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12912,Performance,optimiz,optimized,12912,"l ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13168,Performance,optimiz,optimized,13168,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13234,Performance,optimiz,optimized,13234,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13269,Performance,optimiz,optimized,13269,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13313,Performance,optimiz,optimized,13313,"'t have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13360,Performance,load,loads,13360,"'t have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13804,Performance,optimiz,optimized,13804,"ode snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14613,Performance,optimiz,optimization,14613," U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15214,Performance,load,load,15214,"tion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, labe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15294,Performance,load,load,15294,"tion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, labe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15746,Performance,optimiz,optimizing,15746,"actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15803,Performance,optimiz,optimizations,15803,"actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16032,Performance,optimiz,optimizations,16032,"i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results confl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16314,Performance,load,load,16314,"br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not me",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16481,Performance,load,load,16481," one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16542,Performance,load,load,16542,"else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new v",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17191,Performance,optimiz,optimization,17191,"hen trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18073,Performance,optimiz,optimizations,18073,", and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18332,Performance,optimiz,optimizations,18332,"o; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18825,Performance,optimiz,optimized,18825,"stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Develop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18928,Performance,optimiz,optimized,18928,"stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Develop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19334,Performance,optimiz,optimizations,19334," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19485,Performance,optimiz,optimized,19485," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19553,Performance,optimiz,optimization,19553," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19770,Performance,optimiz,optimize-memoryssa,19770," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19830,Performance,optimiz,optimizations,19830," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:4695,Security,access,access,4695,"t don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:4958,Security,access,accessing,4958,"the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:5098,Security,access,accesses,5098,"defined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:5483,Security,access,access,5483,"ess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9880,Security,access,access,9880," The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10001,Security,access,access,10001,"ven:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10093,Security,access,access,10093,"ven:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10214,Security,access,access,10214,"o ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Spec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10327,Security,access,access,10327,"d; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10356,Security,access,access,10356,"d; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11082,Security,access,access,11082,"esults; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12110,Security,access,access,12110," is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12209,Security,access,access,12209,"oryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12387,Security,access,accesses,12387,"oryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12491,Security,access,access,12491,"r and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12774,Security,access,accesses,12774,"the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. /",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13219,Security,access,access,13219,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13814,Security,access,access,13814,"ode snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13838,Security,access,access,13838,"ode snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14552,Security,access,access,14552," U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19495,Security,access,access,19495," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:1170,Usability,intuit,intuition,1170,"t the; interactions between various memory operations. Its goal is to replace; ``MemoryDependenceAnalysis`` for most (if not all) use-cases. This is because,; unless you're very careful, use of ``MemoryDependenceAnalysis`` can easily; result in quadratic-time algorithms in LLVM. Additionally, ``MemorySSA`` doesn't; have as many arbitrary limits as ``MemoryDependenceAnalysis``, so you should get; better results, too. One common use of ``MemorySSA`` is to quickly find out; that something definitely cannot happen (for example, reason that a hoist; out of a loop can't happen). At a high level, one of the goals of ``MemorySSA`` is to provide an SSA based; form for memory, complete with def-use and use-def chains, which; enables users to quickly find may-def and may-uses of memory operations.; It can also be thought of as a way to cheaply give versions to the complete; state of memory, and associate memory operations with those versions. This document goes over how ``MemorySSA`` is structured, and some basic; intuition on how ``MemorySSA`` works. A paper on MemorySSA (with notes about how it's implemented in GCC) `can be; found here <http://www.airs.com/dnovillo/Papers/mem-ssa.pdf>`_. Though, it's; relatively out-of-date; the paper references multiple memory partitions, but GCC; eventually swapped to just using one, like we now have in LLVM. Like; GCC's, LLVM's MemorySSA is intraprocedural. MemorySSA Structure; ===================. MemorySSA is a virtual IR. After it's built, ``MemorySSA`` will contain a; structure that maps ``Instruction``\ s to ``MemoryAccess``\ es, which are; ``MemorySSA``'s parallel to LLVM ``Instruction``\ s. Each ``MemoryAccess`` can be one of three types:. - ``MemoryDef``; - ``MemoryPhi``; - ``MemoryUse``. ``MemoryDef``\ s are operations which may either modify memory, or which; introduce some kind of ordering constraints. Examples of ``MemoryDef``\ s; include ``store``\ s, function calls, ``load``\ s with ``acquire`` (or higher); ordering, volatile",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9204,Usability,clear,clearly,9204,"act with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10675,Usability,simpl,simple,10675,"ies ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17961,Usability,simpl,simply,17961,", and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18302,Usability,simpl,simply,18302,"o; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:330,Availability,error,error,330,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:459,Availability,error,errors,459,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1864,Availability,error,errors,1864,"ink your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:3078,Deployability,update,update,3078,"he start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/arm-a-profile-architecture-2018-developments-armv85a; .. _AddressSanitizer: https://clang.llvm.org/docs/AddressSanitizer.html; .. _HardwareAssistedAddressSanitizer: https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:3236,Energy Efficiency,allocate,allocated,3236,"he start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/arm-a-profile-architecture-2018-developments-armv85a; .. _AddressSanitizer: https://clang.llvm.org/docs/AddressSanitizer.html; .. _HardwareAssistedAddressSanitizer: https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1927,Modifiability,variab,variable,1927,"ink your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2158,Modifiability,variab,variable,2158,"x -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2258,Modifiability,variab,variable,2258,"gy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2358,Modifiability,variab,variable,2358,"gy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2454,Modifiability,variab,variables,2454,"gy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2647,Modifiability,variab,variables,2647,"enerate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:336,Safety,detect,detector,336,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:432,Safety,detect,detects,432,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1308,Safety,safe,safety,1308,"zer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1875,Safety,detect,detected,1875,"ink your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2179,Safety,safe,safety,2179,"x -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1659,Security,access,access,1659," slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2787,Security,sanitiz,sanitizers,2787,"Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/arm-a-profile-architecture-2018-developments-armv85a; .. _AddressSanitizer: https://clang.llvm.org/docs/AddressSanitizer.html",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:853,Availability,down,down,853,"=================================; MergeFunctions pass, how it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/do",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:3097,Availability,down,down,3097,"p://en.wikipedia.org/wiki/Basic_block>`_"",; ""`user <https://llvm.org/docs/ProgrammersManual.html#the-user-class>`_"",; ""`value <https://llvm.org/docs/ProgrammersManual.html#the-value-class>`_"",; ""`instruction; <https://llvm.org/docs/ProgrammersManual.html#the-instruction-class>`_"". As a good starting point, the Kaleidoscope tutorial can be used:. :doc:`tutorial/index`. It's especially important to understand chapter 3 of tutorial:. :doc:`tutorial/LangImpl03`. The reader should also know how passes work in LLVM. They could use this; article as a reference and start point here:. :doc:`WritingAnLLVMPass`. What else? Well perhaps the reader should also have some experience in LLVM pass; debugging and bug-fixing. Narrative structure; -------------------; The article consists of three parts. The first part explains pass functionality; on the top-level. The second part describes the comparison procedure itself.; The third part describes the merging process. In every part, the author tries to put the contents in the top-down form.; The top-level methods will first be described followed by the terminal ones at; the end, in the tail of each part. If the reader sees the reference to the; method that wasn't described yet, they will find its description a bit below. Basics; ======. How to do it?; -------------; Do we need to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20002,Availability,ping,ping,20002,"t procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, boo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20097,Availability,ping,ping,20097,"t procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, boo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:28025,Availability,alive,alive,28025,"ld be good to do the; next: after merging the places where overridable function were used, still use; overridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If F may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: whether the definition of this; global may be replaced by something non-equivalent at link time. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same name and attributes like function; *F*. It takes maximum alignment of *F* and *G*. 2. Replace all uses of function *F* with uses of function *H*. It is the two; steps procedure instead. First of all, we must take into account, all functions; from whom *F* is called would be changed: since we change the call argument; (from *F* to *H*). If so we must to review these caller functions again after; this procedure. We remove callers from ``FnTree``, method with name; ``removeUsers(F)`` does that (don't confuse with ``replaceAllUsesWith``):. 2.1. ``Inside removeUsers(Value*; V)`` we go through the all values that use value *V* (or *F* in our context).; If value is instruction, we go to function that holds this instruction and; mark it as to-be-analyzed-again (put to ``Deferred`` set), we also remove; caller from ``FnTre",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:1325,Deployability,update,update,1325,".g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/docs/ProgrammersManual.html#the-function-class>`_"",; ""`basic block <http://en.wikipedia.org/wiki/Basic_block>`_"",; ""`user <https://llvm.org/docs/ProgrammersManual.html#the-user-class>`_"",; ""`value <https://llvm.org/docs/ProgrammersManual.html#the-value-class>`_"",; ""`instruction; <https://llvm.o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:25946,Energy Efficiency,reduce,reduce,25946," both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs  return the result. 3. Compare operation types, use *cmpType*. All the same  if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``rep",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5153,Integrability,rout,routines,5153,"`void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the funct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6087,Integrability,rout,routine,6087,"rger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the fla",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6279,Integrability,depend,depends,6279,"y another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7898,Integrability,wrap,wrapper,7898,"tation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented < operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred``  merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumerate only strong functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *D",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10326,Integrability,rout,routine,10326,"ingle function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10363,Integrability,rout,routines,10363,"ingle function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10428,Integrability,rout,routines,10428,"ingle function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:16364,Integrability,depend,depends,16364,"t* are to be expanded and their element types will be checked the same; way. If we get -1 or 1 on some stage, return it. Otherwise return 0. 8. Steps 1-6 describe all the possible cases, if we passed steps 1-6 and didn't; get any conclusions, then invoke ``llvm_unreachable``, since it's quite an; unexpectable case. cmpValues(const Value*, const Value*); -------------------------------------; Method that compares local values. This method gives us an answer to a very curious question: whether we could; treat local values as equal, and which value is greater otherwise. It's; better to start from example:. Consider the situation when we're looking at the same place in left; function ""*FL*"" and in right function ""*FR*"". Every part of *left* place is; equal to the corresponding part of *right* place, and (!) both parts use; *Value* instances, for example:. .. code-block:: text. instr0 i32 %LV ; left side, function FL; instr0 i32 %RV ; right side, function FR. So, now our conclusion depends on *Value* instances comparison. The main purpose of this method is to determine relation between such values. What can we expect from equal functions? At the same place, in functions; ""*FL*"" and ""*FR*"" we expect to see *equal* values, or values *defined* at; the same place in ""*FL*"" and ""*FR*"". Consider a small example here:. .. code-block:: text. define void %f(i32 %pf0, i32 %pf1) {; instr0 i32 %pf0 instr1 i32 %pf1 instr2 i32 123; }. .. code-block:: text. define void %g(i32 %pg0, i32 %pg1) {; instr0 i32 %pg0 instr1 i32 %pg0 instr2 i32 123; }. In this example, *pf0* is associated with *pg0*, *pf1* is associated with; *pg1*, and we also declare that *pf0* < *pf1*, and thus *pg0* < *pf1*. Instructions with opcode ""*instr0*"" would be *equal*, since their types and; opcodes are equal, and values are *associated*. Instructions with opcode ""*instr1*"" from *f* is *greater* than instructions; with opcode ""*instr1*"" from *g*; here we have equal types and opcodes, but; ""*pf1* is greater than ""*",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:18697,Integrability,depend,depends,18697," instances. In basic-block enumeration loop we associate *i*-th; BasicBlock from the left function with *i*-th BasicBlock from the right; function.; * Instructions.; * Instruction operands. Note, we can meet *Value* here we have never seen; before. In this case it is not a function argument, nor *BasicBlock*, nor; *Instruction*. It is a global value. It is a constant, since it's the only; supposed global here. The method also compares: Constants that are of the; same type and if right constant can be losslessly bit-casted to the left; one, then we also compare them. How to implement cmpValues?; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; *Association* is a case of equality for us. We just treat such values as equal,; but, in general, we need to implement antisymmetric relation. As mentioned; above, to understand what is *less*, we can use order in which we; meet values. If both values have the same order in a function (met at the same; time), we then treat values as *associated*. Otherwise  it depends on who was; first. Every time we run the top-level compare method, we initialize two identical; maps (one for the left side, another one for the right side):. ``map<Value, int> sn_mapL, sn_mapR;``. The key of the map is the *Value* itself, the *value*  is its order (call it; *serial number*). To add value *V* we need to perform the next procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemen",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:23133,Integrability,depend,depends,23133,"lass, return 1. 2.1.3. If both types are of the first class type, proceed to the next step; (2.1.3.1). 2.1.3.1. If types are vectors, compare their bitwidth using the; *cmpNumbers*. If result is not 0, return it. 2.1.3.2. Different types, but not a vectors:. * if both of them are pointers, good for us, we can proceed to step 3.; * if one of types is pointer, return result of *isPointer* flags; comparison (*cmpFlags* operation).; * otherwise we have no methods to prove bitcastability, and thus return; result of types comparison (-1 or 1). Steps below are for the case when types are equal, or case when constants are; bitcastable:. 3. One of constants is a ""*null*"" value. Return the result of; ``cmpFlags(L->isNullValue, R->isNullValue)`` comparison. 4. Compare value IDs, and return result if it is not 0:. .. code-block:: c++. if (int Res = cmpNumbers(L->getValueID(), R->getValueID())); return Res;. 5. Compare the contents of constants. The comparison depends on the kind of; constants, but on this stage it is just a lexicographical comparison. Just see; how it was described in the beginning of ""*Functions comparison*"" paragraph.; Mathematically, it is equal to the next case: we encode left constant and right; constant (with similar way *bitcode-writer* does). Then compare left code; sequence and right code sequence. compare(const BasicBlock*, const BasicBlock*); ---------------------------------------------; Compares two *BasicBlock* instances. It enumerates instructions from left *BB* and right *BB*. 1. It assigns serial numbers to the left and right instructions, using; ``cmpValues`` method. 2. If one of left or right is *GEP* (``GetElementPtr``), then treat *GEP* as; greater than other instructions. If both instructions are *GEPs* use ``cmpGEP``; method for comparison. If result is -1 or 1, pass it to the top-level; comparison (return it). 3.1. Compare operations. Call ``cmpOperation`` method. If result is -1 or; 1, return it. 3.2. Compare number of operands, if resul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:27231,Integrability,wrap,wrapper,27231,"ions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``replaceAllUsesWith`` operation here (*RAUW*). 2. *F* could not be overridden, while *G* could. It would be good to do the; next: after merging the places where overridable function were used, still use; overridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If F may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: whether the definition of this; global may be replaced by something non-equivalent at link time. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same nam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30309,Integrability,wrap,wrapper,30309," *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If F could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. Aliases act as *second name* for the aliasee value. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. As-usual: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30332,Integrability,interface,interface,30332," *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If F could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. Aliases act as *second name* for the aliasee value. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. As-usual: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30393,Integrability,wrap,wrapper,30393," *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If F could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. Aliases act as *second name* for the aliasee value. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. As-usual: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:31180,Integrability,wrap,wrapper,31180,"e *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If F could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. Aliases act as *second name* for the aliasee value. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. As-usual: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:31209,Integrability,interface,interface,31209,"e *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If F could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. Aliases act as *second name* for the aliasee value. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. As-usual: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4365,Performance,perform,performs,4365," to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4418,Performance,perform,performs,4418," to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5462,Performance,optimiz,optimization,5462,"t by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and te",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:11476,Performance,perform,perform,11476,"ill use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object comparison results. It is similar to the next ""tree"" objects; comparison:. #. For two trees *T1* and *T2* we perform *depth-first-traversal* and have; two sequences as a product: ""*T1Items*"" and ""*T2Items*"". #. We then compare chains ""*T1Items*"" and ""*T2Items*"" in; the most-significant-item-first order. The result of items comparison; would be the result of *T1* and *T2* comparison itself. FunctionComparator::compare(void); ---------------------------------; A brief look at the source code tells us that the comparison starts in the; ``int FunctionComparator::compare(void)`` method. 1. The first parts to be compared are the function's attributes and some; properties that is outside the attributes term, but still could make the; function different without changing its body. This part of the comparison is; usually done within simple *cmpNumbers* or *cmpFlags* operations (e.g.; ``cmpFlags(F1->hasGC(), F2->hasGC())``). Below is a full list of function's; properties to be compared on this stage:. * *Attributes* (those are returned by ``Function::getAttributes()``; method). * *GC*, for equivalence, *RHS* and *LHS* should be bot",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:19028,Performance,perform,perform,19028,"ion*. It is a global value. It is a constant, since it's the only; supposed global here. The method also compares: Constants that are of the; same type and if right constant can be losslessly bit-casted to the left; one, then we also compare them. How to implement cmpValues?; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; *Association* is a case of equality for us. We just treat such values as equal,; but, in general, we need to implement antisymmetric relation. As mentioned; above, to understand what is *less*, we can use order in which we; meet values. If both values have the same order in a function (met at the same; time), we then treat values as *associated*. Otherwise  it depends on who was; first. Every time we run the top-level compare method, we initialize two identical; maps (one for the left side, another one for the right side):. ``map<Value, int> sn_mapL, sn_mapR;``. The key of the map is the *Value* itself, the *value*  is its order (call it; *serial number*). To add value *V* we need to perform the next procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:25724,Performance,load,load,25724,"asic block). cmpGEP; ------; Compares two GEPs (``getelementptr`` instructions). It differs from regular operations comparison with the only thing: possibility; to use ``accumulateConstantOffset`` method. So, if we get constant offset for both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs  return the result. 3. Compare operation types, use *cmpType*. All the same  if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:1294,Safety,avoid,avoid,1294,".g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/docs/ProgrammersManual.html#the-function-class>`_"",; ""`basic block <http://en.wikipedia.org/wiki/Basic_block>`_"",; ""`user <https://llvm.org/docs/ProgrammersManual.html#the-user-class>`_"",; ""`value <https://llvm.org/docs/ProgrammersManual.html#the-value-class>`_"",; ""`instruction; <https://llvm.o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:3568,Safety,detect,detect,3568," reference and start point here:. :doc:`WritingAnLLVMPass`. What else? Well perhaps the reader should also have some experience in LLVM pass; debugging and bug-fixing. Narrative structure; -------------------; The article consists of three parts. The first part explains pass functionality; on the top-level. The second part describes the comparison procedure itself.; The third part describes the merging process. In every part, the author tries to put the contents in the top-down form.; The top-level methods will first be described followed by the terminal ones at; the end, in the tail of each part. If the reader sees the reference to the; method that wasn't described yet, they will find its description a bit below. Basics; ======. How to do it?; -------------; Do we need to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4681,Safety,detect,detection,4681,"do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4718,Safety,detect,detector,4718,"do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4830,Safety,detect,detector,4830,"have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into ac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4869,Safety,detect,detectors,4869,"have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into ac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:26103,Safety,detect,detected,26103,"-; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs  return the result. 3. Compare operation types, use *cmpType*. All the same  if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``replaceAllUsesWith`` operation here (*RAUW*). 2. *F* could not be overridden, while *G* could. It would be good to do the; next: after merging the places where overridable function were ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5411,Security,hash,hashing,5411,"t by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and te",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5598,Security,access,access,5598,"; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5642,Security,access,access,5642,"es that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5769,Security,hash,hash-table,5769,"es that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5802,Security,hash,hashes,5802,"r functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5826,Security,hash,hashing,5826,"of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5989,Security,hash,hash,5989,"n parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6079,Security,hash,hashing,6079,"rger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the fla",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6394,Security,access,access,6394,"ashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6589,Security,hash,hashing,6589,"uld this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are tw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6700,Security,hash,hashing,6700,"uld this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are tw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6960,Security,access,access,6960," we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7076,Security,access,access,7076,"garithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented < operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred``  ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7437,Security,hash,hash,7437,"f the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented < operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred``  merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7555,Security,access,access,7555,"rprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented < operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred``  merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumer",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:658,Testability,log,logic,658,"=================================; MergeFunctions pass, how it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/do",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:1068,Testability,log,logic,1068,"ow it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/docs/ProgrammersManual.html#the-function-class>`_"",; ""`basi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4431,Testability,log,logical,4431," to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5565,Testability,log,logarithmical,5565,"; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6239,Testability,log,logarithmical,6239,"e can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6323,Testability,log,log,6323,"y another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6405,Testability,log,logarithmical,6405,"ashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6447,Testability,test,tested,6447,"ashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6527,Testability,log,logarithmical,6527,"r level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6738,Testability,log,logarithmical-search,6738,"uld this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are tw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6806,Testability,log,logarithmical,6806,"equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merge",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6858,Testability,log,logarithmical-search,6858," we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:9153,Testability,log,logarithmical,9153," from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumerate only strong functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *Deferred* list. If it is not empty: refill the *worklist* contents with; *Deferred* list and redo step 2, if the *Deferred* list is empty, then exit; from method. Comparison and logarithmical search; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Let's recall our task: for every function *F* from module *M*, we have to find; equal functions *F`* in the shortest time possible , and merge them into a; single function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:9543,Testability,log,log,9543,"g functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *Deferred* list. If it is not empty: refill the *worklist* contents with; *Deferred* list and redo step 2, if the *Deferred* list is empty, then exit; from method. Comparison and logarithmical search; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Let's recall our task: for every function *F* from module *M*, we have to find; equal functions *F`* in the shortest time possible , and merge them into a; single function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the foll",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10771,Testability,log,logic,10771,"e"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object comparison results. It is similar to the next ""tree"" objects; comparison:. #. For two trees *T1* and *T2* we perform *depth-first-traversal* and have; two sequences as a product: ""*T1Items*"" and ""*T2Items*"". #. We then compare chains ""*T1Items*"" and ""*T2Items*"" in; the most-significant-item-first order. The result of items comparison; would be the result of *T1* and *T2* comparison itself. FunctionComparator::compare(void); --------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20375,Testability,test,test-suite,20375,"on:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())),; RightRes = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; if (LeftRes.first->second == RightRes.first->second) return 0;; if (LeftRes.first->second < RightRes.first->second) return -1;; return 1;. Now when *cmpValues* returns 0, we can proceed the comparison procedure.; Otherwise, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20474,Testability,log,log,20474,"on:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())),; RightRes = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; if (LeftRes.first->second == RightRes.first->second) return 0;; if (LeftRes.first->second < RightRes.first->second) return -1;; return 1;. Now when *cmpValues* returns 0, we can proceed the comparison procedure.; Otherwise, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:25747,Testability,log,log,25747,"tions). It differs from regular operations comparison with the only thing: possibility; to use ``accumulateConstantOffset`` method. So, if we get constant offset for both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs  return the result. 3. Compare operation types, use *cmpType*. All the same  if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. S",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:26000,Testability,log,log,26000," both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs  return the result. 3. Compare operation types, use *cmpType*. All the same  if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``rep",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:27159,Testability,stub,stub,27159,"* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``replaceAllUsesWith`` operation here (*RAUW*). 2. *F* could not be overridden, while *G* could. It would be good to do the; next: after merging the places where overridable function were used, still use; overridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If F may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: whether the definition of this; global may be replaced by something non-equivalent at link time. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:28124,Testability,stub,stub,28124,"verridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If F may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: whether the definition of this; global may be replaced by something non-equivalent at link time. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same name and attributes like function; *F*. It takes maximum alignment of *F* and *G*. 2. Replace all uses of function *F* with uses of function *H*. It is the two; steps procedure instead. First of all, we must take into account, all functions; from whom *F* is called would be changed: since we change the call argument; (from *F* to *H*). If so we must to review these caller functions again after; this procedure. We remove callers from ``FnTree``, method with name; ``removeUsers(F)`` does that (don't confuse with ``replaceAllUsesWith``):. 2.1. ``Inside removeUsers(Value*; V)`` we go through the all values that use value *V* (or *F* in our context).; If value is instruction, we go to function that holds this instruction and; mark it as to-be-analyzed-again (put to ``Deferred`` set), we also remove; caller from ``FnTree``. 2.2. Now we can do the replacement: call ``F->replaceAllUsesWith(H)``. 3. *H* (that now ""officiall",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:28187,Testability,stub,stub,28187," tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If F may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: whether the definition of this; global may be replaced by something non-equivalent at link time. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same name and attributes like function; *F*. It takes maximum alignment of *F* and *G*. 2. Replace all uses of function *F* with uses of function *H*. It is the two; steps procedure instead. First of all, we must take into account, all functions; from whom *F* is called would be changed: since we change the call argument; (from *F* to *H*). If so we must to review these caller functions again after; this procedure. We remove callers from ``FnTree``, method with name; ``removeUsers(F)`` does that (don't confuse with ``replaceAllUsesWith``):. 2.1. ``Inside removeUsers(Value*; V)`` we go through the all values that use value *V* (or *F* in our context).; If value is instruction, we go to function that holds this instruction and; mark it as to-be-analyzed-again (put to ``Deferred`` set), we also remove; caller from ``FnTree``. 2.2. Now we can do the replacement: call ``F->replaceAllUsesWith(H)``. 3. *H* (that now ""officially"" plays *F*'s role) is replaced with alias to *F*.; Do the same with *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:893,Usability,learn,learn,893,"=================================; MergeFunctions pass, how it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/do",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:8400,Usability,simpl,simple,8400,"on, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree``  the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented < operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred``  merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumerate only strong functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *Deferred* list. If it is not empty: refill the *worklist* contents with; *Deferred* list and redo step 2, if the *Deferred* list is empty, then exit; from method. Comparison and logarithmical search; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Let's recall our task: for every function *F* from module *M*, we have to find; equal functions *F`* in the shortest time possible , and merge them into a; single func",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:12206,Usability,simpl,simple,12206,"=========; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object comparison results. It is similar to the next ""tree"" objects; comparison:. #. For two trees *T1* and *T2* we perform *depth-first-traversal* and have; two sequences as a product: ""*T1Items*"" and ""*T2Items*"". #. We then compare chains ""*T1Items*"" and ""*T2Items*"" in; the most-significant-item-first order. The result of items comparison; would be the result of *T1* and *T2* comparison itself. FunctionComparator::compare(void); ---------------------------------; A brief look at the source code tells us that the comparison starts in the; ``int FunctionComparator::compare(void)`` method. 1. The first parts to be compared are the function's attributes and some; properties that is outside the attributes term, but still could make the; function different without changing its body. This part of the comparison is; usually done within simple *cmpNumbers* or *cmpFlags* operations (e.g.; ``cmpFlags(F1->hasGC(), F2->hasGC())``). Below is a full list of function's; properties to be compared on this stage:. * *Attributes* (those are returned by ``Function::getAttributes()``; method). * *GC*, for equivalence, *RHS* and *LHS* should be both either without; *GC* or with the same one. * *Section*, just like a *GC*: *RHS* and *LHS* should be defined in the; same section. * *Variable arguments*. *LHS* and *RHS* should be both either with or; without *var-args*. * *Calling convention* should be the same. 2. Function type. Checked by ``FunctionComparator::cmpType(Type*, Type*)``; method. It checks return type and parameters type; the method itself will be; described later. 3. Associate function formal parameters with each other. Then comparing function; bodies, if we see the usage of *LHS*'s *i*-th argument in *LHS*'s body, then,; we want to see usage of *RHS*'s *i*-th argument at the same place in *RHS*'s; body, otherwise fun",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:19288,Usability,simpl,simple,19288,"so compare them. How to implement cmpValues?; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; *Association* is a case of equality for us. We just treat such values as equal,; but, in general, we need to implement antisymmetric relation. As mentioned; above, to understand what is *less*, we can use order in which we; meet values. If both values have the same order in a function (met at the same; time), we then treat values as *associated*. Otherwise  it depends on who was; first. Every time we run the top-level compare method, we initialize two identical; maps (one for the left side, another one for the right side):. ``map<Value, int> sn_mapL, sn_mapR;``. The key of the map is the *Value* itself, the *value*  is its order (call it; *serial number*). To add value *V* we need to perform the next procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the onl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30967,Usability,simpl,simple,30967,"e *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If F could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. Aliases act as *second name* for the aliasee value. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. As-usual: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:22518,Availability,mask,mask,22518,"FI Index.; The other operands are tracked by the ``MCCFIInstruction`` object. The syntax is:. .. code-block:: text. CFI_INSTRUCTION offset $w30, -16. which may be emitted later in the MC layer as:. .. code-block:: text. .cfi_offset w30, -16. IntrinsicID Operands; ^^^^^^^^^^^^^^^^^^^^. An Intrinsic ID operand contains a generic intrinsic ID or a target-specific ID. The syntax for the ``returnaddress`` intrinsic is:. .. code-block:: text. $x0 = COPY intrinsic(@llvm.returnaddress). Predicate Operands; ^^^^^^^^^^^^^^^^^^. A Predicate operand contains an IR predicate from ``CmpInst::Predicate``, like; ``ICMP_EQ``, etc. For an int eq predicate ``ICMP_EQ``, the syntax is:. .. code-block:: text. %2:gpr(s32) = G_ICMP intpred(eq), %0, %1. .. TODO: Describe the parsers default behaviour when optional YAML attributes; are missing.; .. TODO: Describe the syntax for virtual register YAML definitions.; .. TODO: Describe the machine function's YAML flag attributes.; .. TODO: Describe the syntax for the register mask machine operands.; .. TODO: Describe the frame information YAML mapping.; .. TODO: Describe the syntax of the stack object machine operands and their; YAML definitions.; .. TODO: Describe the syntax of the block address machine operands.; .. TODO: Describe the syntax of the metadata machine operands, and the; instructions debug location attribute.; .. TODO: Describe the syntax of the register live out machine operands.; .. TODO: Describe the syntax of the machine memory operands. Comments; ^^^^^^^^. Machine operands can have C/C++ style comments, which are annotations enclosed; between ``/*`` and ``*/`` to improve readability of e.g. immediate operands.; In the example below, ARM instructions EOR and BCC and immediate operands; ``14`` and ``0`` have been annotated with their condition codes (CC); definitions, i.e. the ``always`` and ``eq`` condition codes:. .. code-block:: text. dead renamable $r2, $cpsr = tEOR killed renamable $r2, renamable $r1, 14 /* CC::always */, $",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:27747,Availability,avail,available,27747,"$noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR_REF`` s position records where the variable takes on the; designated value in the same way. More information about how these constructs are used is available in; :doc:`InstrRefDebugInfo`. The related documents :doc:`SourceLevelDebugging` and; :doc:`HowToUpdateDebugInfo` may be useful as well.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:10947,Energy Efficiency,power,power,10947,"lock:: text. bb.0.entry:; successors: %bb.1.then(32), %bb.2.else(16). .. _bb-liveins:. Live In Registers; ^^^^^^^^^^^^^^^^^. The machine basic block's live in registers have to be specified before any of; the instructions:. .. code-block:: text. bb.0.entry:; liveins: $edi, $esi. The list of live in registers and successors can be empty. The language also; allows multiple live in register and successor lists - they are combined into; one list by the parser. Miscellaneous Attributes; ^^^^^^^^^^^^^^^^^^^^^^^^. The attributes ``IsAddressTaken``, ``IsLandingPad``,; ``IsInlineAsmBrIndirectTarget`` and ``Alignment`` can be specified in brackets; after the block's definition:. .. code-block:: text. bb.0.entry (address-taken):; <instructions>; bb.2.else (align 4):; <instructions>; bb.3(landing-pad, align 4):; <instructions>; bb.4 (inlineasm-br-indirect-target):; <instructions>. .. TODO: Describe the way the reference to an unnamed LLVM IR block can be; preserved. ``Alignment`` is specified in bytes, and must be a power of two. .. _mir-instructions:. Machine Instructions; --------------------. A machine instruction is composed of a name,; :ref:`machine operands <machine-operands>`,; :ref:`instruction flags <instruction-flags>`, and machine memory operands. The instruction's name is usually specified before the operands. The example; below shows an instance of the X86 ``RETQ`` instruction with a single machine; operand:. .. code-block:: text. RETQ $eax. However, if the machine instruction has one or more explicitly defined register; operands, the instruction's name has to be specified after them. The example; below shows an instance of the AArch64 ``LDPXpost`` instruction with three; defined register operands:. .. code-block:: text. $sp, $fp, $lr = LDPXpost $sp, 2. The instruction names are serialized using the exact definitions from the; target's ``*InstrInfo.td`` files, and they are case sensitive. This means that; similar instruction names like ``TSTri`` and ``tSTRi`` repres",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:17740,Energy Efficiency,power,power,17740,"bregister indices are target specific, and are typically; defined in the target's ``*RegisterInfo.td`` file. Constant Pool Indices; ^^^^^^^^^^^^^^^^^^^^^. A constant pool index (CPI) operand is printed using its index in the; function's ``MachineConstantPool`` and an offset. For example, a CPI with the index 1 and offset 8:. .. code-block:: text. %1:gr64 = MOV64ri %const.1 + 8. For a CPI with the index 0 and offset -12:. .. code-block:: text. %1:gr64 = MOV64ri %const.0 - 12. A constant pool entry is bound to a LLVM IR ``Constant`` or a target-specific; ``MachineConstantPoolValue``. When serializing all the function's constants the; following format is used:. .. code-block:: text. constants:; - id: <index>; value: <value>; alignment: <alignment>; isTargetSpecific: <target-specific>. where:; - ``<index>`` is a 32-bit unsigned integer;; - ``<value>`` is a `LLVM IR Constant; <https://www.llvm.org/docs/LangRef.html#constants>`_;; - ``<alignment>`` is a 32-bit unsigned integer specified in bytes, and must be; power of two;; - ``<target-specific>`` is either true or false. Example:. .. code-block:: text. constants:; - id: 0; value: 'double 3.250000e+00'; alignment: 8; - id: 1; value: 'g-(LPC0+8)'; alignment: 4; isTargetSpecific: true. Global Value Operands; ^^^^^^^^^^^^^^^^^^^^^. The global value machine operands reference the global values from the; :ref:`embedded LLVM IR module <embedded-module>`.; The example below shows an instance of the X86 ``MOV64rm`` instruction that has; a global value operand named ``G``:. .. code-block:: text. $rax = MOV64rm $rip, 1, _, @G, _. The named global values are represented using an identifier with the '@' prefix.; If the identifier doesn't match the regular expression; `[-a-zA-Z$._][-a-zA-Z$._0-9]*`, then this identifier must be quoted. The unnamed global values are represented using an unsigned numeric value with; the '@' prefix, like in the following examples: ``@0``, ``@989``. Target-dependent Index Operands; ^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2729,Integrability,depend,dependent,2729," You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing late",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3962,Integrability,depend,depend,3962," line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4384,Integrability,depend,depend,4384,"ice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which st",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4572,Integrability,depend,depend,4572,"t care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6022,Integrability,depend,depends,6022,"ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6209,Integrability,depend,depends,6209,". _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:14427,Integrability,depend,dependent,14427,"o be; represented using a '``$noreg``' named register, although the former syntax; is preferred. .. _machine-operands:. Machine Operands; ----------------. There are eighteen different kinds of machine operands, and all of them can be; serialized. Immediate Operands; ^^^^^^^^^^^^^^^^^^. The immediate machine operands are untyped, 64-bit signed integers. The; example below shows an instance of the X86 ``MOV32ri`` instruction that has an; immediate machine operand ``-42``:. .. code-block:: text. $eax = MOV32ri -42. An immediate operand is also used to represent a subregister index when the; machine instruction has one of the following opcodes:. - ``EXTRACT_SUBREG``. - ``INSERT_SUBREG``. - ``REG_SEQUENCE``. - ``SUBREG_TO_REG``. In case this is true, the Machine Operand is printed according to the target. For example:. In AArch64RegisterInfo.td:. .. code-block:: text. def sub_32 : SubRegIndex<32>;. If the third operand is an immediate with the value ``15`` (target-dependent; value), based on the instruction's opcode and the operand's index the operand; will be printed as ``%subreg.sub_32``:. .. code-block:: text. %1:gpr64 = SUBREG_TO_REG 0, %0, %subreg.sub_32. For integers > 64bit, we use a special machine operand, ``MO_CImmediate``,; which stores the immediate in a ``ConstantInt`` using an ``APInt`` (LLVM's; arbitrary precision integers). .. TODO: Describe the FPIMM immediate operands. .. _register-operands:. Register Operands; ^^^^^^^^^^^^^^^^^. The :ref:`register <registers>` primitive is used to represent the register; machine operands. The register operands can also have optional; :ref:`register flags <register-flags>`,; :ref:`a subregister index <subregister-indices>`,; and a reference to the tied register operand.; The full syntax of a register operand is shown below:. .. code-block:: text. [<flags>] <register> [ :<subregister-idx-name> ] [ (tied-def <tied-op>) ]. This example shows an instance of the X86 ``XOR32rr`` instruction that has; 5 register operands with ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:18672,Integrability,depend,dependent,18672,"d integer specified in bytes, and must be; power of two;; - ``<target-specific>`` is either true or false. Example:. .. code-block:: text. constants:; - id: 0; value: 'double 3.250000e+00'; alignment: 8; - id: 1; value: 'g-(LPC0+8)'; alignment: 4; isTargetSpecific: true. Global Value Operands; ^^^^^^^^^^^^^^^^^^^^^. The global value machine operands reference the global values from the; :ref:`embedded LLVM IR module <embedded-module>`.; The example below shows an instance of the X86 ``MOV64rm`` instruction that has; a global value operand named ``G``:. .. code-block:: text. $rax = MOV64rm $rip, 1, _, @G, _. The named global values are represented using an identifier with the '@' prefix.; If the identifier doesn't match the regular expression; `[-a-zA-Z$._][-a-zA-Z$._0-9]*`, then this identifier must be quoted. The unnamed global values are represented using an unsigned numeric value with; the '@' prefix, like in the following examples: ``@0``, ``@989``. Target-dependent Index Operands; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. A target index operand is a target-specific index and an offset. The; target-specific index is printed using target-specific names and a positive or; negative offset. For example, the ``amdgpu-constdata-start`` is associated with the index ``0``; in the AMDGPU backend. So if we have a target index operand with the index 0; and the offset 8:. .. code-block:: text. $sgpr2 = S_ADD_U32 _, target-index(amdgpu-constdata-start) + 8, implicit-def _, implicit-def _. Jump-table Index Operands; ^^^^^^^^^^^^^^^^^^^^^^^^^. A jump-table index operand with the index 0 is printed as following:. .. code-block:: text. tBR_JTr killed $r0, %jump-table.0. A machine jump-table entry contains a list of ``MachineBasicBlocks``. When serializing all the function's jump-table entries, the following format is used:. .. code-block:: text. jumpTable:; kind: <kind>; entries:; - id: <index>; blocks: [ <bbreference>, <bbreference>, ... ]. where ``<kind>`` is describing how the jump ta",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4196,Modifiability,variab,variables,4196,"function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will cr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5820,Modifiability,variab,variable,5820,"lock; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:23988,Modifiability,variab,variable,23988,"nds.; .. TODO: Describe the syntax of the machine memory operands. Comments; ^^^^^^^^. Machine operands can have C/C++ style comments, which are annotations enclosed; between ``/*`` and ``*/`` to improve readability of e.g. immediate operands.; In the example below, ARM instructions EOR and BCC and immediate operands; ``14`` and ``0`` have been annotated with their condition codes (CC); definitions, i.e. the ``always`` and ``eq`` condition codes:. .. code-block:: text. dead renamable $r2, $cpsr = tEOR killed renamable $r2, renamable $r1, 14 /* CC::always */, $noreg; t2Bcc %bb.4, 0 /* CC:eq */, killed $cpsr. As these annotations are comments, they are ignored by the MI parser.; Comments can be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24495,Modifiability,variab,variable,24495,"t2Bcc %bb.4, 0 /* CC:eq */, killed $cpsr. As these annotations are comments, they are ignored by the MI parser.; Comments can be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24578,Modifiability,variab,variable,24578,"otations are comments, they are ignored by the MI parser.; Comments can be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24628,Modifiability,variab,variable,24628,"an be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24746,Modifiability,variab,variable,24746,"s; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24937,Modifiability,variab,variable,24937," constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:25048,Modifiability,variab,variable,25048,"ay optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an exp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:25155,Modifiability,variab,variable,25155,"ode, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:25422,Modifiability,variab,variable,25422,"``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referenc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26091,Modifiability,variab,variable,26091,"- ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26236,Modifiability,variab,variable,26236,"identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, bef",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26350,Modifiability,variab,variable,26350,"gging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26591,Modifiability,variab,variable,26591," is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26641,Modifiability,variab,variable,26641," is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26683,Modifiability,variab,variable,26683,"Expression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR_REF`` s position records where the variable takes on the; designated value in the same way. More information about how these const",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:27634,Modifiability,variab,variable,27634,"$noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR_REF`` s position records where the variable takes on the; designated value in the same way. More information about how these constructs are used is available in; :doc:`InstrRefDebugInfo`. The related documents :doc:`SourceLevelDebugging` and; :doc:`HowToUpdateDebugInfo` may be useful as well.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4421,Performance,perform,performed,4421," whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4508,Performance,load,load,4508," the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the momen",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4641,Performance,load,load,4641,"t care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5191,Performance,load,loader,5191,"external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6784,Performance,load,load,6784,"structions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body: |; bb.0.entry:; liveins: $rdi. $eax = MOV32rm $rdi, 1, _, 0, _; $eax = INC32r killed $eax, implicit-def dead $eflags; MOV32mr killed $rdi, 1, _, 0, _, $eax; CALL64pcrel32 @foo <regmask...>; RETQ $eax; ... The document above consists of attributes that represent the various; properties and data structures in a machine function. The attribute ``name`` is required, and its value should be identical to the; name of a function that this machine function is based on. The attribute ``body`` is a `YAML block literal string`_. Its value represent",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3103,Security,access,accessible,3103,"n, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3352,Security,expose,exposesReturnsTwice,3352,"est.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:523,Testability,test,testing,523,"========================================; Machine IR (MIR) Format Reference Manual; ========================================. .. contents::; :local:. .. warning::; This is a work in progress. Introduction; ============. This document is a reference manual for the Machine IR (MIR) serialization; format. MIR is a human readable serialization format that is used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, yo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1145,Testability,test,testing,1145,"cal:. .. warning::; This is a work in progress. Introduction; ============. This document is a reference manual for the Machine IR (MIR) serialization; format. MIR is a human readable serialization format that is used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1197,Testability,test,tests,1197,"ent is a reference manual for the Machine IR (MIR) serialization; format. MIR is a human readable serialization format that is used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run mult",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1364,Testability,test,tests,1364,"used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1574,Testability,test,tests,1574,"view; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1813,Testability,test,test,1813,"up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARG",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1925,Testability,test,test,1925,"ML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2136,Testability,test,test,2136,"ML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2190,Testability,test,test,2190,"ite MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2358,Testability,test,test,2358,"embly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `expose",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2492,Testability,test,test,2492,"llows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2790,Testability,test,test,2790," You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing late",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3257,Testability,test,test,3257," index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; functi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3758,Testability,test,testing,3758,"he MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3949,Testability,test,test,3949," line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4371,Testability,test,test,4371,"ice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which st",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4559,Testability,test,test,4559,"t care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5936,Testability,test,test,5936,"y operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5972,Testability,test,tests,5972,"ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5997,Testability,test,test,5997,"ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6172,Testability,test,tests,6172,". _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6183,Testability,test,test,6183,". _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3136,Usability,simpl,simplified,3136,"n, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3162,Usability,simpl,simplify-mir,3162,"p bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equiv",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4877,Usability,simpl,simply,4877,"Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized righ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24602,Usability,simpl,simplest,24602,"an be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2157,Availability,avail,available,2157,"erification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1139,Integrability,message,message,1139,"expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:280,Performance,optimiz,optimizer,280,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:650,Performance,optimiz,optimizer,650,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1186,Performance,perform,perform,1186,"expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1546,Performance,perform,perform,1546,"something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1959,Performance,perform,performance,1959,"g input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+===========================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2182,Performance,optimiz,optimization,2182,"erification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2336,Performance,optimiz,optimization,2336,"re branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation added during by the LLVM backend |; +----------------+--------------------------------------------------------------------------------------+; | CS-IR | Context Sensitive IR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:613,Safety,detect,detect,613,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:958,Usability,simpl,simple,958,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:836,Availability,echo,echo,836,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2014,Availability,down,download,2014,"s is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3155,Availability,down,down,3155," code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a syml",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5378,Availability,error,error,5378,"but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We shou",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5726,Availability,error,errors,5726,"rget name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; *******************",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12286,Availability,ping,ping,12286,"-------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13580,Availability,error,errors,13580,"e-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even whe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14662,Availability,failure,failure,14662,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14780,Availability,error,error,14780,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:15000,Availability,error,error,15000,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:613,Deployability,install,install,613,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1765,Deployability,install,install,1765,"arning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-proj",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2576,Deployability,install,installed,2576,"figure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run unde",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5774,Deployability,update,update,5774,"me we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10178,Deployability,update,updated,10178,"ign goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10788,Deployability,update,update,10788,"o open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11468,Deployability,patch,patch,11468,"the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11929,Deployability,patch,patches,11929,"ing a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12714,Deployability,patch,patches,12714,"oal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a varie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12836,Deployability,patch,patches,12836,"at do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13545,Deployability,patch,patch,13545," like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broke",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13714,Deployability,patch,patch,13714,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13736,Deployability,configurat,configurations,13736,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13768,Deployability,configurat,configurations,13768,"ly a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confus",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13943,Deployability,configurat,configurations,13943,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13976,Deployability,configurat,configuration,13976,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14151,Deployability,patch,patch,14151,"loper; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14408,Deployability,continuous,continuously,14408,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11627,Energy Efficiency,sustainab,sustainable,11627,"Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the directio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13214,Energy Efficiency,power,power,13214,"ave to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5384,Integrability,message,message,5384,"but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We shou",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5758,Integrability,message,messages,5758,"rget name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; *******************",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5786,Integrability,message,message,5786,"me we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5906,Integrability,message,message,5906," changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5985,Integrability,message,message,5985,":. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6902,Integrability,message,message,6902,"ang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:8547,Integrability,message,message,8547," LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9961,Integrability,depend,depending,9961,"er; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11371,Integrability,message,message,11371,"change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1549,Modifiability,config,configure,1549,"or,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2363,Modifiability,config,configure,2363,"ou'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3376,Modifiability,variab,variables,3376," to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools lik",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11665,Modifiability,maintainab,maintainable,11665,"Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the directio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13736,Modifiability,config,configurations,13736,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13768,Modifiability,config,configurations,13768,"ly a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confus",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13943,Modifiability,config,configurations,13943,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13976,Modifiability,config,configuration,13976,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2951,Performance,perform,performing,2951," one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in bui",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3469,Performance,optimiz,optimized,3469,"ices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12426,Performance,optimiz,optimize,12426,"quest will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2529,Safety,detect,detected,2529,"figure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run unde",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9021,Security,access,access,9021,"n suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11266,Security,access,access,11266,".org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11441,Security,access,access,11441,"the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12654,Security,access,access,12654,"be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12768,Security,access,access,12768,"oal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a varie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13009,Security,access,access,13009," `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a whi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13191,Security,access,access,13191,"ime consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:423,Testability,test,tests,423,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:869,Testability,test,test,869,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:897,Testability,test,test,897,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1696,Testability,test,tests,1696,",. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2962,Testability,test,test,2962," one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in bui",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3661,Testability,test,test,3661,"are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to buil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3769,Testability,assert,assertions,3769,"/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The fi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:4355,Testability,test,testing,4355,"iables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the er",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:4464,Testability,test,test,4464,"optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5127,Testability,test,tests,5127," a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To ver",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6224,Testability,test,test,6224," check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something simila",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6239,Testability,test,test,6239,"s for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6381,Testability,test,tests,6381,". .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; itera",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6525,Testability,test,tests,6525,"d: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6583,Testability,test,tests,6583,"n DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6814,Testability,test,test,6814,"ang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6935,Testability,test,tests,6935,"function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6974,Testability,test,tests,6974,"function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7053,Testability,test,test,7053," you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7092,Testability,test,test,7092," change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7347,Testability,test,tests,7347,"ke sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future rea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7463,Testability,test,test,7463,"ke sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future rea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7497,Testability,test,tests,7497,"faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ g",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7521,Testability,test,tests,7521,"he tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clar",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7688,Testability,test,test,7688,".84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7791,Testability,test,test,7791,"and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7842,Testability,test,tests,7842,"oking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://git",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9255,Testability,log,log,9255," local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11687,Testability,test,tested,11687,"Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the directio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13704,Testability,test,test,13704,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13936,Testability,test,tested,13936,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14786,Testability,log,logs,14786,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:15006,Testability,log,logs,15006,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:119,Usability,guid,guide,119,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:355,Usability,simpl,simple,355,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1012,Usability,clear,clear,1012,"oFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9311,Usability,simpl,simple,9311,"xplain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10098,Usability,simpl,simply,10098,"e; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10781,Usability,simpl,simply,10781,"o open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12618,Usability,feedback,feedback,12618,"quest will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7795,Availability,down,down,7795,"============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR ana",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19044,Availability,error,error,19044,"s of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19949,Availability,avail,available,19949,"debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing effor",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20184,Availability,avail,available,20184,"nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:1254,Deployability,pipeline,pipeline,1254,"he-new-pass-manager/>`_. Just Tell Me How To Run The Default Optimization Pipeline With The New Pass Manager; ===================================================================================. .. code-block:: c++. // Create the analysis managers.; // These must be declared in this order so that they are destroyed in the; // correct order due to inter-analysis-manager references.; LoopAnalysisManager LAM;; FunctionAnalysisManager FAM;; CGSCCAnalysisManager CGAM;; ModuleAnalysisManager MAM;. // Create the new pass manager builder.; // Take a look at the PassBuilder constructor parameters for more; // customization, e.g. specifying a TargetMachine or various debugging; // options.; PassBuilder PB;. // Register all the basic analyses with the managers.; PB.registerModuleAnalyses(MAM);; PB.registerCGSCCAnalyses(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5251,Deployability,pipeline,pipeline,5251,"s1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5368,Deployability,pipeline,pipeline,5368,"M;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipelin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5615,Deployability,pipeline,pipeline,5615,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5863,Deployability,pipeline,pipeline,5863," to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6202,Deployability,pipeline,pipeline,6202,"ss manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6327,Deployability,pipeline,pipeline,6327,"level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6460,Deployability,pipeline,pipeline,6460,"ay want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6516,Deployability,pipeline,pipelines,6516,"ntrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9369,Deployability,update,updates,9369," function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9735,Deployability,pipeline,pipeline,9735,"chedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop pas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11088,Deployability,update,updated,11088,"tation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12156,Deployability,update,update,12156,"alyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12480,Deployability,update,update,12480,"ay to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12543,Deployability,update,updaters,12543,"ay to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12915,Deployability,update,update,12915,"is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12994,Deployability,update,updated,12994,"are what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20243,Deployability,pipeline,pipeline,20243,"nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20762,Deployability,pipeline,pipeline,20762,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20934,Deployability,pipeline,pipeline,20934,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21077,Deployability,pipeline,pipeline,21077,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21486,Deployability,pipeline,pipelines,21486,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21708,Deployability,pipeline,pipeline,21708,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:2076,Energy Efficiency,adapt,adaptor,2076,"es(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePass is a loop pass; FPM.addPass(createFunctionToLoopPassAdaptor(LoopRotatePass()));. The IR hierarchy in terms of the new PM is Module -> (CGSCC ->) Function ->; Loop, where going through a CGSCC is optional. .. code-block:: c++. FunctionPassManager FPM;; // loop -> function; FPM.addPass(createFunctionToLoopPassAdaptor(LoopFooPass()));. CGSCCPassManager CGPM;; // loop -> function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(FunctionFooPass()));. ModulePassManager MPM;; // loop -> function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:3865,Energy Efficiency,adapt,adaptors,3865,"function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass()));. // loop -> function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass()))));; // function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(FunctionFooPass())));. A pass manager of a specific IR unit is also a pass of that kind. For; example, a ``FunctionPassManager`` is a function pass, meaning it can be; added to a ``ModulePassManager``:. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:18641,Energy Efficiency,adapt,adaptor,18641,"ysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19654,Energy Efficiency,adapt,adaptor,19654,"ction(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contai",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:2048,Integrability,wrap,wrapped,2048,"es(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePass is a loop pass; FPM.addPass(createFunctionToLoopPassAdaptor(LoopRotatePass()));. The IR hierarchy in terms of the new PM is Module -> (CGSCC ->) Function ->; Loop, where going through a CGSCC is optional. .. code-block:: c++. FunctionPassManager FPM;; // loop -> function; FPM.addPass(createFunctionToLoopPassAdaptor(LoopFooPass()));. CGSCCPassManager CGPM;; // loop -> function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(FunctionFooPass()));. ModulePassManager MPM;; // loop -> function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5344,Integrability,inject,inject,5344,"M;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipelin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5571,Integrability,inject,injecting,5571,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6179,Integrability,inject,inject,6179,"ss manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10841,Integrability,depend,depend,10841," potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:16476,Integrability,depend,depends,16476,"servedSet<AllAnalysesOn<Function>>());; return !(PAC.preserved() || PAC.preservedSet<AllAnalysesOn<Function>>(); || PAC.preservedSet<CFGAnalyses>());; }. says that if the ``PreservedAnalyses`` specifically preserves; ``FooAnalysis``, or if ``PreservedAnalyses`` preserves all analyses (implicit; in ``PAC.preserved()``), or if ``PreservedAnalyses`` preserves all function; analyses, or ``PreservedAnalyses`` preserves all analyses that only care; about the CFG, the ``FooAnalysisResult`` should not be invalidated. If an analysis is stateless and generally shouldn't be invalidated, use the; following:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; // Check whether the analysis has been explicitly invalidated. Otherwise, it's; // stateless and remains preserved.; auto PAC = PA.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:16861,Integrability,depend,dependencies,16861,"ses`` preserves all function; analyses, or ``PreservedAnalyses`` preserves all analyses that only care; about the CFG, the ``FooAnalysisResult`` should not be invalidated. If an analysis is stateless and generally shouldn't be invalidated, use the; following:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; // Check whether the analysis has been explicitly invalidated. Otherwise, it's; // stateless and remains preserved.; auto PAC = PA.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that depe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17751,Integrability,depend,depend,17751,"llAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17853,Integrability,depend,dependent,17853,"llAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:18605,Integrability,wrap,wrap,18605,"ysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19050,Integrability,message,messages,19050,"s of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20837,Integrability,depend,dependent,20837,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:2076,Modifiability,adapt,adaptor,2076,"es(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePass is a loop pass; FPM.addPass(createFunctionToLoopPassAdaptor(LoopRotatePass()));. The IR hierarchy in terms of the new PM is Module -> (CGSCC ->) Function ->; Loop, where going through a CGSCC is optional. .. code-block:: c++. FunctionPassManager FPM;; // loop -> function; FPM.addPass(createFunctionToLoopPassAdaptor(LoopFooPass()));. CGSCCPassManager CGPM;; // loop -> function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(FunctionFooPass()));. ModulePassManager MPM;; // loop -> function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:3865,Modifiability,adapt,adaptors,3865,"function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass()));. // loop -> function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass()))));; // function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(FunctionFooPass())));. A pass manager of a specific IR unit is also a pass of that kind. For; example, a ``FunctionPassManager`` is a function pass, meaning it can be; added to a ``ModulePassManager``:. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6475,Modifiability,plugin,plugins,6475,"ntrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6588,Modifiability,plugin,plugins,6588,"hat allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6628,Modifiability,plugin,plugin,6628,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6643,Modifiability,plugin,plugin,6643,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6668,Modifiability,plugin,plugin,6668,"er PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provid",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6724,Modifiability,plugin,plugin,6724,"anager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes wh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:18641,Modifiability,adapt,adaptor,18641,"ysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19654,Modifiability,adapt,adaptor,19654,"ction(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contai",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21344,Modifiability,extend,extend,21344,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:1241,Performance,optimiz,optimization,1241,"he-new-pass-manager/>`_. Just Tell Me How To Run The Default Optimization Pipeline With The New Pass Manager; ===================================================================================. .. code-block:: c++. // Create the analysis managers.; // These must be declared in this order so that they are destroyed in the; // correct order due to inter-analysis-manager references.; LoopAnalysisManager LAM;; FunctionAnalysisManager FAM;; CGSCCAnalysisManager CGAM;; ModuleAnalysisManager MAM;. // Create the new pass manager builder.; // Take a look at the PassBuilder constructor parameters for more; // customization, e.g. specifying a TargetMachine or various debugging; // options.; PassBuilder PB;. // Register all the basic analyses with the managers.; PB.registerModuleAnalyses(MAM);; PB.registerCGSCCAnalyses(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4640,Performance,cache,cache,4640,"ion pass; FPM.addPass(InstSimplifyPass());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-bloc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4782,Performance,optimiz,optimization,4782,"r(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4884,Performance,optimiz,optimized,4884,"tors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5272,Performance,optimiz,optimization,5272,"s1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6567,Performance,load,loading,6567,"hat allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6618,Performance,load,load-pass-plugin,6618,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6655,Performance,load,loads,6655,"er PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provid",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6958,Performance,cache,cache,6958,"entation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7378,Performance,cache,cache,7378,"TargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``Function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7486,Performance,cache,cached,7486,"parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR anal",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8656,Performance,cache,cached,8656,"scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9722,Performance,optimiz,optimization,9722,"chedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop pas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9799,Performance,concurren,concurrency,9799,"chedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop pas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9871,Performance,concurren,concurrency,9871,"st reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10002,Performance,cache,cached,10002,"cans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10123,Performance,concurren,concurrency,10123,"cans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10875,Performance,concurren,concurrency,10875," over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has beco",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14081,Performance,cache,cached,14081,"ysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14198,Performance,cache,cached,14198,";. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14438,Performance,cache,cached,14438,"s's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<Fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14530,Performance,cache,cached,14530,"s's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<Fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14618,Performance,cache,cached,14618,"xy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<FooAnalysis>();; // the default would be:; // return !(PAC.preserved() || PAC.preservedSet<AllAnalysesOn<Function>>());; return !(PAC.preserved() || PAC.preservedSet<AllAnalysesOn<Function>>(); || PAC.preservedSet<CFGAnalyses",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20749,Performance,optimiz,optimization,20749,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20921,Performance,optimiz,optimization,20921,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5344,Security,inject,inject,5344,"M;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipelin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5542,Security,expose,exposes,5542,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5571,Security,inject,injecting,5571,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6179,Security,inject,inject,6179,"ss manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6285,Security,sanitiz,sanitizer,6285,"level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8398,Security,access,access,8398,"'s ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8416,Security,access,access,8416,"ger to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8754,Security,access,access,8754,"scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10475,Security,access,accessing,10475,"cture for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10566,Security,access,access,10566,"ly degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10597,Security,access,accessing,10597,"ly degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11241,Security,access,access,11241,"hey could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11374,Security,access,accessible,11374,"lidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as wh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14044,Security,access,accessing,14044,"ysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17170,Security,access,accessible,17170,"lock:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; // Check whether the analysis has been explicitly invalidated. Otherwise, it's; // stateless and remains preserved.; auto PAC = PA.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11745,Usability,simpl,simply,11745,"e function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11758,Usability,clear,clear,11758,"e function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14001,Usability,clear,clear,14001,"dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14247,Usability,clear,clear,14247," still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14428,Usability,clear,clear,14428,"s's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<Fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17437,Usability,clear,clear,17437,"A.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17495,Usability,simpl,simply,17495,"es also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires exp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20275,Usability,simpl,simply,20275,"There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``Targ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:12139,Availability,down,downloads,12139," <LangRef.html#module-flags-metadata>`; for details.). Executing PTX; =============. The most common way to execute PTX assembly on a GPU device is to use the CUDA; Driver API. This API is a low-level interface to the GPU driver and allows for; JIT compilation of PTX code to native GPU machine code. Initializing the Driver API:. .. code-block:: c++. CUdevice device;; CUcontext context;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a compute device context; cuCtxCreate(&context, 0, device);. JIT compiling a PTX string to a device binary:. .. code-block:: c++. CUmodule module;; CUfunction function;. // JIT compile a null-terminated PTX string; cuModuleLoadData(&module, (void*)PTXString);. // Get a handle to the ""myfunction"" kernel function; cuModuleGetFunction(&function, module, ""myfunction"");. For full examples of executing PTX assembly, please see the `CUDA Samples; <https://developer.nvidia.com/cuda-downloads>`_ distribution. Common Issues; =============. ptxas complains of undefined function: __nvvm_reflect; -----------------------------------------------------. When linking with libdevice, the ``NVVMReflect`` pass must be used. See; :ref:`libdevice` for more information. Tutorial: A Simple Compute Kernel; =================================. To start, let us take a look at a simple compute kernel written directly in; LLVM IR. The kernel implements vector addition, where each thread computes one; element of the output vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.re",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19225,Availability,error,error,19225,"data node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8808,Deployability,pipeline,pipeline,8808,"LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended tha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9306,Deployability,pipeline,pipeline,9306,"ompilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; Mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9851,Deployability,pipeline,pipeline,9851,"tely after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:22812,Deployability,install,installed,22812,"ing kernel\n"";. // Kernel launch; checkCudaErrors(cuLaunchKernel(function, gridSizeX, gridSizeY, gridSizeZ,; blockSizeX, blockSizeY, blockSizeZ,; 0, NULL, KernelParams, NULL));. // Retrieve device data; checkCudaErrors(cuMemcpyDtoH(&hostC[0], devBufferC, sizeof(float)*16));. std::cout << ""Results:\n"";; for (unsigned i = 0; i != 16; ++i) {; std::cout << hostA[i] << "" + "" << hostB[i] << "" = "" << hostC[i] << ""\n"";; }. // Clean up after ourselves; delete [] hostA;; delete [] hostB;; delete [] hostC;. // Clean-up; checkCudaErrors(cuMemFree(devBufferA));; checkCudaErrors(cuMemFree(devBufferB));; checkCudaErrors(cuMemFree(devBufferC));; checkCudaErrors(cuModuleUnload(cudaModule));; checkCudaErrors(cuCtxDestroy(context));. return 0;; }. You will need to link with the CUDA driver and specify the path to cuda.h. .. code-block:: text. # clang++ sample.cpp -o sample -O2 -g -I/usr/local/cuda-5.5/include -lcuda. We don't need to specify a path to ``libcuda.so`` since this is installed in a; system location by the driver, not the CUDA toolkit. If everything goes as planned, you should see the following output when; running the compiled program:. .. code-block:: text. Using CUDA Device [0]: GeForce GTX 680; Device Compute Capability: 3.0; Launching kernel; Results:; 0 + 0 = 0; 1 + 2 = 3; 2 + 4 = 6; 3 + 6 = 9; 4 + 8 = 12; 5 + 10 = 15; 6 + 12 = 18; 7 + 14 = 21; 8 + 16 = 24; 9 + 18 = 27; 10 + 20 = 30; 11 + 22 = 33; 12 + 24 = 36; 13 + 26 = 39; 14 + 28 = 42; 15 + 30 = 45. .. note::. You will likely see a different device identifier based on your hardware. Tutorial: Linking with Libdevice; ================================. In this tutorial, we show a simple example of linking LLVM IR with the; libdevice library. We will use the same kernel as the previous tutorial,; except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25421,Deployability,configurat,configuration,25421,"entptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9927,Energy Efficiency,schedul,schedule,9927,"list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================. The value of this flag is determined by the ""nvvm-reflect-ftz"" module flag.; The following sets the ftz flag to 1. .. code-block:: llvm. !llvm.module.flag = !{!0}; !0 = !{i32 4, !""nvvm-reflect-ftz"", i32 1}. (``i32 4`` indic",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:3231,Integrability,interface,interface,3231,"========; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". These are overloaded intrinsics. You can use these on any pointer types. .. code-block:: llvm. declare i8* @llvm.nvvm.ptr.global.to.gen.p0i8.p1i8(i8 addrspace(1)*); declare i8* @llvm.nvvm.ptr.shared.to.gen.p0i8.p3i8(i8 ad",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:3423,Integrability,interface,interface,3423,"s space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". These are overloaded intrinsics. You can use these on any pointer types. .. code-block:: llvm. declare i8* @llvm.nvvm.ptr.global.to.gen.p0i8.p1i8(i8 addrspace(1)*); declare i8* @llvm.nvvm.ptr.shared.to.gen.p0i8.p3i8(i8 addrspace(3)*); declare i8* @llvm.nvvm.ptr.constant.to.gen.p0i8.p4i8(i8 addrspace(4)*); declare i8* @llvm.nvvm.ptr.local.to.gen.p0i8.p5i8(i8 addrspace(5)*). Overview:; """""""""""""""""". The '``llvm.nvvm.ptr.*.to.gen``' intrinsics convert a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8221,Integrability,depend,depends,8221,""""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Elimi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:11348,Integrability,interface,interface,11348,"----------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================. The value of this flag is determined by the ""nvvm-reflect-ftz"" module flag.; The following sets the ftz flag to 1. .. code-block:: llvm. !llvm.module.flag = !{!0}; !0 = !{i32 4, !""nvvm-reflect-ftz"", i32 1}. (``i32 4`` indicates that the value set here overrides the value in another; module we link with. See the `LangRef <LangRef.html#module-flags-metadata>`; for details.). Executing PTX; =============. The most common way to execute PTX assembly on a GPU device is to use the CUDA; Driver API. This API is a low-level interface to the GPU driver and allows for; JIT compilation of PTX code to native GPU machine code. Initializing the Driver API:. .. code-block:: c++. CUdevice device;; CUcontext context;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a compute device context; cuCtxCreate(&context, 0, device);. JIT compiling a PTX string to a device binary:. .. code-block:: c++. CUmodule module;; CUfunction function;. // JIT compile a null-terminated PTX string; cuModuleLoadData(&module, (void*)PTXString);. // Get a handle to the ""myfunction"" kernel function; cuModuleGetFunction(&function, module, ""myfunction"");. For full examples of executing PTX assembly, please see the `CUDA Samples; <https://developer.nvidia.com/cuda-downloads>`_ distribution. Common Issues; =============. ptxas complains of undefined function: __nvvm_reflect; -----------------------------------------------------. When linking with libdevice, the ``NVVMReflect`` pass must be used. See; :ref:`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:2313,Modifiability,variab,variable,2313,"nel function calling a device function in LLVM IR. The; function ``@my_kernel`` is callable from host code, but ``@my_fmad`` is not. .. code-block:: llvm. define float @my_fmad(float %x, float %y, float %z) {; %mul = fmul float %x, %y; %add = fadd float %mul, %z; ret float %add; }. define void @my_kernel(float* %ptr) {; %val = load float, float* %ptr; %ret = call float @my_fmad(float %val, float %val, float %val); store float %ret, float* %ptr; ret void; }. !nvvm.annotations = !{!1}; !1 = !{void (float*)* @my_kernel, !""kernel"", i32 1}. When compiled, the PTX kernel functions are callable by host-side code. .. _address_spaces:. Address Spaces; --------------. The NVPTX back-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:2947,Modifiability,variab,variables,2947,"he PTX kernel functions are callable by host-side code. .. _address_spaces:. Address Spaces; --------------. The NVPTX back-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:3093,Modifiability,variab,variables,3093,"ack-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". These are overloaded intrinsics. You can use these on any pointer types. .. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9528,Modifiability,variab,variables,9528,"al; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; contro",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25421,Modifiability,config,configuration,25421,"entptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25809,Modifiability,variab,variables,25809," float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param_1];; mul.wide.s32 %rl4, %r3, 4;; add.s64 %rl5, %rl2, %rl4;; ld.param.u64 %rl6, [kernel_param_2];; add.s64 %rl7, %rl3, %rl4;; add.s64 %rl1, %rl6, %rl4;; ld.global.f32 %f1, [%rl5];; ld.global.f32 %f2, [%rl7];; setp.eq.f32 %p1, %f1, 0f3F800000;; setp.eq.f32 %p2, %f2, 0f00000000;; or.pred %p3, %p1, %p2;; @%p3 bra BB0_1;; bra.uni BB0_2;; BB0_1:; mov.f32 %f110, 0f3F800000;; st.global.f32 [%rl1], %f110;; r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:1690,Performance,load,load,1690,"ons; ===========. Marking Functions as Kernels; ----------------------------. In PTX, there are two types of functions: *device functions*, which are only; callable by device code, and *kernel functions*, which are callable by host; code. By default, the back-end will emit device functions. Metadata is used to; declare a function as a kernel function. This metadata is attached to the; ``nvvm.annotations`` named metadata object, and has the following format:. .. code-block:: text. !0 = !{<function-ref>, metadata !""kernel"", i32 1}. The first parameter is a reference to the kernel function. The following; example shows a kernel function calling a device function in LLVM IR. The; function ``@my_kernel`` is callable from host code, but ``@my_fmad`` is not. .. code-block:: llvm. define float @my_fmad(float %x, float %y, float %z) {; %mul = fmul float %x, %y; %add = fadd float %mul, %z; ret float %add; }. define void @my_kernel(float* %ptr) {; %val = load float, float* %ptr; %ret = call float @my_fmad(float %val, float %val, float %val); store float %ret, float* %ptr; ret void; }. !nvvm.annotations = !{!1}; !1 = !{void (float*)* @my_kernel, !""kernel"", i32 1}. When compiled, the PTX kernel functions are callable by host-side code. .. _address_spaces:. Address Spaces; --------------. The NVPTX back-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:7740,Performance,perform,performance,7740,"====================================; ``threadId`` ``@llvm.nvvm.read.ptx.sreg.tid.*``; ``blockIdx`` ``@llvm.nvvm.read.ptx.sreg.ctaid.*``; ``blockDim`` ``@llvm.nvvm.read.ptx.sreg.ntid.*``; ``gridDim`` ``@llvm.nvvm.read.ptx.sreg.nctaid.*``; ============ =====================================. Barriers; --------. '``llvm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8794,Performance,optimiz,optimization,8794,"LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended tha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9293,Performance,optimiz,optimization,9293,"ompilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; Mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9838,Performance,optimiz,optimization,9838,"tely after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:10697,Performance,optimiz,optimized,10697," code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================. The value of this flag is determined by the ""nvvm-reflect-ftz"" module flag.; The following sets the ftz flag to 1. .. code-block:: llvm. !llvm.module.flag = !{!0}; !0 = !{i32 4, !""nvvm-reflect-ftz"", i32 1}. (``i32 4`` indicates that the value set here overrides the value in another; module we link with. See the `LangRef <LangRef.html#module-flags-metadata>`; for details.). Executing PTX; =============. The most common way to execute PTX assembly on a GPU device is to use the CUDA; Driver API. This API is a low-level interface to the GPU driver and allows for; JIT compilation of PTX code to native GPU machine code. Initializing the Driver API:. .. code-block:: c++. CUdevice device;; CUcontext context;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:13626,Performance,load,load,13626," vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = A + B; %valC = fadd float %valA, %valB. ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:. .. code-block:: text. # llc -mcpu=sm_20 kernel.ll -o kernel.ptx. .. note::. If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32``; in the module data layout string and use ``nvptx-nvidia-cuda`` as the; target triple. The output we get from ``llc`` (as of LLVM 3.4):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .f32 %f<4>;; .reg .s32 %r<2>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:13682,Performance,load,load,13682," vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = A + B; %valC = fadd float %valA, %valB. ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:. .. code-block:: text. # llc -mcpu=sm_20 kernel.ll -o kernel.ptx. .. note::. If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32``; in the module data layout string and use ``nvptx-nvidia-cuda`` as the; target triple. The output we get from ``llc`` (as of LLVM 3.4):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .f32 %f<4>;; .reg .s32 %r<2>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:17440,Performance,load,load,17440,"read.ptx.sreg.ntid.{x,y,z}`` blockDim.{x,y,z}; ``i32 @llvm.nvvm.read.ptx.sreg.nctaid.{x,y,z}`` gridDim.{x,y,z}; ``void @llvm.nvvm.barrier0()`` __syncthreads(); ================================================ ====================. Address Spaces; ^^^^^^^^^^^^^^. You may have noticed that all of the pointer types in the LLVM IR example had; an explicit address space specifier. What is address space 1? NVIDIA GPU; devices (generally) have four types of memory:. - Global: Large, off-chip memory; - Shared: Small, on-chip memory shared among all threads in a CTA; - Local: Per-thread, private memory; - Constant: Read-only memory shared across all threads. These different types of memory are represented in LLVM IR as address spaces.; There is also a fifth address space used by the NVPTX code generator that; corresponds to the ""generic"" address space. This address space can represent; addresses in any other address space (with a few exceptions). This allows; users to write IR functions that can load/store memory using the same; instructions. Intrinsics are provided to convert pointers between the generic; and non-generic address spaces. See :ref:`address_spaces` and :ref:`nvptx_intrinsics` for more information. Kernel Metadata; ^^^^^^^^^^^^^^^. In PTX, a function can be either a `kernel` function (callable from the host; program), or a `device` function (callable only from GPU code). You can think; of `kernel` functions as entry-points in the GPU program. To mark an LLVM IR; function as a `kernel` function, we make use of special LLVM metadata. The; NVPTX back-end will look for a named metadata node called; ``nvvm.annotations``. This named metadata must contain a list of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:18970,Performance,load,loading,18970,"function as a `kernel` function, we make use of special LLVM metadata. The; NVPTX back-end will look for a named metadata node called; ``nvvm.annotations``. This named metadata must contain a list of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initializatio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19111,Performance,load,load,19111,"ist of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19212,Performance,perform,perform,19212,"data node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19420,Performance,load,loaded,19420,"r the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;; checkCudaErrors(cuDeviceComputeCapability(&devMajor, &devMinor, device));; std::cout << ""Device Compute Capability: ""; << devMajor << ""."" << devMinor << ""\n"";; if (devMajor < 2) {; std::cerr << ""ERROR: Device 0 is not SM 2.0 or g",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:24572,Performance,load,load,24572,"xcept that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:24628,Performance,load,load,24628,"xcept that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25006,Performance,perform,perform,25006," declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25236,Performance,perform,performed,25236,"d = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25350,Performance,perform,performed,25350,"entptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:6507,Security,access,access,6507," These intrinsics modify the pointer value to be a valid pointer in the target; non-generic address space. Reading PTX Special Registers; -----------------------------. '``llvm.nvvm.read.ptx.sreg.*``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare i32 @llvm.nvvm.read.ptx.sreg.tid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.tid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.tid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.ntid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.ntid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.ntid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.warpsize(). Overview:; """""""""""""""""". The '``@llvm.nvvm.read.ptx.sreg.*``' intrinsics provide access to the PTX; special registers, in particular the kernel launch bounds. These registers; map in the following way to CUDA builtins:. ============ =====================================; CUDA Builtin PTX Special Register Intrinsic; ============ =====================================; ``threadId`` ``@llvm.nvvm.read.ptx.sreg.tid.*``; ``blockIdx`` ``@llvm.nvvm.read.ptx.sreg.ctaid.*``; ``blockDim`` ``@llvm.nvvm.read.ptx.sreg.ntid.*``; ``gridDim`` ``@llvm.nvvm.read.ptx.sreg.nctaid.*``; ============ =====================================. Barriers; --------. '``llvm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Link",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19692,Testability,assert,assert,19692,"ns``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;; checkCudaErrors(cuDeviceComputeCapability(&devMajor, &devMinor, device));; std::cout << ""Device Compute Capability: ""; << devMajor << ""."" << devMinor << ""\n"";; if (devMajor < 2) {; std::cerr << ""ERROR: Device 0 is not SM 2.0 or greater\n"";; return 1;; }. std::ifstream t(""kernel.ptx"");; if (!t.is_open()) {; std::cerr << ""kernel.ptx not found\n"";; return 1;; }; std::string str((std::istreambuf_iterator<char>(t)),; std::istreambuf_iterator<char>());. // Create driver context; check",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8080,Usability,guid,guide,8080,"vm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:12522,Usability,simpl,simple,12522,"text;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a compute device context; cuCtxCreate(&context, 0, device);. JIT compiling a PTX string to a device binary:. .. code-block:: c++. CUmodule module;; CUfunction function;. // JIT compile a null-terminated PTX string; cuModuleLoadData(&module, (void*)PTXString);. // Get a handle to the ""myfunction"" kernel function; cuModuleGetFunction(&function, module, ""myfunction"");. For full examples of executing PTX assembly, please see the `CUDA Samples; <https://developer.nvidia.com/cuda-downloads>`_ distribution. Common Issues; =============. ptxas complains of undefined function: __nvvm_reflect; -----------------------------------------------------. When linking with libdevice, the ``NVVMReflect`` pass must be used. See; :ref:`libdevice` for more information. Tutorial: A Simple Compute Kernel; =================================. To start, let us take a look at a simple compute kernel written directly in; LLVM IR. The kernel implements vector addition, where each thread computes one; element of the output vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19081,Usability,simpl,simple,19081,"ist of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:23493,Usability,simpl,simple,23493,"Module));; checkCudaErrors(cuCtxDestroy(context));. return 0;; }. You will need to link with the CUDA driver and specify the path to cuda.h. .. code-block:: text. # clang++ sample.cpp -o sample -O2 -g -I/usr/local/cuda-5.5/include -lcuda. We don't need to specify a path to ``libcuda.so`` since this is installed in a; system location by the driver, not the CUDA toolkit. If everything goes as planned, you should see the following output when; running the compiled program:. .. code-block:: text. Using CUDA Device [0]: GeForce GTX 680; Device Compute Capability: 3.0; Launching kernel; Results:; 0 + 0 = 0; 1 + 2 = 3; 2 + 4 = 6; 3 + 6 = 9; 4 + 8 = 12; 5 + 10 = 15; 6 + 12 = 18; 7 + 14 = 21; 8 + 16 = 24; 9 + 18 = 27; 10 + 20 = 30; 11 + 22 = 33; 12 + 24 = 36; 13 + 26 = 39; 14 + 28 = 42; 15 + 30 = 45. .. note::. You will likely see a different device identifier based on your hardware. Tutorial: Linking with Libdevice; ================================. In this tutorial, we show a simple example of linking LLVM IR with the; libdevice library. We will use the same kernel as the previous tutorial,; except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:3675,Availability,redundant,redundant,3675,"gh LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:4132,Availability,down,down,4132,"verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.; Currently there is no distinction between signed and unsigned integer types, but; rather each integer operation (e.g. add) contains flags to signal how to treat; the integer. Previously LLVM IR distinguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11858,Deployability,pipeline,pipeline,11858,"ossible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11900,Deployability,release,release,11900,"le to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``LLVMBuildLoad``) are no; longer supported. The following typed pointer functionality is still to be removed:. * Vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:4380,Energy Efficiency,reduce,reduces,4380," pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.; Currently there is no distinction between signed and unsigned integer types, but; rather each integer operation (e.g. add) contains flags to signal how to treat; the integer. Previously LLVM IR distinguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9954,Integrability,interface,interface,9954," %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:10943,Integrability,interface,interface,10943,"nd will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in pro",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11176,Integrability,interface,interface,11176,"ldInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11313,Integrability,interface,interface,11313," LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2934,Modifiability,evolve,evolved,2934,"s some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11372,Modifiability,plugin,plugin-opt,11372,"LVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:12731,Modifiability,plugin,plugin-opt,12731,"NG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``LLVMBuildLoad``) are no; longer supported. The following typed pointer functionality is still to be removed:. * Various APIs that are no longer relevant with opaque pointers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:707,Performance,load,load,707,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:744,Performance,load,load,744,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:891,Performance,load,load,891,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:960,Performance,load,load,960,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:1005,Performance,load,load,1005,"==; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2458,Performance,optimiz,optimization,2458,"ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2606,Performance,optimiz,optimization,2606,"il/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; ope",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2627,Performance,optimiz,optimization,2627,"rth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2915,Performance,optimiz,optimizations,2915,"s some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:4280,Performance,optimiz,optimizations,4280,"complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.; Currently there is no distinction between signed and unsigned integer types, but; rather each integer operation (e.g. add) contains flags to signal how to treat; the integer. Previously LLVM IR distinguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all poin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6636,Performance,optimiz,optimizations,6636,"i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6686,Performance,load,load,6686,"i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6826,Performance,load,loads,6826,"y converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have been explicitly excluded.; * To get the type of a ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:8776,Performance,load,load,8776,"aquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have been explicitly excluded.; * To get the type of a byval argument, use ``getParamByValType()``. Similar; method exists for other ABI-affecting attributes that need to know the; element type, such as byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. F",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9008,Performance,load,load,9008,"s byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9097,Performance,load,load,9097," can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9195,Performance,load,load,9195,"ot naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9507,Performance,load,load,9507,"`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11845,Performance,optimiz,optimization,11845,"ossible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:1939,Safety,safe,safe,1939,"s of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimization",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:3189,Safety,detect,detect,3189," LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:3675,Safety,redund,redundant,3675,"gh LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6782,Safety,avoid,avoid,6782,"terType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6607,Security,access,access,6607,"i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6809,Security,access,accesses,6809,"terType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9246,Security,access,accessed,9246,"ot naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9610,Security,access,accessed,9610,"ent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:5907,Testability,test,testing,5907,"nguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * U",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:5984,Testability,test,test,5984,"nsition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6151,Testability,test,test,6151,"=========. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:7187,Testability,assert,assertions,7187," ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have been explicitly excluded.; * To get the type of a byval argument, use ``getParamByValType()``. Similar; method exists for other ABI-affecting attributes that need to know the; element type, such as byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not natura",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:8933,Testability,test,test,8933,"s byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9055,Testability,test,test,9055," can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:12248,Testability,test,tested,12248,"NG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``LLVMBuildLoad``) are no; longer supported. The following typed pointer functionality is still to be removed:. * Various APIs that are no longer relevant with opaque pointers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:5875,Usability,simpl,simplifies,5875,"nguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * U",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:100,Availability,error,errors,100,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:452,Availability,down,down,452,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1519,Availability,failure,failure,1519,"rect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5782,Availability,redundant,redundant,5782,"nning pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMCo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1919,Integrability,message,message,1919,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2312,Integrability,wrap,wrapper,2312,"t.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:6903,Integrability,depend,depends,6903,"used exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to ch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2728,Modifiability,plug-in,plug-in,2728,"t command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be asso",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2978,Modifiability,plugin,plugin-opt,2978,"t could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3214,Modifiability,plugin,plugin-opt,3214,"alue will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and inc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3589,Modifiability,variab,variable,3589,". clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:87,Performance,optimiz,optimization,87,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:284,Performance,optimiz,optimization,284,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:506,Performance,optimiz,optimization,506,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:742,Performance,perform,performing,742,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:753,Performance,optimiz,optimizations,753,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:832,Performance,perform,perform,832,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1155,Performance,optimiz,optimization,1155," ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1887,Performance,perform,perform,1887,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1899,Performance,optimiz,optimizations,1899,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1963,Performance,optimiz,optimization,1963,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2055,Performance,optimiz,optimization,2055,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2078,Performance,optimiz,optimizations,2078,"at uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2120,Performance,optimiz,optimization,2120,"at uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2139,Performance,perform,performed,2139,"at uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2179,Performance,optimiz,optimizations,2179,"ax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed v",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2668,Performance,optimiz,optimizations,2668,"t command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be asso",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3534,Performance,optimiz,optimizations,3534,". clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3620,Performance,optimiz,optimization,3620,"llowing syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3747,Performance,perform,performed,3747,"llowing syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3763,Performance,optimiz,optimization,3763,"llowing syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4291,Performance,optimiz,optimizations,4291,"-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4359,Performance,optimiz,optimizations,4359,"-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4924,Performance,perform,performs,4924,"on of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:8752,Performance,perform,performed,8752,"-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable because the behavior should be the same in either; case. The majority of LLVM passes which can be skipped have already been instrumented; in the manner described above. If you are adding a new pass or believe you; have found a pass which is not being included in the opt-bisect process but; should be, you can add it as described above. Adding Finer Granularity; ========================. Once the pass in which an incorrect transformation is performed has been; determined, it may be useful to perform further analysis in order to determine; which specific transformation is causing the problem. Debug counters; can be used for this purpose.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:8804,Performance,perform,perform,8804,"-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable because the behavior should be the same in either; case. The majority of LLVM passes which can be skipped have already been instrumented; in the manner described above. If you are adding a new pass or believe you; have found a pass which is not being included in the opt-bisect process but; should be, you can add it as described above. Adding Finer Granularity; ========================. Once the pass in which an incorrect transformation is performed has been; determined, it may be useful to perform further analysis in order to determine; which specific transformation is causing the problem. Debug counters; can be used for this purpose.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:627,Safety,safe,safely,627,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5782,Safety,redund,redundant,5782,"nning pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMCo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:7246,Security,access,accessed,7246," Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5158,Testability,test,test-opt,5158,"table and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running p",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5191,Testability,test,test,5191," one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5424,Testability,test,test,5424," a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5526,Testability,test,test,5526,"cation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5598,Testability,test,test,5598," skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g)",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5739,Testability,test,test,5739,FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:335,Availability,avail,available,335,"===============================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; se",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4932,Availability,error,error,4932,"of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setN",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5619,Availability,error,error,5619,"sing their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) that uses constructs like symbol linkage and visibility, and weak [3]_; and common symbol definitions. To see how this works, imagine a program ``foo`` which links against a pair; of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5672,Availability,failure,failures,5672,"sing their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) that uses constructs like symbol linkage and visibility, and weak [3]_; and common symbol definitions. To see how this works, imagine a program ``foo`` which links against a pair; of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:6957,Availability,error,error,6957,"tLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) that uses constructs like symbol linkage and visibility, and weak [3]_; and common symbol definitions. To see how this works, imagine a program ``foo`` which links against a pair; of dynamic libraries: ``libA`` and ``libB``. On the command line, building this; program might look like:. .. code-block:: bash. $ clang++ -shared -o libA.dylib a1.cpp a2.cpp; $ clang++ -shared -o libB.dylib b1.cpp b2.cpp; $ clang++ -o myapp myapp.cpp -L. -lA -lB; $ ./myapp. In ORC, this would translate into API calls on a hypothetical CXXCompilingLayer; (with error checking omitted for brevity) as:. .. code-block:: c++. ExecutionSession ES;; RTDyldObjectLinkingLayer ObjLinkingLayer(; ES, []() { return std::make_unique<SectionMemoryManager>(); });; CXXCompileLayer CXXLayer(ES, ObjLinkingLayer);. // Create JITDylib ""A"" and add code to it using the CXX layer.; auto &LibA = ES.createJITDylib(""A"");; CXXLayer.add(LibA, MemoryBuffer::getFile(""a1.cpp""));; CXXLayer.add(LibA, MemoryBuffer::getFile(""a2.cpp""));. // Create JITDylib ""B"" and add code to it using the CXX layer.; auto &LibB = ES.createJITDylib(""B"");; CXXLayer.add(LibB, MemoryBuffer::getFile(""b1.cpp""));; CXXLayer.add(LibB, MemoryBuffer::getFile(""b2.cpp""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:8517,Availability,error,error,8517,"bB = ES.createJITDylib(""B"");; CXXLayer.add(LibB, MemoryBuffer::getFile(""b1.cpp""));; CXXLayer.add(LibB, MemoryBuffer::getFile(""b2.cpp""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look up the JIT'd main, cast it to a function pointer, then call it.; auto MainSym = ExitOnErr(ES.lookup({&MainJD}, ""main""));; auto *Main = MainSym.getAddress().toPtr<int(*)(int, char *[])>();. int Result = Main(...);. This example tells us nothing about *how* or *when* compilation will happen.; That will depend on the implementation of the hypothetical CXXCompilingLayer.; The same linker-based symbol resolution rules will apply regardless of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:8595,Availability,error,error,8595,"p""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look up the JIT'd main, cast it to a function pointer, then call it.; auto MainSym = ExitOnErr(ES.lookup({&MainJD}, ""main""));; auto *Main = MainSym.getAddress().toPtr<int(*)(int, char *[])>();. int Result = Main(...);. This example tells us nothing about *how* or *when* compilation will happen.; That will depend on the implementation of the hypothetical CXXCompilingLayer.; The same linker-based symbol resolution rules will apply regardless of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a quer",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10884,Availability,error,error,10884,"es a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11451,Availability,error,error,11451," notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12872,Availability,failure,failure,12872,"nt APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20914,Availability,error,error,20914,"n ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3111,Deployability,release,release,3111,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5393,Deployability,configurat,configuration,5393,"ule will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17205,Deployability,update,update,17205,". code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally wa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24436,Deployability,release,released,24436,"D = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materiali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:36046,Deployability,release,released,36046,"ProcessControl: Improvements to in-tree support for out-of-process; execution**. The ``TargetProcessControl`` API provides various operations on the JIT; target process (the one which will execute the JIT'd code), including; memory allocation, memory writes, function execution, and process queries; (e.g. for the target triple). By targeting this API new components can be; developed which will work equally well for in-process and out-of-process; JITing. 2. **ORC RPC based TargetProcessControl implementation**. An ORC RPC based implementation of the ``TargetProcessControl`` API is; currently under development to enable easy out-of-process JITing via; file descriptors / sockets. 3. **Core State Machine Cleanup**. The core ORC state machine is currently implemented between JITDylib and; ExecutionSession. Methods are slowly being moved to `ExecutionSession`. This; will tidy up the code base, and also allow us to support asynchronous removal; of JITDylibs (in practice deleting an associated state object in; ExecutionSession and leaving the JITDylib instance in a defunct state until; all references to it have been released). Near Future Work; ----------------. 1. **ORC JIT Runtime Libraries**. We need a runtime library for JIT'd code. This would include things like; TLS registration, reentry functions, registration code for language runtimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21069,Energy Efficiency,reduce,reduce,21069,"ram representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol string",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22099,Energy Efficiency,reduce,reduce,22099,"ookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage rel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22162,Energy Efficiency,efficient,efficient,22162,"ookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage rel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2633,Integrability,wrap,wrapper,2633,"ne running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3064,Integrability,depend,dependency,3064,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3160,Integrability,depend,dependencies,3160,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:8234,Integrability,depend,depend,8234,"auto &LibA = ES.createJITDylib(""A"");; CXXLayer.add(LibA, MemoryBuffer::getFile(""a1.cpp""));; CXXLayer.add(LibA, MemoryBuffer::getFile(""a2.cpp""));. // Create JITDylib ""B"" and add code to it using the CXX layer.; auto &LibB = ES.createJITDylib(""B"");; CXXLayer.add(LibB, MemoryBuffer::getFile(""b1.cpp""));; CXXLayer.add(LibB, MemoryBuffer::getFile(""b2.cpp""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look up the JIT'd main, cast it to a function pointer, then call it.; auto MainSym = ExitOnErr(ES.lookup({&MainJD}, ""main""));; auto *Main = MainSym.getAddress().toPtr<int(*)(int, char *[])>();. int Result = Main(...);. This example tells us nothing about *how* or *when* compilation will happen.; That will depend on the implementation of the hypothetical CXXCompilingLayer.; The same linker-based symbol resolution rules will apply regardless of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two oth",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9453,Integrability,synchroniz,synchronization,9453,"less of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9923,Integrability,wrap,wrappers,9923,"a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9966,Integrability,wrap,wrap,9966,"a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11601,Integrability,wrap,wrappers,11601," once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11962,Integrability,wrap,wraps,11962,"avior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12517,Integrability,synchroniz,synchronization,12517," in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:16485,Integrability,interface,interface,16485,"tions in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from mul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19302,Integrability,interface,interfaces,19302,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20931,Integrability,synchroniz,synchronization,20931,"n ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21287,Integrability,wrap,wrapper,21287,"iled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:25633,Integrability,wrap,wrapped,25633,"OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materialize`` and ``discard``. The; ``materialize`` function will be called for you when any symbol provided by the unit is looked up,; and it should just call the ``emit`` function on your layer, passing in the given; ``MaterializationResponsibility`` and the wrapped program representation. The ``discard`` function; will be called if some weak symbol provided by your unit is not needed (because the JIT found an; overriding definition). You can use this to drop your definition early, or just ignore it and let; the linker drops the definition later. Here is an example of an ASTLayer:. .. code-block:: c++. // ... In you JIT class; AstLayer astLayer;; // ... class AstMaterializationUnit : public orc::MaterializationUnit {; public:; AstMaterializationUnit(AstLayer &l, Ast &ast); : llvm::orc::MaterializationUnit(l.getInterface(ast)), astLayer(l),; ast(ast) {};. llvm::StringRef getName() const override {; return ""AstMaterializationUnit"";; }. void materialize(std::unique_ptr<orc::MaterializationResponsibility> r) override {; astLayer.emit(std::move(r), ast);; };. private:; void discard(const llvm::orc::JITDylib &jd, const llvm::orc::SymbolStringPtr &",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:27889,Integrability,wrap,wrappers,27889,"ResourceTrackerSP &rt, Ast &ast) {; return rt->getJITDylib().define(std::make_unique<AstMaterializationUnit>(*this, ast), rt);; }. void emit(std::unique_ptr<orc::MaterializationResponsibility> mr, Ast &ast) {; // compileAst is just function that compiles the given AST and returns; // a `llvm::orc::ThreadSafeModule`; baseLayer.emit(std::move(mr), compileAst(ast));; }. llvm::orc::MaterializationUnit::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:33115,Integrability,interface,interface,33115,"ion:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. JD.define(; absoluteSymbols({; { Mangle(""puts""), ExecutorAddr::fromPtr(&puts)},; { Mangle(""gets""), ExecutorAddr::fromPtr(&getS)}; }));. Using absoluteSymbols is reasonable if the set of symbols to be reflected is; small and fixed. On the other hand, if the set of symbols is large or variable; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached; to a JITDylib, receiving a callback whenever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(D",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4236,Modifiability,extend,extends,4236,"ogram representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5393,Modifiability,config,configuration,5393,"ule will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12340,Modifiability,layers,layers,12340," in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17397,Modifiability,inherit,inherit,17397,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19452,Modifiability,layers,layers,19452,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19530,Modifiability,layers,layers,19530,"calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19785,Modifiability,layers,layers,19785,"e(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20135,Modifiability,layers,layers,20135," from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve loo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20199,Modifiability,layers,layers,20199," 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20272,Modifiability,layers,layers,20272," The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21183,Modifiability,layers,layers,21183,"`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21483,Modifiability,layers,layers,21483," See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to app",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:32539,Modifiability,variab,variable,32539," Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the same as any other symbol defined within the; execution session (i.e. they are findable via `ExecutionSession::lookup`, and; so visible to the JIT linker during linking). One way to reflect external symbols is to add them manually using the; absoluteSymbols function:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. JD.define(; absoluteSymbols({; { Mangle(""puts""), ExecutorAddr::fromPtr(&puts)},; { Mangle(""gets""), ExecutorAddr::fromPtr(&getS)}; }));. Using absoluteSymbols is reasonable if the set of symbols to be reflected is; small and fixed. On the other hand, if the set of symbols is large or variable; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached; to a JITDylib, receiving a callback whenever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:999,Performance,perform,performance,999,"===========================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; sessio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:1068,Performance,optimiz,optimizations,1068,"=========================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default make",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2913,Performance,concurren,concurrently,2913,"**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of reloc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2984,Performance,concurren,concurrently,2984,"ssion object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a sy",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3516,Performance,concurren,concurrent,3516," compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLL",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3957,Performance,perform,performed,3957,"hed my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.take",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4717,Performance,load,loaded,4717,"ents to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure()",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9483,Performance,concurren,concurrent,9483,"less of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11183,Performance,queue,queue,11183,"e; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:13660,Performance,cache,cached,13660,"sociated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. ..",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17414,Performance,concurren,concurrency,17414,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17497,Performance,concurren,concurrently,17497,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17754,Performance,concurren,concurrent,17754,"symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17779,Performance,concurren,concurrent,17779,"symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19240,Performance,concurren,concurrent,19240,"e reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19367,Performance,concurren,concurrency,19367,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19441,Performance,concurren,concurrent,19441,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21112,Performance,perform,performance,21112,"ram representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol string",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21369,Performance,concurren,concurrently,21369,"iled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22086,Performance,perform,performance,22086,"ookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage rel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22394,Performance,perform,perform,22394,"lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage relationships; --------------------------------------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``Ex",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22831,Performance,perform,perform,22831,"urceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage relationships; --------------------------------------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = .",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24347,Performance,perform,performed,24347,"D = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materiali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:28293,Performance,concurren,concurrent,28293,"::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29775,Performance,load,loadModuleOnContext,29775,"::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:30514,Performance,concurren,concurrent,30514,"hed to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAnd",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:30818,Performance,concurren,concurrency,30818,"feModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:30847,Performance,load,loading,30847,"feModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:31178,Performance,load,loading,31178," result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the same as any other symbol defined within the; execution session (i.e. they are findable via `ExecutionSession::lookup`, and; so visible to the JIT linker during linking). One way to reflect external symbols is to add them manually using the; absoluteSymbols function:. .. code-block:: c++. const DataLayout &DL = g",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:33544,Performance,load,loadModule,33544,"e; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached; to a JITDylib, receiving a callback whenever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symboli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:34383,Performance,load,loadModule,34383,"; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symbolic resolution; using the JIT symbol tables should be preferred: it keeps the IR and objects; readable and reusable in subsequent JIT sessions. Hardcoded addresses are; difficult to read, and usually only good for one session. Roadmap; =======. ORC is still undergoing active development. Some current and future works are; listed below. Current Work; ------------. 1. **TargetProcessControl: Improvements to in-tree support for out-of-process; execution**. The ``TargetProcessControl`` API provides various operations on the JIT; target process (the one which will execute the JIT'd code), including; memory allocation, memory writes, function execution, and process queries; (e.g. for the target triple). By targeting this API new components can be; developed which will work equally well for in-process and out-of-process; JITing. 2. **OR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37043,Performance,concurren,concurrent,37043," need a runtime library for JIT'd code. This would include things like; TLS registration, reentry functions, registration code for language runtimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37270,Performance,latency,latency,37270,"untimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37294,Performance,throughput,throughput,37294,"untimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3208,Safety,safe,safe,3208,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4788,Safety,detect,detect,4788,"uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10359,Safety,safe,safe,10359,"rs compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. -",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12335,Safety,safe,safe,12335," in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17702,Safety,safe,safely,17702,"symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10375,Security,access,access,10375,"rs compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. -",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14249,Security,access,access,14249,"foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15396,Security,access,access,15396,"library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21360,Security,access,accessed,21360,"iled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:28304,Security,access,access,28304,"::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29195,Security,access,accessible,29195,"g scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29499,Security,access,accessing,29499,"memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29920,Security,access,access,29920,"_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:31686,Security,access,access,31686," constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the same as any other symbol defined within the; execution session (i.e. they are findable via `ExecutionSession::lookup`, and; so visible to the JIT linker during linking). One way to reflect external symbols is to add them manually using the; absoluteSymbols function:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. JD.define(; absoluteSymbols({; { Mangle(""puts""), ExecutorAddr::fromPtr(&puts)},; { Mangle(""gets""), ExecutorAddr::fromPtr(&getS)}; }));. Using absoluteSymbols is reasonable if the set of symbols to be reflected is; small and fixed. On the other hand, if the set of symbols is large or variable; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:33728,Security,expose,expose,33728,"ever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symbolic resolution; using the JIT symbol tables should be preferred: it keeps the IR and objects; readable and reusable in subsequent JIT sessions. Hardcoded addresses are; difficult to read, and usually onl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10169,Testability,stub,stubs,10169,"a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14507,Testability,log,log,14507,"ing established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT stand",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14539,Testability,log,log,14539,"; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14580,Testability,log,log,14580,"; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14724,Testability,log,log,14724,"sions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ...",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14764,Testability,log,log,14764,"sions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ...",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15408,Testability,log,log,15408,"library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15429,Testability,log,log,15429,"library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15630,Testability,log,log,15630,"eference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15902,Testability,log,log,15902," MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materializ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:16945,Testability,stub,stub,16945,"lementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that so",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:16965,Testability,stub,stub,16965,"`symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17066,Testability,stub,stub,17066,". code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally wa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17217,Testability,stub,stub,17217,". code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally wa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:627,Usability,simpl,simple,627,"===============================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; se",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:13054,Usability,simpl,simply,13054,"e) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24105,Usability,clear,clear,24105,"ylibs and set up linkage relationships; --------------------------------------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24131,Usability,clear,cleared,24131,"----------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` objec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24164,Usability,usab,usable,24164,"----------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` objec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24258,Usability,clear,clears,24258,"::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37616,Usability,simpl,simplify,37616,"ld; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to have been; used. .. [3] Weak definitions are currently handled correctly within dylibs, but if; multiple dylibs provide a weak definition of a symbol then each will end; up with its own definition (similar to how weak definitions are handled; in Windows DLLs). This will be fix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37985,Usability,simpl,simply,37985," to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to have been; used. .. [3] Weak definitions are currently handled correctly within dylibs, but if; multiple dylibs provide a weak definition of a symbol then each will end; up with its own definition (similar to how weak definitions are handled; in Windows DLLs). This will be fixed in the future.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1120,Availability,avail,available,1120,"ure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. Thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1283,Availability,down,down,1283,"These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ==========",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1347,Availability,avail,available,1347,"em. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1644,Availability,avail,available,1644,"pps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/index.html>`_ (>=version 0.9.4); to let the LLVM JIT tell oprofile about function addresses and line; numbers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:463,Deployability,release,release,463,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:542,Deployability,install,installed,542,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:921,Deployability,install,install,921,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:148,Modifiability,config,configure,148,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:999,Modifiability,config,configure,999,"==================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Sh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1978,Modifiability,inherit,inherit,1978,"pps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/index.html>`_ (>=version 0.9.4); to let the LLVM JIT tell oprofile about function addresses and line; numbers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:678,Performance,optimiz,optimized,678,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:934,Performance,optimiz,optimized,934,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1240,Performance,optimiz,optimization,1240,"These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ==========",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1449,Performance,optimiz,optimizing,1449," so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/ind",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1538,Performance,optimiz,optimized,1538,"ps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/index.html>`_ (>=version 0.9.4); to let the LLVM JIT tell oprofile about function addresses and line; numbers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:692,Testability,assert,assertions,692,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:832,Testability,assert,assertions,832,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1042,Testability,assert,assertions,1042,"================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enabl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2593,Availability,avail,available,2593,"Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2890,Availability,avail,available,2890,"----------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" fil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:3446,Availability,avail,available,3446,"tors. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:3993,Availability,avail,available,3993,"ram:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:4387,Availability,avail,available,4387,"---------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:4767,Availability,avail,available,4767,"functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:5184,Availability,avail,available,5184,"int dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information An",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9055,Availability,avail,available,9055,"is that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; --------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9111,Availability,error,error,9111,"is that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; --------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9272,Availability,avail,available,9272,"ion query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9341,Availability,error,error,9341,"ion query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9495,Availability,avail,available,9495,"eadable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categori",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9566,Availability,error,error,9566,"eadable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categori",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:15042,Availability,avail,available,15042,"er. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It should eventually be removed. ``constmerge``: Merge Duplicate Global Constants; ------------------------------------------------. Merges duplicate global constants together into a single constant that is; shared. This is useful because some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:15932,Availability,redundant,redundant,15932,"e some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:16863,Availability,alive,alive,16863,"-------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformati",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17447,Availability,redundant,redundant,17447,"all. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17488,Availability,redundant,redundant,17488," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19091,Availability,down,down,19091,"s guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instru",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19317,Availability,redundant,redundant,19317,"lized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant powe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23098,Availability,redundant,redundant,23098,"ssors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibilit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:29566,Availability,avail,available,29566," be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory reference",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:37391,Availability,avail,available,37391,"ss readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-debug-info``: Strip debug info for unused symbols; --------------------------------------------------------------. .. FIXME: this description is the same as for -strip. performs code stripping. this transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-prototypes``: Strip Unused Function Prototypes; -----------------------------------------------------------. This pass loops over all of the functions in the input module, looking for dead; declarations and removes them. Dead declarations are declarations of functions; for which no implementation is available (i.e., declarations for unused library; functions). ``strip-debug-declare``: Strip all ``llvm.dbg.declare`` intrinsics; ------------------------------------------------------------------. .. FIXME: this description is the same as for -strip. This pass implements code stripping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal globals and functions; #. debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-nondebug``: Strip all symbols, except dbg symbols, from a module; ------------------------------------------------------------------------. .. FIXME: this description is the same as for -strip. This pass implements code stripping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal glob",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9739,Deployability,pipeline,pipelined,9739,"his pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:14300,Deployability,update,update,14300," done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first order. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It should eventually be removed. ``constmerge``: Merge Duplicate Global Constants; ------------------------------------------------. Merges duplicate global constants together into a single constant that is; shared. This is useful because some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargeli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27834,Deployability,update,updates,27834,"simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global des",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1510,Energy Efficiency,adapt,adapted,1510,"for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; ----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11522,Energy Efficiency,allocate,allocated,11522," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20336,Energy Efficiency,power,power-of-two,20336,"-----------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21062,Energy Efficiency,reduce,reduce,21062,"<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threads of control flow running through a; basic block. This pass looks at blocks that have multiple pr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24056,Energy Efficiency,reduce,reduce,24056,"eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25894,Energy Efficiency,reduce,reduce,25894,"s are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39359,Energy Efficiency,efficient,efficient,39359,"ping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal globals and functions; #. debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``tailcallelim``: Tail Call Elimination; ---------------------------------------. This file transforms calls of the current function (self recursion) followed by; a return instruction with a branch to the entry of the function, creating a; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2073,Integrability,depend,dependences,2073,"sses; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:8177,Integrability,depend,depends,8177,"anyway. Optimization passes may make conditions that this pass checks for more or less; obvious. If an optimization pass appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-reada",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:8268,Integrability,interface,interface,8268,"appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11092,Integrability,depend,dependence,11092,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11134,Integrability,depend,dependencies,11134,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11196,Integrability,depend,dependencies,11196,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18681,Integrability,depend,dependence,18681,"is transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25641,Integrability,wrap,wrapper,25641,"if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28979,Integrability,wrap,wrapper,28979,"ks best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:31714,Integrability,rout,routine,31714," and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5)  x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1510,Modifiability,adapt,adapted,1510,"for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; ----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:6151,Modifiability,variab,variables,6151,"-------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information Analysis; ----------------------------------------------------. Interface for lazy computation of value constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TODO comments, but those; aren't comprehensive either. Second, many conditions cannot be checked; statically. This pass does no dynamic instrumentation, so it can't check for; all possible problems. Another limitation is that it assumes all code will be executed. A store; through a null pointer in a basic block which is never reached is harmless, but; this pass wil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:10619,Modifiability,variab,variables,10619,"n to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:10833,Modifiability,variab,variable,10833," are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Eliminat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11533,Modifiability,variab,variables,11533," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11696,Modifiability,variab,variables,11696,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17157,Modifiability,variab,variables,17157,"o not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17274,Modifiability,variab,variables,17274,"addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17689,Modifiability,variab,variables,17689,"igned to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the ind",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17903,Modifiability,variab,variable,17903,"e alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17981,Modifiability,variab,variable,17981," to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transf",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18057,Modifiability,variab,variable,18057,"----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transforme",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18727,Modifiability,variab,variable,18727,"is transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21600,Modifiability,variab,variables,21600,"nsformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threads of control flow running through a; basic block. This pass looks at blocks that have multiple predecessors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25032,Modifiability,variab,variable,25032,"t optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25152,Modifiability,variab,variable,25152,"y of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26109,Modifiability,variab,variable,26109," then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28597,Modifiability,variab,variables,28597,"cfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30743,Modifiability,rewrite,rewrite,30743," generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this functio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39278,Modifiability,variab,variable,39278,"ping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal globals and functions; #. debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``tailcallelim``: Tail Call Elimination; ---------------------------------------. This file transforms calls of the current function (self recursion) followed by; a return instruction with a branch to the entry of the function, creating a; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:218,Performance,optimiz,optimization,218,"====================================; LLVM's Analysis and Transform Passes; ====================================. .. contents::; :local:. Introduction; ============. This document serves as a high level summary of the optimization features that; LLVM provides. Optimizations are implemented as Passes that traverse some; portion of a program to either collect information or transform the program.; The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:7237,Performance,optimiz,optimization,7237,"ue constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TODO comments, but those; aren't comprehensive either. Second, many conditions cannot be checked; statically. This pass does no dynamic instrumentation, so it can't check for; all possible problems. Another limitation is that it assumes all code will be executed. A store; through a null pointer in a basic block which is never reached is harmless, but; this pass will warn about it anyway. Optimization passes may make conditions that this pass checks for more or less; obvious. If an optimization pass appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:7312,Performance,optimiz,optimization,7312,"ue constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TODO comments, but those; aren't comprehensive either. Second, many conditions cannot be checked; statically. This pass does no dynamic instrumentation, so it can't check for; all possible problems. Another limitation is that it assumes all code will be executed. A store; through a null pointer in a basic block which is never reached is harmless, but; this pass will warn about it anyway. Optimization passes may make conditions that this pass checks for more or less; obvious. If an optimization pass appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:12689,Performance,load,loaded,12689,"fe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block pl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:13053,Performance,load,loaded,13053,"that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first orde",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17381,Performance,perform,performs,17381,"all. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17479,Performance,perform,performs,17479," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17498,Performance,load,load,17498," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18991,Performance,perform,performed,18991,"ormed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant op",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19120,Performance,optimiz,optimization,19120,"s guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instru",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19786,Performance,perform,performed,19786,"p is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19983,Performance,perform,performed,19983,"tions have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21236,Performance,optimiz,optimization,21236,"X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threads of control flow running through a; basic block. This pass looks at blocks that have multiple predecessors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forwa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23238,Performance,optimiz,optimizations,23238,"f the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23501,Performance,perform,performs,23501,"the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23855,Performance,load,loads,23855,"dary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23977,Performance,optimiz,optimizations,23977,"(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and store",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24205,Performance,optimiz,optimizations,24205,"eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24330,Performance,load,loads,24330,"rminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24388,Performance,load,load,24388,"ariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24802,Performance,load,loads,24802,"s, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24959,Performance,load,loads,24959,"t optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25981,Performance,perform,performs,25981," then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26704,Performance,perform,performs,26704,"only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize gene",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28610,Performance,load,loads,28610,"cfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28683,Performance,perform,performance,28683,"cfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30584,Performance,load,loads,30584,"able in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30751,Performance,load,loads,30751," generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this functio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30953,Performance,perform,performs,30953,"come dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:32189,Performance,perform,performs,32189,"ntroduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5)  x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse post order traversal of current function (starting; at 2), which effectively gives values in deep loops higher rank than values not; in loops. ``rel-lookup-table-converter``: Relative lookup table converter; ---------------------------------------------------------------. This pass converts lookup tables to PIC-friendly relative lookup tables. ``reg2mem``: Demote all values to stack slots; ---------------------------------------------. This file demotes all registers to memory",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:33341,Performance,load,load,33341,"ciates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5)  x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse post order traversal of current function (starting; at 2), which effectively gives values in deep loops higher rank than values not; in loops. ``rel-lookup-table-converter``: Relative lookup table converter; ---------------------------------------------------------------. This pass converts lookup tables to PIC-friendly relative lookup tables. ``reg2mem``: Demote all values to stack slots; ---------------------------------------------. This file demotes all registers to memory references. It is intended to be the; inverse of :ref:`mem2reg <passes-mem2reg>`. By converting to ``load``; instructions, the only values live across basic blocks are ``alloca``; instructions and ``load`` instructions before ``phi`` nodes. It is intended; that this should make CFG hacking much easier. To make later hacking easier,; the entry block is split into two, such that all introduced ``alloca``; instructions (and nothing else) are in the entry block. ``sroa``: Scalar Replacement of Aggregates; ------------------------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:33439,Performance,load,load,33439,"ciates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5)  x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse post order traversal of current function (starting; at 2), which effectively gives values in deep loops higher rank than values not; in loops. ``rel-lookup-table-converter``: Relative lookup table converter; ---------------------------------------------------------------. This pass converts lookup tables to PIC-friendly relative lookup tables. ``reg2mem``: Demote all values to stack slots; ---------------------------------------------. This file demotes all registers to memory references. It is intended to be the; inverse of :ref:`mem2reg <passes-mem2reg>`. By converting to ``load``; instructions, the only values live across basic blocks are ``alloca``; instructions and ``load`` instructions before ``phi`` nodes. It is intended; that this should make CFG hacking much easier. To make later hacking easier,; the entry block is split into two, such that all introduced ``alloca``; instructions (and nothing else) are in the entry block. ``sroa``: Scalar Replacement of Aggregates; ------------------------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:36706,Performance,perform,performs,36706,"(...); A; C. This can increase the size of the code exponentially (doubling it every time a; loop is unswitched) so we only unswitch if the resultant code will be smaller; than a threshold. This pass expects :ref:`LICM <passes-licm>` to be run before it to hoist; invariant conditions out of the loop, to make the unswitching opportunity; obvious. ``strip``: Strip all symbols from a module; ------------------------------------------. Performs code stripping. This transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-debug-info``: Strip debug info for unused symbols; --------------------------------------------------------------. .. FIXME: this description is the same as for -strip. performs code stripping. this transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-prototypes``: Strip Unused Function Prototypes; -----------------------------------------------------------. This pass loops over all of the functions in the input module, looking for dead; declarations and removes them. Dead declarations are declarations of functions; for which no implementation is available (i.e., declarations for unused library; functions). ``strip-debug-declare``: Strip all ``llvm.dbg.declare`` intrinsics; ------------------------------------------------------------------. .. FIXME: this description is the same as for -strip. This pass implements code stripping. Specifically, it can delete:. #. names",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39386,Performance,perform,performed,39386,"ld only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``tailcallelim``: Tail Call Elimination; ---------------------------------------. This file transforms calls of the current function (self recursion) followed by; a return instruction with a branch to the entry of the function, creating a; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:40763,Performance,optimiz,optimization,40763," value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41025,Performance,optimiz,optimization,41025,"d Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped togeth",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41274,Performance,perform,performing,41274,"s only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedd",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41285,Performance,optimiz,optimizing,41285,"s only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedd",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41507,Performance,perform,performed,41507,"om the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2066,Safety,detect,detect,2066,"sses; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:10131,Safety,detect,detects,10131,"; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11385,Safety,safe,safety,11385,"s; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11561,Safety,safe,safe,11561," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11653,Safety,avoid,avoid,11653,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11691,Safety,safe,safe,11691,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:15932,Safety,redund,redundant,15932,"e some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17447,Safety,redund,redundant,17447,"all. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17488,Safety,redund,redundant,17488," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19317,Safety,redund,redundant,19317,"lized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant powe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23098,Safety,redund,redundant,23098,"ssors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibilit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23721,Safety,safe,safe,23721,"--------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28793,Safety,safe,safe,28793," should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; --------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2096,Security,access,accesses,2096,"sses; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11578,Security,access,access,11578," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11639,Security,sanitiz,sanitizers,11639,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:16168,Security,access,access,16168,"milar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global varia",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26205,Security,access,access,26205,"--------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39816,Security,access,access,39816,"; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numb",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41412,Security,access,access,41412,"--------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:42823,Security,secur,security,42823,"add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruction that returns a value that does; not agree with the function return value type.; #. Function call argument types match the function prototype.; #. All other things that are tested by asserts spread about the code. Note that this does not provide full security verification (like Java), but; instead just tries to ensure that code is well-formed. .. _passes-view-cfg:. ``view-cfg``: View CFG of function; ----------------------------------. Displays the control flow graph using the GraphViz tool.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-cfg-only``: View CFG of function (with no function bodies); -----------------------------------------------------------------. Displays the control flow graph using the GraphViz tool, but omitting function; bodies.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-dom``: View dominance tree of function; ---------------------------------------------. Displays the dominator tree using the GraphViz tool. ``view-dom-only``: View dominance tree of fun",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11123,Testability,test,tests,11123,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20221,Testability,log,logical,20221,"-------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:31728,Testability,log,log,31728," and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5)  x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:31799,Testability,log,log,31799," and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5)  x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41059,Testability,test,testing,41059,"d Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped togeth",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41563,Testability,log,logicals,41563," anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:42745,Testability,test,tested,42745,"other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruction that returns a value that does; not agree with the function return value type.; #. Function call argument types match the function prototype.; #. All other things that are tested by asserts spread about the code. Note that this does not provide full security verification (like Java), but; instead just tries to ensure that code is well-formed. .. _passes-view-cfg:. ``view-cfg``: View CFG of function; ----------------------------------. Displays the control flow graph using the GraphViz tool.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-cfg-only``: View CFG of function (with no function bodies); -----------------------------------------------------------------. Displays the control flow graph using the GraphViz tool, but omitting function; bodies.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-dom``: View dominance tree of function; ---------------------------------------------.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:42755,Testability,assert,asserts,42755,"other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruction that returns a value that does; not agree with the function return value type.; #. Function call argument types match the function prototype.; #. All other things that are tested by asserts spread about the code. Note that this does not provide full security verification (like Java), but; instead just tries to ensure that code is well-formed. .. _passes-view-cfg:. ``view-cfg``: View CFG of function; ----------------------------------. Displays the control flow graph using the GraphViz tool.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-cfg-only``: View CFG of function (with no function bodies); -----------------------------------------------------------------. Displays the control flow graph using the GraphViz tool, but omitting function; bodies.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-dom``: View dominance tree of function; ---------------------------------------------.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1263,Usability,simpl,simple,1263,"mplemented as Passes that traverse some; portion of a program to either collect information or transform the program.; The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1358,Usability,simpl,simply,1358," The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2221,Usability,simpl,simple,2221,"This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; wil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2402,Usability,simpl,simple,2402,"as queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:5525,Usability,simpl,simple,5525,"`dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information Analysis; ----------------------------------------------------. Interface for lazy computation of value constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. Firs",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:5715,Usability,simpl,simple,5715,"is pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information Analysis; ----------------------------------------------------. Interface for lazy computation of value constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:8781,Usability,simpl,simple,8781,"termine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9942,Usability,simpl,simply,9942,"-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; ----------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:12805,Usability,simpl,simplification,12805,"--------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:13658,Usability,simpl,simple,13658,"ment is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first order. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It shoul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:13673,Usability,guid,guided,13673,"ment is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first order. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It shoul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:16076,Usability,simpl,simple,16076,"milar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global varia",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17143,Usability,simpl,simple,17143,"o not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17742,Usability,simpl,simpler,17742,"igned to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the ind",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19426,Usability,simpl,simple,19426,"s like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19512,Usability,simpl,simplification,19512,"(i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19690,Usability,simpl,simple,19690,"it value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20418,Usability,simpl,simplify,20418,"tructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; ---------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20606,Usability,simpl,simply,20606,"e:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20661,Usability,simpl,simplified,20661,"orithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21018,Usability,simpl,simple,21018,"t, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ````, or ```` to; ``=`` or ```` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2``  ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #.  etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23285,Usability,simpl,simpler,23285,"f the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23955,Usability,simpl,simplifies,23955,"(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and store",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26429,Usability,simpl,simple,26429,"ribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; head",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26589,Usability,simpl,simplify,26589,"---------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26607,Usability,simpl,simplify,26607,"er around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simpl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26772,Usability,simpl,simpler,26772,"only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize gene",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26838,Usability,simpl,simpler,26838,"only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize gene",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26966,Usability,simpl,simplify,26966,"uction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; ----",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27114,Usability,simpl,simplifies,27114,"new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27456,Usability,simpl,simplifies,27456,"ound in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27621,Usability,simpl,simplifycfg,27621,"his pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27641,Usability,simpl,simplifycfg,27641,"his pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27972,Usability,simpl,simple,27972,"lify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by cr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28265,Usability,simpl,simple,28265,"ks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:29597,Usability,simpl,simply,29597," be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory reference",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30065,Usability,simpl,simplifycfg,30065,"ctions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations rela",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:34717,Usability,simpl,simplifycfg,34717," Replacement of Aggregates; ------------------------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks are dead unless proven otherwise; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:34733,Usability,simpl,simplifycfg,34733,"------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks are dead unless proven otherwise; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (...); A; C. This can increase the size of the c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:35368,Usability,simpl,simple-loop-unswitch,35368,"ise; * Assumes BasicBlocks are dead unless proven otherwise; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (...); A; C. This can increase the size of the code exponentially (doubling it every time a; loop is unswitched) so we only unswitch if the resultant code will be smaller; than a threshold. This pass expects :ref:`LICM <passes-licm>` to be run before it to hoist; invariant conditions out of the loop, to make the unswitching opportunity; obvious. ``strip``: Strip all symbols from a module; ------------------------------------------. Performs code stripping. This transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. Note that this transformation makes code much less readable, so it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:35393,Usability,simpl,simple-loop-unswitch,35393,"e; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (...); A; C. This can increase the size of the code exponentially (doubling it every time a; loop is unswitched) so we only unswitch if the resultant code will be smaller; than a threshold. This pass expects :ref:`LICM <passes-licm>` to be run before it to hoist; invariant conditions out of the loop, to make the unswitching opportunity; obvious. ``strip``: Strip all symbols from a module; ------------------------------------------. Performs code stripping. This transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the strip utili",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2339,Availability,avail,available,2339," be provided after a section name string; (e.g. ``!0 = !{""s1"", !1, !2}``), and a single constant tuple may be reused for; different sections (e.g. ``!0 = !{""s1"", !1, ""s2"", !1}``). Binary Encoding; ===============. *Instructions* result in emitting a single PC, and *functions* result in; emission of the start of the function and a 32-bit size. This is followed by; the auxiliary constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:4565,Availability,toler,tolerant,4565,"e the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization passes shall preserve PC sections metadata as follows:. 1. Replacements will preserve PC sections metadata of the replaced; instruction. 2. Duplications will preserve PC sections metadata of the copied; instruction. 3. Merging will preserve PC sections metadata of one of the two; instructions (no guarantee on which instruction's metadata is used). 4. Deletions will loose PC sections metadata. This is similar to debug info, and the ``BuildMI()`` helper provides a; convenient way to propagate debug info and ``!pcsections`` metadata in the; ``MIMetadata`` bundle. Note for Metadata Users; -----------------------. Use cases for ``!pcsections`` metadata should either be fully tolerant to; missing metadata, or the passes inserting ``!pcsections`` metadata should run; *after* all LLVM IR optimization passes to preserve the metadata until being; translated to MIR.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2958,Deployability,pipeline,pipeline,2958,"ve relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2009,Integrability,depend,depends,2009,"ction#1``, ``section#2``, ..., ``section#N`` in the; metadata causes the backend to emit the PC for the associated instruction or; function to all named sections. For each emitted PC in a section #N, the; constants ``aux-consts#N`` in the tuple ``!N`` will be emitted after the PC.; Multiple tuples with constant data may be provided after a section name string; (e.g. ``!0 = !{""s1"", !1, !2}``), and a single constant tuple may be reused for; different sections (e.g. ``!0 = !{""s1"", !1, ""s2"", !1}``). Binary Encoding; ===============. *Instructions* result in emitting a single PC, and *functions* result in; emission of the start of the function and a 32-bit size. This is followed by; the auxiliary constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Prop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2716,Performance,optimiz,optimizations,2716,"y constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2924,Performance,optimiz,optimization,2924,"ve relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:3873,Performance,optimiz,optimization,3873,"e the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization passes shall preserve PC sections metadata as follows:. 1. Replacements will preserve PC sections metadata of the replaced; instruction. 2. Duplications will preserve PC sections metadata of the copied; instruction. 3. Merging will preserve PC sections metadata of one of the two; instructions (no guarantee on which instruction's metadata is used). 4. Deletions will loose PC sections metadata. This is similar to debug info, and the ``BuildMI()`` helper provides a; convenient way to propagate debug info and ``!pcsections`` metadata in the; ``MIMetadata`` bundle. Note for Metadata Users; -----------------------. Use cases for ``!pcsections`` metadata should either be fully tolerant to; missing metadata, or the passes inserting ``!pcsections`` metadata should run; *after* all LLVM IR optimization passes to preserve the metadata until being; translated to MIR.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:4677,Performance,optimiz,optimization,4677,"e the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization passes shall preserve PC sections metadata as follows:. 1. Replacements will preserve PC sections metadata of the replaced; instruction. 2. Duplications will preserve PC sections metadata of the copied; instruction. 3. Merging will preserve PC sections metadata of one of the two; instructions (no guarantee on which instruction's metadata is used). 4. Deletions will loose PC sections metadata. This is similar to debug info, and the ``BuildMI()`` helper provides a; convenient way to propagate debug info and ``!pcsections`` metadata in the; ``MIMetadata`` bundle. Note for Metadata Users; -----------------------. Use cases for ``!pcsections`` metadata should either be fully tolerant to; missing metadata, or the passes inserting ``!pcsections`` metadata should run; *after* all LLVM IR optimization passes to preserve the metadata until being; translated to MIR.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:1802,Safety,avoid,avoid,1802," ... ]; [ !""<section#2"">; [ , !2 ... ]; ... ]; }; !1 = !{ iXX <aux-consts#1>, ... }; !2 = !{ iXX <aux-consts#2>, ... }; ... The occurrence of ``section#1``, ``section#2``, ..., ``section#N`` in the; metadata causes the backend to emit the PC for the associated instruction or; function to all named sections. For each emitted PC in a section #N, the; constants ``aux-consts#N`` in the tuple ``!N`` will be emitted after the PC.; Multiple tuples with constant data may be provided after a section name string; (e.g. ``!0 = !{""s1"", !1, !2}``), and a single constant tuple may be reused for; different sections (e.g. ``!0 = !{""s1"", !1, ""s2"", !1}``). Binary Encoding; ===============. *Instructions* result in emitting a single PC, and *functions* result in; emission of the start of the function and a 32-bit size. This is followed by; the auxiliary constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarant",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3867,Availability,down,down,3867,"ion on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:11544,Availability,avail,available,11544,"-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a  to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repository. Please make sure that either:. * You set a git hash as ``sourceControlBaseRevision`` in Phabricator which is; available on the GitHub repository,; * **or** you define the dependencies of your patch in Phabricator,; * **or** your patch can be applied to the main branch. Only then can the build server apply the patch locally and run the builds and; tests. Accessing build results; ^^^^^^^^^^^^^^^^^^^^^^^; Phabricator will automatically trigger a build for every new patch you upload or; modify. Phabricator shows the build results at the top of the entry. Clicking on; the links (in the red box) will show more details:. .. image:: Phabricator_premerge_results.png. The CI will compile and run tests, run clang-format and clang-tidy on lines; changed. If a unit test failed, this is shown below the build status. You can also expand; the unit test to see the details:. .. image:: Phabricator_premerge_unit_tests.png. Opting Out; ^^^^^^^^^^. In case you want to opt-out entirely of pre-merge testing, add yourself to the; `OPT OUT project <https://reviews.llvm.org/project/view/83/>`_. If you decide; to opt-out, please let us know why, so we might b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:16360,Availability,down,down,16360,"abricator review. Check you are happy with the commit message and amend it if necessary.; For example, ensure the 'Author' property of the commit is set to the original author.; You can use a command to correct the author property if it is incorrect:. ::. git commit --amend --author=""John Doe <jdoe@llvm.org>"". Then, make sure the commit is up-to-date, and commit it. This can be done by running; the following:. ::. git pull --rebase https://github.com/llvm/llvm-project.git main; git show # Ensure the patch looks correct.; ninja check-$whatever # Rerun the appropriate tests if needed.; git push https://github.com/llvm/llvm-project.git HEAD:main. Abandoning a change; -------------------. If you decide you should not commit the patch, you should explicitly abandon; the review so that reviewers don't think it is still open. In the web UI,; scroll to the bottom of the page where normally you would enter an overall; comment. In the drop-down Action list, which defaults to ""Comment,"" you should; select ""Abandon Revision"" and then enter a comment explaining why. Click the; Submit button to finish closing the review. Status; ------. Please let us know whether you like it and what could be improved! We're still; working on setting up a bug tracker, but you can email klimek-at-google-dot-com; and chandlerc-at-gmail-dot-com and CC the llvm-dev mailing list with questions; until then. We also could use help implementing improvements. This sadly is; really painful and hard because the Phabricator codebase is in PHP and not as; testable as you might like. However, we've put exactly what we're deploying up; on an `llvm-reviews GitHub project`_ where folks can hack on it and post pull; requests. We're looking into what the right long-term hosting for this is, but; note that it is a derivative of an existing open source project, and so not; trivially a good fit for an official LLVM project. .. _LLVM's Phabricator: https://reviews.llvm.org; .. _`https://reviews.llvm.org`: https://revie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:402,Deployability,patch,patches,402,".. _phabricator-reviews:. =============================; Code Reviews with Phabricator; =============================. .. warning::. Phabricator is deprecated and will be switched to read-only mode in October; 2023, for new code contributions use :ref:`GitHub Pull Requests <github-reviews>`. .. contents::; :local:. If you prefer to use a web user interface for code reviews, you can now submit; your patches for Clang and LLVM at `LLVM's Phabricator`_ instance. While Phabricator is a useful tool for some, the relevant -commits mailing list; is the system of record for all LLVM code review. The mailing list should be; added as a subscriber on all reviews, and Phabricator users should be prepared; to respond to free-form comments in mail sent to the commits list. Sign up; -------. To get started with Phabricator, navigate to `https://reviews.llvm.org`_ and; click the power icon in the top right. You can register with a GitHub account,; a Google account, or you can create your own profile. Make *sure* that the email address registered with Phabricator is subscribed; to the relevant -commits mailing list. If you are not subscribed to the commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:1552,Deployability,patch,patches,1552,"st; is the system of record for all LLVM code review. The mailing list should be; added as a subscriber on all reviews, and Phabricator users should be prepared; to respond to free-form comments in mail sent to the commits list. Sign up; -------. To get started with Phabricator, navigate to `https://reviews.llvm.org`_ and; click the power icon in the top right. You can register with a GitHub account,; a Google account, or you can create your own profile. Make *sure* that the email address registered with Phabricator is subscribed; to the relevant -commits mailing list. If you are not subscribed to the commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2137,Deployability,patch,patch,2137,"he commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2214,Deployability,patch,patch,2214,"he commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2220,Deployability,update,update,2220,"he commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2270,Deployability,update,update,2270,"omatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2353,Deployability,update,update,2353,"omatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2578,Deployability,patch,patches,2578,"get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format pa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2653,Deployability,patch,patches,2653,"ng a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3311,Deployability,patch,patch,3311,"r update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3357,Deployability,patch,patch,3357,"habricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Di",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3403,Deployability,patch,patch,3403," _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3451,Deployability,patch,patch,3451,"-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3482,Deployability,patch,patch,3482,"-------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3585,Deployability,patch,patches,3585,"-------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3612,Deployability,patch,patch,3612,"led; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repos",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3710,Deployability,patch,patch,3710,"c` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4353,Deployability,update,updated,4353,"tch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4361,Deployability,patch,patch,4361,"tch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4432,Deployability,update,updated,4432,"h.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4462,Deployability,update,updated,4462,"h.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4470,Deployability,patch,patch,4470,"h.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5054,Deployability,patch,patch,5054,":`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5121,Deployability,patch,patch-series,5121,"itory field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5147,Deployability,patch,patch,5147," cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5815,Deployability,patch,patch,5815,"le as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5842,Deployability,patch,patch,5842,"le as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5853,Deployability,patch,patches,5853,"le as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5867,Deployability,patch,patch,5867,"ring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own lin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6160,Deployability,patch,patch,6160,"ating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6461,Deployability,patch,patch-summaries,6461,"Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. W",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6485,Deployability,patch,patch,6485,"^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please re",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6648,Deployability,patch,patch,6648,"ast review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creati",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6782,Deployability,patch,patch,6782," where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6840,Deployability,patch,patch,6840," where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7220,Deployability,update,update,7220,"s with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7424,Deployability,update,updated,7424,"ild Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7440,Deployability,patch,patches,7440,"ild Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7483,Deployability,patch,patches,7483,"o new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7670,Deployability,patch,patches,7670,"th the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial revie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7684,Deployability,patch,patch,7684,"th the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial revie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7842,Deployability,update,update,7842," ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial reviewer(s):. * Use ``git blame`` and the commit log to find names of people who have; recently modified the same area of code tha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:8515,Deployability,patch,patch,8515,"ase remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial reviewer(s):. * Use ``git blame`` and the commit log to find names of people who have; recently modified the same area of code that you are modifying.; * Look in CODE_OWNERS.TXT to see who might be responsible for that area.; * If you've discussed the change on a dev list, the people who participated; might be appropriate reviewers. Even if you think the code owner is the busiest person in the world, it's still; okay to put them as a reviewer. Being the code owner means they have accepted; responsibility for making sure the review happens. Reviewing code with Phabricator; -------------------------------. Phabricator allows you to add inline comments as well as overall comments; to a revision. To add an inline comment, select the lines of code you want; to comment on by clicking and dragging the line numbers in the dif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:9837,Deployability,patch,patch,9837,"difying.; * Look in CODE_OWNERS.TXT to see who might be responsible for that area.; * If you've discussed the change on a dev list, the people who participated; might be appropriate reviewers. Even if you think the code owner is the busiest person in the world, it's still; okay to put them as a reviewer. Being the code owner means they have accepted; responsibility for making sure the review happens. Reviewing code with Phabricator; -------------------------------. Phabricator allows you to add inline comments as well as overall comments; to a revision. To add an inline comment, select the lines of code you want; to comment on by clicking and dragging the line numbers in the diff pane.; When you have added all your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main bran",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10422,Deployability,continuous,continuous,10422,"t; to comment on by clicking and dragging the line numbers in the diff pane.; When you have added all your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a  to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repos",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10433,Deployability,integrat,integration,10433,"t; to comment on by clicking and dragging the line numbers in the diff pane.; When you have added all your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a  to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repos",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10485,Deployability,patch,patches,10485,"your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a  to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repository. Please make sure that either:. * You set a git hash as ``sourceControlBaseRevision`` in Phabric",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10622,Deployability,patch,patch,10622,"t the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a  to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repository. Please make sure that either:. * You set a git hash as ``sourceControlBaseRevision`` in Phabricator which is; available on the GitHub repository,; * **or** you define the dependencies of your patch in Phabricator,; * **",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
